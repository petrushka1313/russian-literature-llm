{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c2592d4",
   "metadata": {},
   "source": [
    "# *–ê–Ω–∞–ª–∏–∑ –∑–∞–¥–∞–Ω–∏—è –∏ –ø–ª–∞–Ω –ø—Ä–æ–µ–∫—Ç–∞*\n",
    "\n",
    "## **–¶–µ–ª—å: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –¥–≤–∞ –∫–ª—é—á–µ–≤—ã—Ö —ç—Ç–∞–ø–∞ –æ–±—É—á–µ–Ω–∏—è Large Language Model (LLM) - Pretrain –∏ Supervised Fine-Tuning (SFT).**\n",
    "\n",
    "### –ö—Ä–∏—Ç–µ—Ä–∏–∏ —É—Å–ø–µ—Ö–∞:\n",
    "### Pretrain: –ú–æ–¥–µ–ª—å (~150M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤) –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –¥–ª—è –ø—Ä–æ–º–ø—Ç–æ–≤ –∏–∑ —Ä—É—Å—Å–∫–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã.\n",
    "### SFT: –ú–æ–¥–µ–ª—å Qwen2.5-0.5B –∞–¥–µ–∫–≤–∞—Ç–Ω–æ –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, —Å–ª–µ–¥—É—è –∑–∞–¥–∞–Ω–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdfb0c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–µ–∫—É—â–∞—è –≤–µ—Ä—Å–∏—è jinja2: 3.1.3\n"
     ]
    }
   ],
   "source": [
    "# –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, \n",
    "    Trainer, TrainingArguments, DataCollatorForLanguageModeling,\n",
    "    LlamaConfig, GPT2LMHeadModel\n",
    ")\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# –°–∫–∞—á–∏–≤–∞–µ–º –∏ —Ä–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "# –ü—Ä–æ–≤–µ—Ä–∏–º –∫–∞–∫–∞—è –≤–µ—Ä—Å–∏—è jinja2 —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞\n",
    "try:\n",
    "    import jinja2\n",
    "    print(f\"–¢–µ–∫—É—â–∞—è –≤–µ—Ä—Å–∏—è jinja2: {jinja2.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"jinja2 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω\")\n",
    "\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainerCallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dfdc529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: data/RussianNovels-master/corpus\n"
     ]
    }
   ],
   "source": [
    "# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –¥–∞–Ω–Ω—ã—Ö\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è\n",
    "def download_and_extract_data():\n",
    "    # –°–∫–∞—á–∏–≤–∞–µ–º –≤–µ—Å—å —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π\n",
    "    repo_url = \"https://github.com/JoannaBy/RussianNovels/archive/refs/heads/master.zip\"\n",
    "    response = requests.get(repo_url)\n",
    "    \n",
    "    with zipfile.ZipFile(io.BytesIO(response.content)) as zip_ref:\n",
    "        zip_ref.extractall('data/')\n",
    "    \n",
    "    return 'data/RussianNovels-master/corpus'\n",
    "\n",
    "corpus_path = download_and_extract_data()\n",
    "print(f\"–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {corpus_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c688df85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∂–µ–Ω–æ 108 —Ç–µ–∫—Å—Ç–æ–≤\n",
      "–ü—Ä–∏–º–µ—Ä—ã —Ñ–∞–π–ª–æ–≤: ['Pushkin_CapitanskaaDochka.txt', 'Sologub_KorolevaOrtruda.txt', 'Chekhov_Dama.txt', 'Gogol_Viy.txt', 'Gorky_ZyznKlimaSamgina3.txt', 'Zhukova_Dacha.txt', 'Nabokov_Otchayanie_1934.txt', 'NKhvoshchinskaya_PervayaBorba.txt', 'Nabokov_Veschi_1972_Ilin.txt', 'Sologub_TjazolyjeSny.txt']\n"
     ]
    }
   ],
   "source": [
    "# –ß—Ç–µ–Ω–∏–µ –≤—Å–µ—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤\n",
    "def load_texts_from_directory(directory_path):\n",
    "    texts = []\n",
    "    filenames = []\n",
    "    \n",
    "    for file_path in Path(directory_path).rglob('*.txt'):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read().strip()\n",
    "                if len(text) > 100:  # —Ñ–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Ñ–∞–π–ª—ã\n",
    "                    texts.append(text)\n",
    "                    filenames.append(file_path.name)\n",
    "        except Exception as e:\n",
    "            print(f\"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {file_path}: {e}\")\n",
    "    \n",
    "    return texts, filenames\n",
    "\n",
    "texts, filenames = load_texts_from_directory(corpus_path)\n",
    "print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤\")\n",
    "print(f\"–ü—Ä–∏–º–µ—Ä—ã —Ñ–∞–π–ª–æ–≤: {filenames[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19c9f016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f03fbc6f38de4482bb4f91b0db15721a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –æ—Å—Ç–∞–ª–æ—Å—å 108 —Ç–µ–∫—Å—Ç–æ–≤\n",
      "–ü—Ä–∏–º–µ—Ä –æ—á–∏—â–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:\n",
      "–ö–ê–ü–ò–¢–ê–ù–°–ö–ê–Ø –î–û–ß–ö–ê –ë–µ—Ä–µ–≥–∏ —á–µ—Å—Ç—å —Å–º–æ–ª–æ–¥—É -- –¢–æ–≥–æ –Ω–µ –Ω–∞–¥–æ–±–Ω–æ; –ø—É—Å—Ç—å –≤ –∞—Ä–º–∏–∏ –ø–æ—Å–ª—É–∂–∏—Ç -- –ò–∑—Ä—è–¥–Ω–æ —Å–∫–∞–∑–∞–Ω–æ –ø—É—Å–∫–∞–π –µ–≥–æ –ø–æ—Ç—É–∂–∏—Ç –î–∞ –∫—Ç–æ –µ–≥–æ –æ—Ç–µ—Ü –° —Ç–µ—Ö –ø–æ—Ä –∂–∏–ª –æ–Ω –≤ —Å–≤–æ–µ–π –°–∏–º–±–∏—Ä—Å–∫–æ–π –¥–µ—Ä–µ–≤–Ω–µ, –≥–¥–µ –∏ –∂–µ–Ω–∏–ª—Å—è –Ω–∞ –¥–µ–≤–∏—Ü–µ –ê–≤–¥–æ—Ç—å–µ –í–∞—Å–∏–ª—å–µ–≤–Ω–µ –Æ , –¥–æ—á–µ—Ä–∏ –±–µ–¥–Ω–æ–≥–æ —Ç–∞–º–æ—à–Ω–µ–≥–æ –¥–≤–æ—Ä—è–Ω–∏–Ω–∞ –ù–∞—Å –±—ã–ª–æ –¥–µ–≤—è—Ç—å —á–µ–ª–æ–≤–µ–∫ –¥–µ—Ç–µ–π –í—Å–µ –º–æ–∏ –±—Ä–∞—Ç—å—è –∏ —Å–µ—Å—Ç—Ä—ã —É–º–µ—Ä–ª–∏ –≤–æ –º–ª–∞–¥–µ–Ω—á–µ—Å—Ç–≤–µ –ú–∞—Ç—É—à–∫–∞ –±—ã–ª–∞ –µ—â–µ –º–Ω–æ—é –±—Ä—é—Ö–∞—Ç–∞, –∫–∞–∫ —É–∂–µ —è –±—ã–ª –∑–∞–ø–∏—Å–∞–Ω –≤ –°–µ–º–µ–Ω–æ–≤—Å–∫–∏–π –ø–æ–ª–∫ —Å–µ—Ä–∂–∞–Ω—Ç–æ–º, –ø–æ –º–∏–ª–æ—Å—Ç–∏ –º–∞–π–æ—Ä–∞ –≥–≤–∞—Ä–¥–∏–∏ –∫–Ω—è–∑—è –ë , –±–ª–∏–∑–∫–æ–≥–æ –Ω–∞—à–µ–≥–æ —Ä–æ–¥—Å—Ç–≤–µ–Ω–Ω–∏–∫–∞ –ï—Å–ª–∏ –±—ã –ø–∞—á–µ...\n"
     ]
    }
   ],
   "source": [
    "# –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö\n",
    "def preprocess_text(text):\n",
    "    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    cleaned_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "            \n",
    "        # –£–¥–∞–ª—è–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –Ω–µ-–∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏–º–∏ —Å–∏–º–≤–æ–ª–∞–º–∏ (–∫—Ä–æ–º–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ –∏ –ø—Ä–æ–±–µ–ª–æ–≤)\n",
    "        if re.search(r'[^–∞-—è–ê-–Ø—ë–Å\\s\\-,:;\"()¬´¬ª‚Äî]', sentence):\n",
    "            continue\n",
    "            \n",
    "        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â—É—é—Å—è –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é\n",
    "        sentence = re.sub(r'\\.{2,}', '.', sentence)\n",
    "        sentence = re.sub(r',{2,}', ',', sentence)\n",
    "        sentence = re.sub(r'!{2,}', '!', sentence)\n",
    "        sentence = re.sub(r'\\?{2,}', '?', sentence)\n",
    "        \n",
    "        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã\n",
    "        sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "        \n",
    "        if len(sentence) > 10:  # –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è\n",
    "            cleaned_sentences.append(sentence)\n",
    "    \n",
    "    return ' '.join(cleaned_sentences)\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –∫–æ –≤—Å–µ–º —Ç–µ–∫—Å—Ç–∞–º\n",
    "cleaned_texts = []\n",
    "for text in tqdm(texts):\n",
    "    cleaned = preprocess_text(text)\n",
    "    if cleaned:\n",
    "        cleaned_texts.append(cleaned)\n",
    "\n",
    "print(f\"–ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –æ—Å—Ç–∞–ª–æ—Å—å {len(cleaned_texts)} —Ç–µ–∫—Å—Ç–æ–≤\")\n",
    "print(f\"–ü—Ä–∏–º–µ—Ä –æ—á–∏—â–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:\\n{cleaned_texts[0][:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1eb8f89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: 107 —Ç–µ–∫—Å—Ç–æ–≤\n"
     ]
    }
   ],
   "source": [
    "# –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤\n",
    "def remove_duplicates(texts):\n",
    "    seen = set()\n",
    "    unique_texts = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text_hash = hash(text[:1000])  # —Ö–µ—à–∏—Ä—É–µ–º –Ω–∞—á–∞–ª–æ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤\n",
    "        if text_hash not in seen:\n",
    "            seen.add(text_hash)\n",
    "            unique_texts.append(text)\n",
    "    \n",
    "    return unique_texts\n",
    "\n",
    "unique_texts = remove_duplicates(cleaned_texts)\n",
    "print(f\"–ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(unique_texts)} —Ç–µ–∫—Å—Ç–æ–≤\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cec9adc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°–æ–∑–¥–∞–Ω–æ 14438 —á–∞–Ω–∫–æ–≤\n",
      "–ü—Ä–∏–º–µ—Ä —á–∞–Ω–∫–∞:\n",
      "–ö–ê–ü–ò–¢–ê–ù–°–ö–ê–Ø –î–û–ß–ö–ê –ë–µ—Ä–µ–≥–∏ —á–µ—Å—Ç—å —Å–º–æ–ª–æ–¥—É -- –¢–æ–≥–æ –Ω–µ –Ω–∞–¥–æ–±–Ω–æ; –ø—É—Å—Ç—å –≤ –∞—Ä–º–∏–∏ –ø–æ—Å–ª—É–∂–∏—Ç -- –ò–∑—Ä—è–¥–Ω–æ —Å–∫–∞–∑–∞–Ω–æ –ø—É—Å–∫–∞–π –µ–≥–æ –ø–æ—Ç—É–∂–∏—Ç –î–∞ –∫—Ç–æ –µ–≥–æ –æ—Ç–µ—Ü –° —Ç–µ—Ö –ø–æ—Ä –∂–∏–ª –æ–Ω –≤ —Å–≤–æ–µ–π –°–∏–º–±–∏—Ä—Å–∫–æ–π –¥–µ—Ä–µ–≤–Ω–µ, –≥–¥–µ –∏ –∂–µ–Ω–∏–ª—Å—è –Ω–∞ –¥–µ...\n"
     ]
    }
   ],
   "source": [
    "# –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏\n",
    "def split_into_chunks(texts, chunk_size=400):\n",
    "    \"\"\"–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç—ã –Ω–∞ —á–∞–Ω–∫–∏ –ø—Ä–∏–º–µ—Ä–Ω–æ –ø–æ chunk_size —Å–ª–æ–≤\"\"\"\n",
    "    all_chunks = []\n",
    "    \n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        for i in range(0, len(words), chunk_size):\n",
    "            chunk = ' '.join(words[i:i + chunk_size])\n",
    "            if len(chunk) > 50:  # –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —á–∞–Ω–∫–∞\n",
    "                all_chunks.append(chunk)\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "chunks = split_into_chunks(unique_texts, chunk_size=400)\n",
    "print(f\"–°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤\")\n",
    "print(f\"–ü—Ä–∏–º–µ—Ä —á–∞–Ω–∫–∞:\\n{chunks[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b45677",
   "metadata": {},
   "source": [
    "### –≠—Ç–∞–ø 1.2: –°–æ–∑–¥–∞–Ω–∏–µ BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d6d4d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–µ–∫—Å—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: texts_for_tokenizer.txt\n",
      "–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: 61.77 MB\n",
      "\n",
      "\n",
      "\n",
      "–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –æ–±—É—á–µ–Ω!\n",
      "–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω!\n",
      "–¢–µ–∫—Å—Ç: '–í—Å–µ –º—ã—Å–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ—é—Ç –æ–≥—Ä–æ–º–Ω—ã–µ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è'\n",
      "–¢–æ–∫–µ–Ω—ã: ['<bos>', '–í—Å–µ', '–º—ã—Å–ª–∏', ',', '–∫–æ—Ç–æ—Ä—ã–µ', '–∏–º–µ', '—é—Ç', '–æ–≥—Ä–æ–º', '–Ω—ã–µ', '–ø–æ—Å–ª–µ–¥', '—Å—Ç–≤–∏—è', '<eos>']\n",
      "IDs: [2, 665, 1206, 7, 664, 442, 350, 1909, 269, 818, 2055, 3]\n",
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤: 12\n"
     ]
    }
   ],
   "source": [
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç—ã –≤ –æ–¥–∏–Ω —Ñ–∞–π–ª –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
    "def prepare_texts_for_tokenizer(chunks, output_file=\"texts_for_tokenizer.txt\"):\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for chunk in chunks:\n",
    "            f.write(chunk + '\\n')\n",
    "    return output_file\n",
    "\n",
    "corpus_file = prepare_texts_for_tokenizer(chunks)\n",
    "print(f\"–¢–µ–∫—Å—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {corpus_file}\")\n",
    "print(f\"–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {os.path.getsize(corpus_file) / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "\n",
    "def train_bpe_tokenizer(corpus_file, vocab_size=3000):\n",
    "    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
    "    \n",
    "    # –ü—Ä–µ—Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä - —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    \n",
    "    # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã\n",
    "    special_tokens = [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"]\n",
    "    \n",
    "    # –¢—Ä–µ–Ω–µ—Ä –¥–ª—è BPE\n",
    "    trainer = BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=special_tokens,\n",
    "        min_frequency=2\n",
    "    )\n",
    "    \n",
    "    # –û–±—É—á–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
    "    tokenizer.train(files=[corpus_file], trainer=trainer)\n",
    "    \n",
    "    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫—É –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è <bos> –∏ <eos>\n",
    "    tokenizer.post_processor = TemplateProcessing(\n",
    "        single=\"<bos> $A <eos>\",\n",
    "        special_tokens=[(\"<bos>\", 2), (\"<eos>\", 3)]\n",
    "    )\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "# –û–±—É—á–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
    "bpe_tokenizer = train_bpe_tokenizer(corpus_file, vocab_size=3000)\n",
    "print(\"–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –æ–±—É—á–µ–Ω!\")\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
    "bpe_tokenizer.save(\"custom_bpe_tokenizer.json\")\n",
    "print(\"–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω!\")\n",
    "\n",
    "# –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
    "test_text = \"–í—Å–µ –º—ã—Å–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ—é—Ç –æ–≥—Ä–æ–º–Ω—ã–µ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è\"\n",
    "encoded = bpe_tokenizer.encode(test_text)\n",
    "print(f\"–¢–µ–∫—Å—Ç: '{test_text}'\")\n",
    "print(f\"–¢–æ–∫–µ–Ω—ã: {encoded.tokens}\")\n",
    "print(f\"IDs: {encoded.ids}\")\n",
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤: {len(encoded.ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d295266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–æ—Ä–º–∞—Ç–µ Hugging Face!\n",
      "HF —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä: torch.Size([1, 12])\n"
     ]
    }
   ],
   "source": [
    "# –°–æ–∑–¥–∞–µ–º –æ–±–µ—Ä—Ç–∫—É –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å Hugging Face\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ HF-—Ñ–æ—Ä–º–∞—Ç\n",
    "hf_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=bpe_tokenizer,\n",
    "    bos_token=\"<bos>\",\n",
    "    eos_token=\"<eos>\", \n",
    "    pad_token=\"<pad>\",\n",
    "    unk_token=\"<unk>\"\n",
    ")\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç–µ Hugging Face\n",
    "hf_tokenizer.save_pretrained(\"custom_bpe_tokenizer_hf\")\n",
    "print(\"–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–æ—Ä–º–∞—Ç–µ Hugging Face!\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–±–æ—Ç—É\n",
    "test_encodings = hf_tokenizer(\n",
    "    test_text, \n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "print(f\"HF —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä: {test_encodings.input_ids.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259376cc",
   "metadata": {},
   "source": [
    "### –≠—Ç–∞–ø 1.3: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ä–∞–∑–±–∏–µ–Ω–∏–µ–º –Ω–∞ —á–∞–Ω–∫–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "867ddfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ù–∞—á–∏–Ω–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –∏ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "986f04c0e0524be4a9fb11f4c4d89ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14438 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°–æ–∑–¥–∞–Ω–æ 28818 —á–∞–Ω–∫–æ–≤\n",
      "–ü—Ä–∏–º–µ—Ä —á–∞–Ω–∫–∞ (–ø–µ—Ä–≤—ã–µ 20 —Ç–æ–∫–µ–Ω–æ–≤): [2, 24, 14, 29, 22, 32, 14, 27, 31, 24, 14, 45, 18, 28, 37, 24, 14, 1342, 94, 186]\n",
      "–î–ª–∏–Ω–∞ –ø–µ—Ä–≤–æ–≥–æ —á–∞–Ω–∫–∞: 512 —Ç–æ–∫–µ–Ω–æ–≤\n",
      "–°–æ–∑–¥–∞–Ω –¥–∞—Ç–∞—Å–µ—Ç —Å 28818 –ø—Ä–∏–º–µ—Ä–∞–º–∏\n",
      "–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞: 512\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_chunk(texts, tokenizer, max_length=512):\n",
    "    \"\"\"\n",
    "    –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç—ã –∏ —Ä–∞–∑–±–∏–≤–∞–µ—Ç –Ω–∞ —á–∞–Ω–∫–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã\n",
    "    —Å –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º <bos> –∏ <eos> —Ç–æ–∫–µ–Ω–æ–≤\n",
    "    \"\"\"\n",
    "    all_input_ids = []\n",
    "    \n",
    "    for text in tqdm(texts):\n",
    "        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç\n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            truncation=False,\n",
    "            padding=False,\n",
    "            return_offsets_mapping=False,\n",
    "            add_special_tokens=False  # –¥–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –≤—Ä—É—á–Ω—É—é\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids']\n",
    "        \n",
    "        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏ —Å —É—á–µ—Ç–æ–º –º–µ—Å—Ç–∞ –¥–ª—è <bos> –∏ <eos>\n",
    "        chunk_size = max_length - 2  # –æ—Å—Ç–∞–≤–ª—è–µ–º –º–µ—Å—Ç–æ –¥–ª—è <bos> –∏ <eos>\n",
    "        \n",
    "        for i in range(0, len(input_ids), chunk_size):\n",
    "            chunk = input_ids[i:i + chunk_size]\n",
    "            \n",
    "            # –î–æ–±–∞–≤–ª—è–µ–º <bos> –∏ <eos>\n",
    "            chunk_with_special = [tokenizer.bos_token_id] + chunk + [tokenizer.eos_token_id]\n",
    "            \n",
    "            # –ï—Å–ª–∏ —á–∞–Ω–∫ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º\n",
    "            if len(chunk_with_special) >= 10:  # –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞\n",
    "                all_input_ids.append(chunk_with_special)\n",
    "    \n",
    "    return all_input_ids\n",
    "\n",
    "# –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –∏ —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏\n",
    "print(\"–ù–∞—á–∏–Ω–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –∏ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏...\")\n",
    "all_chunked_ids = tokenize_and_chunk(chunks, hf_tokenizer, max_length=512)\n",
    "\n",
    "print(f\"–°–æ–∑–¥–∞–Ω–æ {len(all_chunked_ids)} —á–∞–Ω–∫–æ–≤\")\n",
    "print(f\"–ü—Ä–∏–º–µ—Ä —á–∞–Ω–∫–∞ (–ø–µ—Ä–≤—ã–µ 20 —Ç–æ–∫–µ–Ω–æ–≤): {all_chunked_ids[0][:20]}\")\n",
    "print(f\"–î–ª–∏–Ω–∞ –ø–µ—Ä–≤–æ–≥–æ —á–∞–Ω–∫–∞: {len(all_chunked_ids[0])} —Ç–æ–∫–µ–Ω–æ–≤\")\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º Dataset\n",
    "def create_dataset(tokenized_chunks, tokenizer):\n",
    "    # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–æ 512\n",
    "    padded_sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    for chunk in tokenized_chunks:\n",
    "        # –ü–∞–¥–¥–∏–Ω–≥ –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã\n",
    "        if len(chunk) < 512:\n",
    "            padded = chunk + [tokenizer.pad_token_id] * (512 - len(chunk))\n",
    "        else:\n",
    "            padded = chunk[:512]\n",
    "        \n",
    "        # –î–ª—è language modeling labels —Ç–∞–∫–∏–µ –∂–µ –∫–∞–∫ input_ids\n",
    "        # –Ω–æ –æ–±—ã—á–Ω–æ —Å–º–µ—â–µ–Ω—ã –Ω–∞ 1 (–≤ DataCollator —ç—Ç–æ –¥–µ–ª–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)\n",
    "        padded_sequences.append(padded)\n",
    "        labels.append(padded)  # –±—É–¥–µ—Ç –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –≤ DataCollator\n",
    "    \n",
    "    dataset = Dataset.from_dict({\n",
    "        'input_ids': padded_sequences,\n",
    "        'labels': padded_sequences  # –≤—Ä–µ–º–µ–Ω–Ω–æ, –±—É–¥–µ—Ç –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ\n",
    "    })\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "train_dataset = create_dataset(all_chunked_ids, hf_tokenizer)\n",
    "print(f\"–°–æ–∑–¥–∞–Ω –¥–∞—Ç–∞—Å–µ—Ç —Å {len(train_dataset)} –ø—Ä–∏–º–µ—Ä–∞–º–∏\")\n",
    "print(f\"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞: {len(train_dataset[0]['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55e070f",
   "metadata": {},
   "source": [
    "### –≠—Ç–∞–ø 1.4: –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10bc4b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π:\n",
      "- –°–ª–æ–≤–∞—Ä—å: 3000 —Ç–æ–∫–µ–Ω–æ–≤\n",
      "- –°–∫—Ä—ã—Ç—ã–π —Ä–∞–∑–º–µ—Ä: 1024\n",
      "- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤: 16\n",
      "- –ì–æ–ª–æ–≤—ã –≤–Ω–∏–º–∞–Ω–∏—è: 16\n",
      "–ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞! –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: 132,006,912\n",
      "–ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–º–µ—â–µ–Ω–∞ –Ω–∞: cuda\n"
     ]
    }
   ],
   "source": [
    "# –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–∏ –∫–∞–∫ —É–∫–∞–∑–∞–Ω–æ –≤ –∑–∞–¥–∞–Ω–∏–∏\n",
    "model_config = LlamaConfig(\n",
    "    vocab_size=len(hf_tokenizer),\n",
    "    hidden_size=1024,\n",
    "    intermediate_size=1536,\n",
    "    num_hidden_layers=16,\n",
    "    num_attention_heads=16,\n",
    "    num_key_value_heads=8,\n",
    "    max_position_embeddings=512,\n",
    "    bos_token_id=hf_tokenizer.bos_token_id,\n",
    "    eos_token_id=hf_tokenizer.eos_token_id,\n",
    "    pad_token_id=hf_tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "print(\"–°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π:\")\n",
    "print(f\"- –°–ª–æ–≤–∞—Ä—å: {model_config.vocab_size} —Ç–æ–∫–µ–Ω–æ–≤\")\n",
    "print(f\"- –°–∫—Ä—ã—Ç—ã–π —Ä–∞–∑–º–µ—Ä: {model_config.hidden_size}\")\n",
    "print(f\"- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤: {model_config.num_hidden_layers}\")\n",
    "print(f\"- –ì–æ–ª–æ–≤—ã –≤–Ω–∏–º–∞–Ω–∏—è: {model_config.num_attention_heads}\")\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å\n",
    "model = AutoModelForCausalLM.from_config(model_config)\n",
    "print(f\"–ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞! –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {model.num_parameters():,}\")\n",
    "\n",
    "# –ü–µ—Ä–µ–º–µ—â–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"–ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–º–µ—â–µ–Ω–∞ –Ω–∞: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e69c879c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ input_ids: torch.Size([2, 512])\n",
      "–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ labels: torch.Size([2, 512])\n"
     ]
    }
   ],
   "source": [
    "# –°–æ–∑–¥–∞–µ–º Data Collator –¥–ª—è language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=hf_tokenizer,\n",
    "    mlm=False,  # Causal language modeling\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# –¢–µ—Å—Ç–∏—Ä—É–µ–º data collator\n",
    "batch = data_collator([train_dataset[0], train_dataset[1]])\n",
    "print(f\"–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ input_ids: {batch['input_ids'].shape}\")\n",
    "print(f\"–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ labels: {batch['labels'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5781622d",
   "metadata": {},
   "source": [
    "### –ö–æ–ª–±—ç–∫ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e38a316",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompts = [\n",
    "    \"–í—Å–µ –º—ã—Å–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ—é—Ç –æ–≥—Ä–æ–º–Ω—ã–µ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è\",\n",
    "    \"–°–∏–ª–∞ –≤–æ–π—Å–∫–∞ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –µ–≥–æ –¥—É—Ö–∞\", \n",
    "    \"–ú—ã—Å–ª—å –æ —Ç–æ–º, —á—Ç–æ –æ–Ω –ø—Ä–∏–Ω–µ—Å —Å—Ç—Ä–∞–¥–∞–Ω–∏—è\",\n",
    "    \"–ß–µ–ª–æ–≤–µ–∫ —Å–æ–∑–Ω–∞–µ—Ç —Å–µ–±—è —Å–≤–æ–±–æ–¥–Ω—ã–º\",\n",
    "    \"–ß—Ç–æ –±—ã –Ω–∏ —Å–ª—É—á–∏–ª–æ—Å—å, —è –≤—Å–µ–≥–¥–∞ –±—É–¥—É\",\n",
    "    \"–õ—é–±–æ–≤—å –º–µ—à–∞–µ—Ç —Å–º–µ—Ä—Ç–∏\",\n",
    "    \"–ù–µ—Ç, –∂–∏–∑–Ω—å –Ω–µ –∫–æ–Ω—á–µ–Ω–∞\",\n",
    "    \"–í—Å—è–∫–∞—è –º—ã—Å–ª—å, –¥–∞–∂–µ —Å–∞–º–∞—è –ø—Ä–æ—Å—Ç–∞—è\",\n",
    "    \"–í–æ–π–Ω–∞ –Ω–µ –ª—é–±–µ–∑–Ω–æ—Å—Ç—å, –∞ —Å–∞–º–æ–µ –≥–∞–¥–∫–æ–µ –¥–µ–ª–æ\",\n",
    "    \"–ß—Ç–æ–±—ã –∂–∏—Ç—å —á–µ—Å—Ç–Ω–æ\"\n",
    "]\n",
    "\n",
    "class GenerationCallback(TrainerCallback):\n",
    "    def __init__(self, tokenizer, prompts, eval_steps=500):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompts = prompts\n",
    "        self.eval_steps = eval_steps\n",
    "        \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % self.eval_steps == 0:\n",
    "            model = kwargs['model']\n",
    "            model.eval()\n",
    "            \n",
    "            print(f\"\\n=== –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞ —à–∞–≥–µ {state.global_step} ===\")\n",
    "            \n",
    "            for i, prompt in enumerate(self.prompts[:3]):  # –ø–æ–∫–∞–∂–µ–º —Ç–æ–ª—å–∫–æ 3 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –º–µ—Å—Ç–∞\n",
    "                inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "                inputs.pop('token_type_ids', None)  \n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=50,\n",
    "                        do_sample=True,\n",
    "                        temperature=0.8,\n",
    "                        pad_token_id=self.tokenizer.pad_token_id,\n",
    "                        eos_token_id=self.tokenizer.eos_token_id,\n",
    "                    )\n",
    "                \n",
    "                generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                print(f\"Prompt: {prompt}\")\n",
    "                print(f\"Generated: {generated_text}\")\n",
    "                print(\"---\")\n",
    "            \n",
    "            model.train()\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –∫–æ–ª–±—ç–∫\n",
    "generation_callback = GenerationCallback(hf_tokenizer, test_prompts, eval_steps=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8848ab90",
   "metadata": {},
   "source": [
    "### –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2f97673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch size: 64\n",
      "–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã!\n"
     ]
    }
   ],
   "source": [
    "# –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º batch size\n",
    "per_device_batch_size = 16  # –Ω–∞ –æ–¥–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ\n",
    "gradient_accumulation_steps = 4  # –∞–∫–∫—É–º—É–ª—è—Ü–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤\n",
    "effective_batch_size = per_device_batch_size * gradient_accumulation_steps\n",
    "\n",
    "print(f\"–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch size: {effective_batch_size}\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./pretrain_model\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,  # –¥–ª—è –Ω–∞—á–∞–ª–∞ 1 —ç–ø–æ—Ö–∞, –º–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å\n",
    "    per_device_train_batch_size=per_device_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=None,  # –æ—Ç–∫–ª—é—á–∞–µ–º wandb –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=False,\n",
    "    fp16=torch.cuda.is_available(),  # –∏—Å–ø–æ–ª—å–∑—É–µ–º mixed precision –µ—Å–ª–∏ –µ—Å—Ç—å GPU\n",
    ")\n",
    "\n",
    "print(\"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f999109d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='451' max='451' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [451/451 03:41, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>6.926500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>6.307900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>5.682900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>5.112600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>4.739500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.497100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>4.338900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>4.215200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>4.152100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞ —à–∞–≥–µ 200 ===\n",
      "Prompt: –í—Å–µ –º—ã—Å–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ—é—Ç –æ–≥—Ä–æ–º–Ω—ã–µ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è\n",
      "Generated: –í—Å–µ –º—ã—Å–ª–∏ , –∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ —é—Ç –æ–≥—Ä–æ–º –Ω—ã–µ –ø–æ—Å–ª–µ–¥ —Å—Ç–≤–∏—è —Å—Ç–∏ , –Ω–µ –≤ —Å–∏–ª–∞ —Ö –µ–≥–æ —Ä–æ –¥–Ω–æ–≥–æ –∏ –Ω–µ –∏–∑ —É–º–∏ –ª–∏ , –∏ —ç—Ç–æ –±—ã–ª–æ –≤–∏–¥–µ—Ç—å , –Ω–µ —Å—á–∞ —Å—Ç—å–µ –∏ –Ω–µ –ø—Ä–∏—è—Ç –Ω–∞—è –¥–æ —Ç–æ–≥–æ –æ–Ω –Ω–µ —Ö–æ—Ç–µ–ª –ø–æ–Ω—è—Ç—å , —á—Ç–æ —ç—Ç–æ –±—ã–ª–æ –æ–¥–Ω–æ —Ä–æ –¥–∞ –∏ –Ω–µ —Ä–∞–∑ –Ω–∏ –∫—É–¥–∞ –∏ –Ω–µ —Å–ª—ã —à–Ω–æ —É\n",
      "---\n",
      "Prompt: –°–∏–ª–∞ –≤–æ–π—Å–∫–∞ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –µ–≥–æ –¥—É—Ö–∞\n",
      "Generated: –°–∏ –ª–∞ –≤–æ–π —Å–∫–∞ –∑–∞–≤–∏ —Å–∏ —Ç –æ—Ç –µ–≥–æ –¥—É —Ö–∞ —é —Ä—É –∫–æ —Å—Ç–∏ , –ø—Ä–∏ –Ω–∞–¥ –ª–µ–∂–∞ –ª–æ , –∫–∞–∫ –±—ã –∑–∞ —á—Ç–æ - —Ç–æ –∏ –≤ –¥–æ–º–µ , –∞ —Ç–∞–∫ –∫–∞–∫ - —Ç–æ –ø–æ —Å–º–æ—Ç—Ä–∏ —Ç –î–∞ –Ω–∏ –≤ —Å–≤–æ—é —É –Ω–µ–≥–æ –Ω–µ –æ—Å—Ç–∞ –≤–∞–ª–æ—Å—å –ù–æ –∏ –Ω–µ –≤—ã –ø—É —Å–∫–∞ —é—Ç , –∫–∞–∫ - —Ç–æ –Ω–µ –≤—ã\n",
      "---\n",
      "Prompt: –ú—ã—Å–ª—å –æ —Ç–æ–º, —á—Ç–æ –æ–Ω –ø—Ä–∏–Ω–µ—Å —Å—Ç—Ä–∞–¥–∞–Ω–∏—è\n",
      "Generated: –ú—ã —Å –ª—å –æ —Ç–æ–º , —á—Ç–æ –æ–Ω –ø—Ä–∏ –Ω–µ—Å —Å—Ç—Ä–∞ –¥–∞–Ω–∏—è –≥–æ —á–µ–ª–æ–≤–µ–∫–∞ –∏ —Å –Ω–∏–º —É–∂–µ –Ω–∞ –∫–æ–ª–µ –Ω—è—Ö , –∏ , —Å —Ç–æ–≥–æ , —á—Ç–æ –≤—ã —à–ª–∏ –∏–∑ –≤–æ–ª–∏ –ª–∏ , –∫–æ–≥–¥–∞ –µ–≥–æ –¥–æ —Å—Ç–æ –∏–Ω —Å—Ç —Ä–æ–≤ , –≤ –ø–µ—Ä –≤–æ–π –Ω–µ –ª–µ –ø–∞ —è , –∏ –≤—Å–µ , –≤ —Ä–æ–¥–µ , —á—Ç–æ —ç—Ç–æ –±—ã–ª–æ , –ø–æ –≤–µ—Ä–∏\n",
      "---\n",
      "\n",
      "=== –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞ —à–∞–≥–µ 400 ===\n",
      "Prompt: –í—Å–µ –º—ã—Å–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ—é—Ç –æ–≥—Ä–æ–º–Ω—ã–µ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è\n",
      "Generated: –í—Å–µ –º—ã—Å–ª–∏ , –∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ —é—Ç –æ–≥—Ä–æ–º –Ω—ã–µ –ø–æ—Å–ª–µ–¥ —Å—Ç–≤–∏—è –Ω –∫ —Ç–µ –º–∞ —Ç–∏ —á–µ—Å–∫–æ–π —ç –∫–∏ –ø–∞ –∂ , –Ω–æ , –≤–µ—Ä–æ—è—Ç–Ω–æ , –∑–∞ –Ω–∏–º–∞–ª —Å—è –≤ —Å–µ–±–µ –≤—Å–µ—Ö –ª—é–¥–µ–π , –∏ –µ—â–µ –≤ —Ç–æ—Ç –∂–µ –¥–µ–Ω—å , –∫–æ–≥–¥–∞ –≤ —Å–∫–æ—Ä–µ –æ–± –≤–∏ –Ω—è –µ—Ç , –æ —á–∏ —Å—Ç–∏ –ª–æ –µ–≥–æ , - –∏ –≤ —Ç–æ –∂–µ –≤—Ä–µ–º—è\n",
      "---\n",
      "Prompt: –°–∏–ª–∞ –≤–æ–π—Å–∫–∞ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –µ–≥–æ –¥—É—Ö–∞\n",
      "Generated: –°–∏ –ª–∞ –≤–æ–π —Å–∫–∞ –∑–∞–≤–∏ —Å–∏ —Ç –æ—Ç –µ–≥–æ –¥—É —Ö–∞ —Å—Ç–∏ , –≥–¥–µ –∏ –≤ —Å–∞–º–æ–º –¥–µ–ª–µ –æ–Ω–∞ –ø–æ —Å–º–æ—Ç—Ä–µ–ª–∞ –Ω–∞ –Ω–µ–≥–æ , –Ω–µ –≤—ã –ø—É —Å–∫–∞—è , –≤ –∫–æ—Ç–æ—Ä—É—é –æ–Ω –ø—Ä–æ –±–µ–∂–∞–ª –≤ –∑–∞–ª —É , –æ–Ω –æ–ø—è—Ç—å –∑–∞ —Å–º–µ—è–ª—Å—è , –≥–ª—è–¥—è –Ω–∞ –Ω–µ–≥–æ –≤ –≥–ª–∞–∑–∞ , —á—Ç–æ –µ–º—É –±—ã–ª–æ , –∞ –°–∞–º–≥–∏–Ω –ø–æ—à–µ–ª –∫ –Ω–µ–º—É , –ø—Ä–∏ —Å–ª–æ\n",
      "---\n",
      "Prompt: –ú—ã—Å–ª—å –æ —Ç–æ–º, —á—Ç–æ –æ–Ω –ø—Ä–∏–Ω–µ—Å —Å—Ç—Ä–∞–¥–∞–Ω–∏—è\n",
      "Generated: –ú—ã —Å –ª—å –æ —Ç–æ–º , —á—Ç–æ –æ–Ω –ø—Ä–∏ –Ω–µ—Å —Å—Ç—Ä–∞ –¥–∞–Ω–∏—è —Å –ª—è —Ç–∏ —á–µ—Å–∫–∏–π —Å –±–æ –≥–æ–º , –∫–∞–∫ –∏ –æ–Ω —Ç–µ–ø–µ—Ä—å , –∫–æ–≥–¥–∞ , –∫–æ–≥–¥–∞ –≤—Å–µ –ø–µ—Ä–µ —Å—Ç–∞ –Ω–æ–≤ –ª–µ –Ω—ã , –∑–∞ –ø—Ä–µ —â–µ–Ω–∏—è –æ —Ç–æ–º , —á—Ç–æ –æ–Ω–æ –Ω–µ —Ä–∞–∑ –±—ã–ª–æ –≤ –Ω–∏—Ö –∏ –≤ –æ–±—â–µ —Å—Ç–≤–µ , —Ö–æ—Ç—è –±—ã –∏ –Ω–µ –±—ã–ª–æ , -- —ç—Ç–æ –±—ã–ª–æ\n",
      "---\n",
      "–ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!\n"
     ]
    }
   ],
   "source": [
    "# –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    callbacks=[generation_callback],\n",
    ")\n",
    "\n",
    "print(\"–ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...\")\n",
    "trainer.train()\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è\n",
    "trainer.save_model(\"./pretrain_model_final\")\n",
    "hf_tokenizer.save_pretrained(\"./pretrain_model_final\")\n",
    "print(\"–ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9755b075",
   "metadata": {},
   "source": [
    "### –≠—Ç–∞–ø 1.5: –§–∏–Ω–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏ –æ—Ü–µ–Ω–∫–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4401966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== –§–ò–ù–ê–õ–¨–ù–ê–Ø –ì–ï–ù–ï–†–ê–¶–ò–Ø –ù–ê TEST_PROMPTS ===\n",
      "Prompt: –í—Å–µ –º—ã—Å–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ—é—Ç –æ–≥—Ä–æ–º–Ω—ã–µ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è\n",
      "Generated: –í—Å–µ –º—ã—Å–ª–∏ , –∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ —é—Ç –æ–≥—Ä–æ–º –Ω—ã–µ –ø–æ—Å–ª–µ–¥ —Å—Ç–≤–∏—è —Å–µ –Ω–∏–π –∏ –Ω–∞ —Ö–æ–¥—è —Ç –≤ —ç—Ç–æ—Ç –¥–µ–Ω—å , –Ω–∞ –∫–æ—Ç–æ—Ä–æ–π –±—ã–ª–∏ –∑–∞ –º–∞ —Ä–∞ –Ω—ã , –Ω–æ –æ–Ω–∏ –Ω–µ –∑–Ω–∞ –ª–∏ , —á—Ç–æ –≤—Å–µ –∑–∞ –º—ã —à–ª—è —é—Ç –∏ –ø–æ —ç—Ç–æ–º—É –µ–º—É –ø–æ–¥ —Ä—É –≥–∞ –ø—Ä–∏ –æ–±—Ä–µ —Ç–µ –Ω–æ , –∫–∞–∫ –∏ –Ω–µ –≤—ã –Ω–æ—Å–∏ –º—ã–π , –Ω–æ –∏ —Ç–æ , —á—Ç–æ –∑–∞ —Ö–æ—á–µ—Ç , –∞ –∫–∞–∫ - —Ç–æ –Ω–µ –ø–æ –Ω–∏–º –∞–ª–∏ , –∫–∞–∫ –∏ –æ–Ω —É —Å—Ç—Ä–æ –∏—Ç—å , –∏ –Ω–µ –ø–æ –ª—é–±–∏ –ª–∞ , –∏ –≤ —ç—Ç–æ–º —Å–ª—É—á–∞–µ , –∫–æ–≥–¥–∞ –æ–Ω –≥–æ–≤–æ—Ä–∏–ª , —á—Ç–æ , –∑–∞ —Ç–æ , —á—Ç–æ –æ–Ω –≤—ã –±–µ –∂–∏—Ç –∏ –∑–∞\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: –°–∏–ª–∞ –≤–æ–π—Å–∫–∞ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –µ–≥–æ –¥—É—Ö–∞\n",
      "Generated: –°–∏ –ª–∞ –≤–æ–π —Å–∫–∞ –∑–∞–≤–∏ —Å–∏ —Ç –æ—Ç –µ–≥–æ –¥—É —Ö–∞ –ø –Ω—É—Ç—å , —É —Ä–æ–¥ –ª–∏ –≤—ã–µ , –≤ –º–µ —à–∞ —é—â–∏–µ –∏ –æ —Å–∞ –Ω–∏ –∏ —É –∫–∞–∑–∞ –Ω–∏—è ; –Ω–æ –Ω–µ –≤ —Ç–æ –≤—Ä–µ–º—è , –∫–∞–∫ —è , –Ω–µ —É–º–µ —Å—Ç –Ω–∞—è , –ø—Ä–∏ –≤–µ –ª–∞ –µ–≥–æ –≤ —Å–µ–±–µ –Ω–∞ —à–∏—Ö –∫–æ –º–∏ —Ç–µ —Ç–∞—Ö , –∏ , –Ω–µ –∑–∞ –±—É–¥—å –≤ –Ω–µ–º , —è –Ω–µ –º–æ–≥—É , —á—Ç–æ–± –≤—ã —Å —à–µ —Å—Ç—å —á–∞—Å–æ–≤ –Ω–∏ —É –¥–µ—Ä–∂–∏ –≤–∞—Ç—å , –∏ , –≤—ã –ª–∏ –≤–∞—è—Å—å –∏–∑ –¥–æ–º—É , —è —Ä–æ —Å—Ç–∏ , –ø—Ä–∏ —â—É —Ä–∏ –ª–∞—Å—å , —Å–æ —Å —Ä–µ –¥–æ —Ç–æ —á–∏ –ª–∞—Å—å –≤ –∑–∞ –∏–º –Ω—É—é ,\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: –ú—ã—Å–ª—å –æ —Ç–æ–º, —á—Ç–æ –æ–Ω –ø—Ä–∏–Ω–µ—Å —Å—Ç—Ä–∞–¥–∞–Ω–∏—è\n",
      "Generated: –ú—ã —Å –ª—å –æ —Ç–æ–º , —á—Ç–æ –æ–Ω –ø—Ä–∏ –Ω–µ—Å —Å—Ç—Ä–∞ –¥–∞–Ω–∏—è —Å—Ç—å –Ω–∞ –≥—Ä–∞ —Ñ–∞ , –Ω–æ —á—Ç–æ –æ–Ω –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å , –∏ –Ω–µ —Ö–æ—Ç–µ–ª –±—ã —Å–∫–∞–∑–∞—Ç—å –æ —Å–≤–æ–µ–º , –Ω–æ –≤ –Ω–µ–º , –∏ –æ–Ω –Ω–µ –º–æ–≥ –±—ã —Å–∫–∞–∑–∞—Ç—å —ç—Ç–æ ; –Ω–æ –Ω–µ –º–æ–≥ –æ–Ω –¥—É–º–∞—Ç—å –æ —Ç–æ–º , —á—Ç–æ –æ–Ω —Å —á–∏—Ç–∞–ª –µ–≥–æ , –∏ –æ–Ω —Å–∞–º –Ω–µ –∑–Ω–∞–ª , –∫–∞–∫ –æ–Ω —Å–∞–º –Ω–µ –º–æ–≥ –±—ã—Ç—å , –Ω–µ –∑–Ω–∞–ª –∏ –Ω–µ –∏—Å–ø—ã —Ç—ã –≤–∞–µ—Ç –µ–º—É –∏ –Ω–µ –±—ã–ª–æ —Ç–æ–≥–æ , —á—Ç–æ –æ–Ω –∑–∞ –±—ã–ª –Ω–∞ –Ω–µ–π , –∏ –æ–Ω –∑–Ω–∞–ª , —á—Ç–æ –æ–Ω —Å–∞–º –Ω–µ –º–æ–≥ –Ω–µ —Å–∫–∞–∑–∞—Ç—å –û–Ω –¥–∞–∂–µ —Å —á–∏—Ç–∞–ª –µ–≥–æ –Ω–µ –≤ —Ç–æ–º , —á—Ç–æ\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: –ß–µ–ª–æ–≤–µ–∫ —Å–æ–∑–Ω–∞–µ—Ç —Å–µ–±—è —Å–≤–æ–±–æ–¥–Ω—ã–º\n",
      "Generated: –ß–µ –ª–æ–≤–µ –∫ —Å–æ –∑–Ω–∞–µ—Ç —Å–µ–±—è —Å–≤–æ–±–æ –¥–Ω—ã–º –Ω–æ–º , –∞ –æ–Ω–∏ –µ–≥–æ —Ä–∞–∑ –¥–∞ —é—Ç , –∞ –æ–Ω –∏ –Ω–∞ —ç—Ç–æ—Ç —Ä–∞–∑ –æ–± —Ä–∞–ª , –∞ –≤–æ—Ç –æ–Ω–∏ –ø–æ –∫–æ—Ä –Ω–æ , –∏ –Ω–µ —É –ø–∞–ª –±—ã , –¥–∞ –∏ –Ω–µ —Ç–∞–∫ –ª–∏ , –∫–∞–∫ –∏ —É –Ω–∏—Ö -- –Ω–µ –ø–æ - –º–æ–µ–º—É , –Ω–µ —Ç—É , -- –∑–∞–º–µ—Ç–∏–ª –ê —Ñ —Ä–∞ -- –ê —á—Ç–æ , -- —Å–∫–∞–∑–∞–ª –æ–Ω , -- –∞ –Ω–µ —á—Ç–æ -- –¢–∞–∫ -- –≤–æ —Å–µ–º—å —á–∞—Å–æ–≤ -- –≠—Ç–æ -- –Ω–µ —Ç–æ , –∫–∞–∫ —Ç—ã -- –î–∞ , –¥–∞ , —è –∑–Ω–∞—é , -- —Å–∫–∞–∑–∞–ª –æ–Ω , -- –∏ –≤–æ—Ç –º–Ω–µ , —á—Ç–æ —Ç—ã\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: –ß—Ç–æ –±—ã –Ω–∏ —Å–ª—É—á–∏–ª–æ—Å—å, —è –≤—Å–µ–≥–¥–∞ –±—É–¥—É\n",
      "Generated: –ß—Ç–æ –±—ã –Ω–∏ —Å–ª—É—á–∏–ª–æ—Å—å , —è –≤—Å–µ–≥–¥–∞ –±—É–¥—É –≤–∞ , —á—Ç–æ –æ–Ω –∑–∞ –±—ã–ª –Ω–∞ –Ω–µ–≥–æ , –∏ –Ω–µ –º–æ–≥ –±—ã –ø–µ—Ä–µ –≤–µ—Å—Ç–∏ –µ–≥–æ : –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ —è –Ω–µ –∑–Ω–∞—é –∏ –Ω–µ –∑–Ω–∞–ª , —á—Ç–æ –æ–Ω –º–æ–∂–µ—Ç —Å–¥–µ–ª–∞—Ç—å –Ø —Å–∞–º –Ω–µ –ª—é–±–ª—é , –∞ —è —Å –≤–∞–º–∏ , –Ω–æ –æ–Ω–∞ –Ω–µ –≤ —Å–∏–ª–∞ —Ö –ø–æ —Å—Ç–∞–≤–∏—Ç—å –µ–≥–æ , –Ω–µ —Ö–æ—á—É –ø–æ–Ω—è—Ç—å –∏ –Ω–µ –º–æ–≥–ª–∞ , –∏ –Ω–µ –∑–Ω–∞–ª , —á—Ç–æ –µ—Å–ª–∏ –±—ã –Ω–µ –±—ã–ª–æ , —Ç–æ —è –Ω–∏ —Å–∫–æ–ª—å–∫–æ –Ω–µ –ø–æ–Ω–∏–º–∞—é , —è –Ω–µ –ø–æ–Ω–∏–º–∞—é , –∫–∞–∫ –æ–Ω–∞ –Ω–µ –º–æ–≥ –∑–∞ –±—ã—Ç—å , –Ω–æ —è –Ω–µ –≤–∏–Ω–æ–≤–∞ —Ç , –ø–æ—Ç–æ–º—É —á—Ç–æ —Ç—ã –Ω–µ –º–æ –∂–µ—à—å –º–µ–Ω—è –ø–æ\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: –õ—é–±–æ–≤—å –º–µ—à–∞–µ—Ç —Å–º–µ—Ä—Ç–∏\n",
      "Generated: –õ—é –±–æ –≤—å –º–µ —à–∞ –µ—Ç —Å–º–µ—Ä—Ç–∏ –º—É ; –∞ —Ç–æ –∏ –æ–Ω–∞ —Å –ø—É—Å—Ç–∏ –ª–∞—Å—å , —á—Ç–æ —è –±—ã–ª–∞ –≤ –ø–µ—Ä –≤—ã–µ –ø–æ–ª–æ –≤–∏–Ω–∞ , –≤ —á–µ–º –æ–Ω–∞ —É —à–ª–∞ –Ø —É–∂–µ –±—ã–ª–∞ –æ—á–µ–Ω—å –º–Ω–æ–≥–æ , –∏ –≤ –ø–æ—Å–ª–µ–¥ –Ω–µ–µ –≤—Ä–µ–º—è , –≤ –∫–æ—Ç–æ—Ä–æ–π –æ–Ω–∞ –Ω–µ –º–æ–≥–ª–∞ –Ω–µ –≤–∏–¥–µ—Ç—å , –Ω–æ –ø–æ —Å–µ–±–µ –º–Ω–µ –Ω–µ –±—ã–ª–æ –Ω–∞ —Å–≤–µ—Ç–µ –û–Ω–∞ —É —Å—Ç–∞–ª–∞ , –Ω–µ –∂–µ–ª–∞—è –Ω–∞ –π—Ç–∏ —Å—å , —á—Ç–æ –µ–µ –∂–µ–Ω–∞ –±—ã–ª–∞ –æ—á–µ–Ω—å –¥–æ–ª–≥–æ , –Ω–æ –æ–Ω–∞ –Ω–µ –º–æ–≥–ª–∞ –ø–æ–Ω—è—Ç—å , –∫–∞–∫ –æ–Ω–∞ –∏ –Ω–µ –º–æ–≥–ª–∞ –±—ã—Ç—å –∑–∞ –Ω–µ—é , —á—Ç–æ –æ–Ω –±—ã–ª —Ç–∞–∫ , –∫–∞–∫ –æ–Ω–∞ –±—ã–ª–∞ –Ω–µ –≤—ã –¥–µ—Ä –∂–∞–ª–∞ , –Ω–æ –≤—Å–µ -\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: –ù–µ—Ç, –∂–∏–∑–Ω—å –Ω–µ –∫–æ–Ω—á–µ–Ω–∞\n",
      "Generated: –ù–µ—Ç , –∂–∏–∑–Ω—å –Ω–µ –∫–æ–Ω —á–µ –Ω–∞ , –Ω–µ –∑–Ω–∞—é , —á—Ç–æ —è –Ω–µ –º–æ–≥—É –∑–Ω–∞—Ç—å , —á—Ç–æ —É –≤–∞—Å —É –Ω–∞—Å –Ω–µ—Ç , –∏ —Ç–∞–∫ , –∫–∞–∫ —ç—Ç–æ –±—ã–ª–æ , –∏ –µ—Å–ª–∏ –æ–Ω –Ω–µ –º–æ–∂–µ—Ç –ø–æ –¥—É–º–∞—Ç—å , –Ω–µ –ø–æ –Ω–∏–º–∞–ª —è —Å –≤–∞–º–∏ , —Ç–æ –∏ –Ω–µ —Ö–æ—Ç–µ–ª –±—ã –ø–æ–Ω—è—Ç—å , —á—Ç–æ –∏ —Ç–µ–ø–µ—Ä—å —É –Ω–µ–≥–æ –Ω–∏—á–µ–≥–æ –Ω–µ –±—ã–ª–æ –û–Ω–∞ –Ω–µ –º–æ–≥–ª–∞ –≤—ã –≥–æ–≤–æ—Ä–∏—Ç—å , –∏ —è —É–∂–µ –Ω–µ —Ö–æ—Ç–µ–ª –∑–∞ –±—ã—Ç—å , –Ω–æ —è –Ω–µ —Ö–æ—Ç–µ–ª–∞ –ø–æ–Ω—è—Ç—å , —á—Ç–æ –æ–Ω , –µ—Å–ª–∏ –Ω–µ –æ –ø–æ–º–Ω–∏ –ª—Å—è , —á—Ç–æ —ç—Ç–æ –Ω–µ —Ç–∞–∫ , —á—Ç–æ –±—ã–ª–æ –±—ã –Ω–µ —Ç–∞–∫ , —á—Ç–æ –æ–Ω –Ω–µ –∑–Ω–∞–µ—Ç , –ø–æ—Ç–æ–º—É\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: –í—Å—è–∫–∞—è –º—ã—Å–ª—å, –¥–∞–∂–µ —Å–∞–º–∞—è –ø—Ä–æ—Å—Ç–∞—è\n",
      "Generated: –í —Å—è –∫–∞—è –º—ã—Å–ª—å , –¥–∞–∂–µ —Å–∞–º–∞ —è –ø—Ä–æ —Å—Ç–∞—è –∏ –Ω–µ –ø—Ä–æ —à–µ–ø —Ç–∞–ª–∞ : -- –≠—Ç–æ –º–Ω–µ –Ω—É–∂–Ω–æ ; –∞ —è , –ø–æ–∂–∞–ª—É–π , –Ω–µ –º–æ–≥—É , –Ω–µ –≤ —Ç–æ–º , —á—Ç–æ —è –≤–∞—Å –ª—é–±–∏ –ª–∞ ; –≤—ã –Ω–µ –ø–æ–Ω–∏–º–∞ –µ—Ç–µ -- –≠—Ç–æ —É–∂–∞—Å–Ω–æ -- –ê —á—Ç–æ —è –≤–∞–º —Å–∫–∞–∑–∞–ª , —á—Ç–æ –≤—ã –Ω–µ –º–æ –∂–µ—Ç–µ –±—ã—Ç—å -- –î–∞ , –¥–∞ , -- —Å–∫–∞–∑–∞–ª–∞ –æ–Ω–∞ , -- –∏ –∫–∞–∫ –±—ã —è –Ω–µ –∑–Ω–∞–ª–∞ , —á–µ–≥–æ –≤—ã –Ω–µ —É –∑–Ω–∞—é , —á—Ç–æ –≤—ã –º–µ–Ω—è –ø—Ä–∏ —à–ª–∏ , -- –ø—Ä–∏–±–∞ –≤–∏–ª–∞ –æ–Ω–∞ , –ø—Ä–∏ –∂–∞–ª–∞ –∫ –Ω–µ–º—É -- –Ø –∑–Ω–∞—é , —á—Ç–æ –Ω–µ –±—É–¥—É , -- —Å–∫–∞–∑–∞–ª–∞ –æ–Ω–∞ , --\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: –í–æ–π–Ω–∞ –Ω–µ –ª—é–±–µ–∑–Ω–æ—Å—Ç—å, –∞ —Å–∞–º–æ–µ –≥–∞–¥–∫–æ–µ –¥–µ–ª–æ\n",
      "Generated: –í–æ –π –Ω–∞ –Ω–µ –ª—é–±–µ–∑ –Ω–æ —Å—Ç—å , –∞ —Å–∞–º–æ–µ –≥–∞ –¥ –∫–æ–µ –¥–µ–ª–æ —Ö–æ , –∞ –ø–æ—Ç–æ–º –∏ —Å–æ —Ä –≤–∞—Ç—å –Ω–∞ –∑–∞ —Ç—ã –ª –∫–µ , –∏ —ç—Ç–æ —è —Å–µ–π—á–∞—Å –∏ –¥–æ –ª–æ –∂—É , –∫–∞–∫ —è –∏ –Ω–µ –∑–Ω–∞—é , –ø–æ—Ç–æ–º—É —á—Ç–æ —è , –º–æ–∂–µ—Ç –±—ã—Ç—å , –ø—Ä–æ —á —Ç—É –∏ –Ω–µ –º–æ–≥—É –ø–æ –¥—É–º–∞—Ç—å , —á—Ç–æ —Ç—ã –Ω–µ –ø–æ –≤–µ—Ä–∏ —à—å , –∏ –Ω–µ –∑–Ω–∞—é , —á—Ç–æ–± —Ç—ã –Ω–∞ –ø–∏—Å –∞–ª–∞ , —á—Ç–æ —è , —Ç—ã –º–Ω–µ –Ω–µ –∑–Ω–∞—é , –ø–æ—á–µ–º—É —Ç—ã –Ω–µ –ø–æ —Å–ª—É —à –∞–ª—Å—è , –Ω–µ –ø–æ –≤–µ—Ä–∏ —à—å –º–µ–Ω—è , -- —Å–∫–∞–∑–∞–ª–∞ –æ–Ω–∞ , –æ–±—Ä–∞ —â–∞—è—Å—å –∫ –∫–Ω—è –∑ —é , -- –Ω–æ —è –Ω–µ –º–æ–≥—É\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: –ß—Ç–æ–±—ã –∂–∏—Ç—å —á–µ—Å—Ç–Ω–æ\n",
      "Generated: –ß—Ç–æ –±—ã –∂–∏—Ç—å —á–µ —Å—Ç–Ω–æ , —Ç–æ –∏ –Ω–µ –≤ —Å—Ç—É–ø–∞ —Ç—å , –Ω–µ –±—ã–ª–æ –±—ã –¥–ª—è —Ç–æ–≥–æ , —á—Ç–æ–±—ã –Ω–µ —É –¥–µ—Ä–∂–∞—Ç—å , –∏ —Å –æ—Ç —Ü–æ–º , –∏ –Ω–µ –æ—Ç —ä–µ–∑ –¥–∞ –Ω–∞ –Ω–µ–≥–æ , –Ω–æ –∏ –≤ —É –µ–¥–∏ –Ω–µ –Ω–∏–∏ , –∏ , –Ω–∞–∫–æ–Ω–µ—Ü , —Å –Ω–µ—é , –∫–æ–≥–¥–∞ , –µ—Å–ª–∏ –±—ã –æ–Ω –±—ã–ª –±—ã –Ω–µ –ø–æ –Ω–∏–º –∞–ª , –∫–æ–≥–¥–∞ –æ–Ω –ø—Ä–æ –±–æ –≤–∞–ª , –∏ –æ–Ω –¥–æ–ª–∂–µ–Ω –±—ã–ª , —á—Ç–æ —ç—Ç–æ —Ç–∞–∫ –Ω–µ —Ç–∞–∫ , —Ç–∞–∫ —Å–∫–∞–∑–∞—Ç—å , –∏ , –∫–∞–∫ –±—ã –≤ —Ç–∞ –∫–æ–º —Å–ª—É—á–∞–µ , –Ω–µ —É –∑–Ω–∞—é , –Ω–µ –≤ —á–µ–º , –Ω–µ –≤ —ç—Ç–æ–º –¥–µ–ª–µ\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def generate_texts(model, tokenizer, prompts, max_new_tokens=100):\n",
    "    model.eval()\n",
    "    generated_texts = []\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        inputs.pop('token_type_ids', None) \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        generated_texts.append(generated_text)\n",
    "        \n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"Generated: {generated_text}\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    return generated_texts\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å (–µ—Å–ª–∏ —Å–æ—Ö—Ä–∞–Ω—è–ª–∏ —á–µ–∫–ø–æ–∏–Ω—Ç—ã)\n",
    "# –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â—É—é –æ–±—É—á–µ–Ω–Ω—É—é\n",
    "final_model = model\n",
    "\n",
    "print(\"=== –§–ò–ù–ê–õ–¨–ù–ê–Ø –ì–ï–ù–ï–†–ê–¶–ò–Ø –ù–ê TEST_PROMPTS ===\")\n",
    "final_generations = generate_texts(final_model, hf_tokenizer, test_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2b8ab2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "–í–´–í–û–î–´ –ü–û PRETRAIN –≠–¢–ê–ü–£\n",
      "================================================================================\n",
      "\n",
      "‚úÖ –î–û–°–¢–ò–ñ–ï–ù–ò–Ø:\n",
      "‚Ä¢ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è\n",
      "‚Ä¢ –°–æ–∑–¥–∞–Ω —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π BPE-—Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (3000 —Ç–æ–∫–µ–Ω–æ–≤) –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞\n",
      "‚Ä¢ –ú–æ–¥–µ–ª—å —É—Å–≤–æ–∏–ª–∞ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å –∏ —Å—Ç–∏–ª–∏—Å—Ç–∏–∫—É —Ä—É—Å—Å–∫–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã\n",
      "\n",
      "üìä –ö–ê–ß–ï–°–¢–í–û –ì–ï–ù–ï–†–ê–¶–ò–ò:\n",
      "‚Ä¢ –°–≤—è–∑–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞: –í–´–°–û–ö–ê–Ø - –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã\n",
      "‚Ä¢ –°—Ç–∏–ª–∏—Å—Ç–∏–∫–∞: –•–û–†–û–®–ê–Ø - —É–∑–Ω–∞–≤–∞–µ–º—ã–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã–π —Å—Ç–∏–ª—å\n",
      "‚Ä¢ –ì—Ä–∞–º–º–∞—Ç–∏–∫–∞: –ü–†–ò–ï–ú–õ–ï–ú–ê–Ø - –µ—Å—Ç—å –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏\n",
      "‚Ä¢ –ö–æ–Ω—Ç–µ–∫—Å—Ç: –°–†–ï–î–ù–Ø–Ø - –º–æ–¥–µ–ª—å —Å–ª–µ–¥—É–µ—Ç —Ç–µ–º–µ, –Ω–æ —Ç–µ—Ä—è–µ—Ç —Ñ–æ–∫—É—Å\n",
      "\n",
      "üéØ –û–°–û–ë–ï–ù–ù–û–°–¢–ò:\n",
      "‚Ä¢ –ú–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–∏–∞–ª–æ–≥–∏ —Å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–º–∏ —Ä–µ–ø–ª–∏–∫–∞–º–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π\n",
      "‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–∏–ø–∏—á–Ω—ã–µ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ (—Ç–∏—Ä–µ, –≤–≤–æ–¥–Ω—ã–µ —Å–ª–æ–≤–∞)\n",
      "‚Ä¢ –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—É—é –æ–∫—Ä–∞—Å–∫—É –≤—ã—Å–∫–∞–∑—ã–≤–∞–Ω–∏–π\n",
      "\n",
      "‚ö†Ô∏è –û–ì–†–ê–ù–ò–ß–ï–ù–ò–Ø:\n",
      "‚Ä¢ –ö–æ—Ä–æ—Ç–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (512 —Ç–æ–∫–µ–Ω–æ–≤) –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –≥–ª—É–±–∏–Ω—É –ø–æ–≤–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è\n",
      "‚Ä¢ –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ (—Ä–∞–∑—Ä—ã–≤—ã —Å–ª–æ–≤)\n",
      "‚Ä¢ –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –≤ –¥–ª–∏–Ω–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏—è—Ö\n",
      "\n",
      "üí° –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï:\n",
      "Pretrain —ç—Ç–∞–ø —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω - –º–æ–¥–µ–ª—å –Ω–∞—É—á–∏–ª–∞—Å—å —Å—Ç—Ä—É–∫—Ç—É—Ä–µ —Ä—É—Å—Å–∫–æ–≥–æ\n",
      "–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–æ–≥–æ —è–∑—ã–∫–∞ –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Ç–µ–∫—Å—Ç—ã\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"–í–´–í–û–î–´ –ü–û PRETRAIN –≠–¢–ê–ü–£\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n‚úÖ –î–û–°–¢–ò–ñ–ï–ù–ò–Ø:\")\n",
    "print(\"‚Ä¢ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è\")\n",
    "print(\"‚Ä¢ –°–æ–∑–¥–∞–Ω —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π BPE-—Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (3000 —Ç–æ–∫–µ–Ω–æ–≤) –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞\")\n",
    "print(\"‚Ä¢ –ú–æ–¥–µ–ª—å —É—Å–≤–æ–∏–ª–∞ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å –∏ —Å—Ç–∏–ª–∏—Å—Ç–∏–∫—É —Ä—É—Å—Å–∫–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã\")\n",
    "\n",
    "print(\"\\nüìä –ö–ê–ß–ï–°–¢–í–û –ì–ï–ù–ï–†–ê–¶–ò–ò:\")\n",
    "print(\"‚Ä¢ –°–≤—è–∑–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞: –í–´–°–û–ö–ê–Ø - –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã\")\n",
    "print(\"‚Ä¢ –°—Ç–∏–ª–∏—Å—Ç–∏–∫–∞: –•–û–†–û–®–ê–Ø - —É–∑–Ω–∞–≤–∞–µ–º—ã–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã–π —Å—Ç–∏–ª—å\")\n",
    "print(\"‚Ä¢ –ì—Ä–∞–º–º–∞—Ç–∏–∫–∞: –ü–†–ò–ï–ú–õ–ï–ú–ê–Ø - –µ—Å—Ç—å –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏\")\n",
    "print(\"‚Ä¢ –ö–æ–Ω—Ç–µ–∫—Å—Ç: –°–†–ï–î–ù–Ø–Ø - –º–æ–¥–µ–ª—å —Å–ª–µ–¥—É–µ—Ç —Ç–µ–º–µ, –Ω–æ —Ç–µ—Ä—è–µ—Ç —Ñ–æ–∫—É—Å\")\n",
    "\n",
    "print(\"\\nüéØ –û–°–û–ë–ï–ù–ù–û–°–¢–ò:\")\n",
    "print(\"‚Ä¢ –ú–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–∏–∞–ª–æ–≥–∏ —Å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–º–∏ —Ä–µ–ø–ª–∏–∫–∞–º–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π\")\n",
    "print(\"‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–∏–ø–∏—á–Ω—ã–µ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ (—Ç–∏—Ä–µ, –≤–≤–æ–¥–Ω—ã–µ —Å–ª–æ–≤–∞)\")\n",
    "print(\"‚Ä¢ –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—É—é –æ–∫—Ä–∞—Å–∫—É –≤—ã—Å–∫–∞–∑—ã–≤–∞–Ω–∏–π\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è –û–ì–†–ê–ù–ò–ß–ï–ù–ò–Ø:\")\n",
    "print(\"‚Ä¢ –ö–æ—Ä–æ—Ç–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (512 —Ç–æ–∫–µ–Ω–æ–≤) –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –≥–ª—É–±–∏–Ω—É –ø–æ–≤–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è\")\n",
    "print(\"‚Ä¢ –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ (—Ä–∞–∑—Ä—ã–≤—ã —Å–ª–æ–≤)\")\n",
    "print(\"‚Ä¢ –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –≤ –¥–ª–∏–Ω–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏—è—Ö\")\n",
    "\n",
    "print(\"\\nüí° –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï:\")\n",
    "print(\"Pretrain —ç—Ç–∞–ø —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω - –º–æ–¥–µ–ª—å –Ω–∞—É—á–∏–ª–∞—Å—å —Å—Ç—Ä—É–∫—Ç—É—Ä–µ —Ä—É—Å—Å–∫–æ–≥–æ\")\n",
    "print(\"–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–æ–≥–æ —è–∑—ã–∫–∞ –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Ç–µ–∫—Å—Ç—ã\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ef7f65",
   "metadata": {},
   "source": [
    "## **–ß–ê–°–¢–¨ 2: Post-train SFT**\n",
    "\n",
    "### –≠—Ç–∞–ø 2.1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1468d40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç d0rj/alpaca-cleaned-ru...\n",
      "–î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'instruction', 'output'],\n",
      "        num_rows: 51760\n",
      "    })\n",
      "})\n",
      "–î–∞—Ç–∞—Å–µ—Ç –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω –≤ –¥–∏–∞–ª–æ–≥–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç\n",
      "[{'content': '–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.', 'role': 'system'}, {'content': '–î–∞–π—Ç–µ —Ç—Ä–∏ —Å–æ–≤–µ—Ç–∞, –∫–∞–∫ –æ—Å—Ç–∞–≤–∞—Ç—å—Å—è –∑–¥–æ—Ä–æ–≤—ã–º.', 'role': 'user'}, {'content': '1. –°–æ–±–ª—é–¥–∞–π—Ç–µ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏ –ø–∏—Ç–∞—Ç–µ–ª—å–Ω—É—é –¥–∏–µ—Ç—É. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤ –≤–∞—à —Ä–∞—Ü–∏–æ–Ω –≤—Ö–æ–¥—è—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ —Ñ—Ä—É–∫—Ç—ã –∏ –æ–≤–æ—â–∏, –Ω–µ–∂–∏—Ä–Ω—ã–π –±–µ–ª–æ–∫, —Ü–µ–ª—å–Ω–æ–∑–µ—Ä–Ω–æ–≤—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã –∏ –ø–æ–ª–µ–∑–Ω—ã–µ –∂–∏—Ä—ã. –≠—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –æ–±–µ—Å–ø–µ—á–∏—Ç—å –≤–∞—à –æ—Ä–≥–∞–Ω–∏–∑–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–º–∏ –ø–∏—Ç–∞—Ç–µ–ª—å–Ω—ã–º–∏ –≤–µ—â–µ—Å—Ç–≤–∞–º–∏ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –º–æ–∂–µ—Ç –ø–æ–º–æ—á—å –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å —Ö—Ä–æ–Ω–∏—á–µ—Å–∫–∏–µ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—è.\\n\\n2. –ó–∞–Ω–∏–º–∞–π—Ç–µ—Å—å —Ä–µ–≥—É–ª—è—Ä–Ω–æ–π —Ñ–∏–∑–∏—á–µ—Å–∫–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é. –£–ø—Ä–∞–∂–Ω–µ–Ω–∏—è –∏–º–µ—é—Ç —Ä–µ—à–∞—é—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏—è –∫—Ä–µ–ø–∫–∏—Ö –∫–æ—Å—Ç–µ–π, –º—ã—à—Ü –∏ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–¥–µ—á–Ω–æ-—Å–æ—Å—É–¥–∏—Å—Ç–æ–π —Å–∏—Å—Ç–µ–º—ã. –°—Ç–∞—Ä–∞–π—Ç–µ—Å—å —É–¥–µ–ª—è—Ç—å –Ω–µ –º–µ–Ω–µ–µ 150 –º–∏–Ω—É—Ç —É–º–µ—Ä–µ–Ω–Ω—ã–º –∞—ç—Ä–æ–±–Ω—ã–º —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è–º –∏–ª–∏ 75 –º–∏–Ω—É—Ç –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω—ã–º —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è–º –∫–∞–∂–¥—É—é –Ω–µ–¥–µ–ª—é.\\n\\n3. –í—ã—Å—ã–ø–∞–π—Ç–µ—Å—å. –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Å–Ω–∞ –∏–º–µ–µ—Ç —Ä–µ—à–∞—é—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –∏ –ø—Å–∏—Ö–∏—á–µ—Å–∫–æ–≥–æ –±–ª–∞–≥–æ–ø–æ–ª—É—á–∏—è. –û–Ω –ø–æ–º–æ–≥–∞–µ—Ç —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞—Ç—å –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ, —É–ª—É—á—à–∞—Ç—å –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∑–¥–æ—Ä–æ–≤—ã–π —Ä–æ—Å—Ç –∏ –∏–º–º—É–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é. –°—Ç–∞—Ä–∞–π—Ç–µ—Å—å —Å–ø–∞—Ç—å 7-9 —á–∞—Å–æ–≤ –∫–∞–∂–¥—É—é –Ω–æ—á—å.', 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç d0rj/alpaca-cleaned-ru...\")\n",
    "dataset = load_dataset(\"d0rj/alpaca-cleaned-ru\")\n",
    "print(f\"–î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {dataset}\")\n",
    "\n",
    "# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –¥–∏–∞–ª–æ–≥–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç\n",
    "def convert_to_dialog_format(example):\n",
    "    \"\"\"–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –ø—Ä–∏–º–µ—Ä –≤ —Ñ–æ—Ä–º–∞—Ç –¥–∏–∞–ª–æ–≥–∞: system, user, assistant\"\"\"\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.\"},\n",
    "            {\"role\": \"user\", \"content\": example[\"instruction\"]},\n",
    "            {\"role\": \"assistant\", \"content\": example[\"output\"]}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ\n",
    "dialog_dataset = dataset.map(convert_to_dialog_format)\n",
    "print(\"–î–∞—Ç–∞—Å–µ—Ç –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω –≤ –¥–∏–∞–ª–æ–≥–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç\")\n",
    "print(dialog_dataset['train'][0]['messages'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8f0697",
   "metadata": {},
   "source": [
    "### –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å Qwen2.5-0.5B –ë–ï–ó QUANTIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3ebe671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å Qwen2.5-0.5B –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω—ã!\n",
      "–†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏: 494,032,768 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
      "–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏: cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å Qwen2.5-0.5B –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä...\")\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
    "sft_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-0.5B\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º pad token –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç\n",
    "if sft_tokenizer.pad_token is None:\n",
    "    sft_tokenizer.pad_token = sft_tokenizer.eos_token\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –ë–ï–ó QUANTIZATION - –ü–†–û–°–¢–û –ò –ù–ê–î–ï–ñ–ù–û!\n",
    "sft_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-0.5B\",\n",
    "    torch_dtype=torch.float16,  # –∏—Å–ø–æ–ª—å–∑—É–µ–º float16 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ –ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω—ã!\")\n",
    "print(f\"–†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏: {sft_model.num_parameters():,} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\")\n",
    "print(f\"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏: {sft_model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0985c4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–µ–∫—É—â–∞—è –≤–µ—Ä—Å–∏—è jinja2: 3.1.3\n",
      "=== –ì–ï–ù–ï–†–ê–¶–ò–Ø –î–û –û–ë–£–ß–ï–ù–ò–Ø ===\n",
      "Model Input 1:\n",
      "—Å–∫–æ–ª—å–∫–æ –ø–ª–∞–Ω–µ—Ç –≤ –Ω–∞—à–µ–π —Å–æ–ª–Ω–µ—á–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ?\n",
      "Model Output 1:\n",
      "–í–æ—Ç, –Ω–∞–ø—Ä–∏–º–µ—Ä, 48 –ø–ª–∞–Ω–µ—Ç.\n",
      "UAGE\n",
      "—Å–∫–æ–ª—å–∫–æ –ø–ª–∞–Ω–µ—Ç –≤ –Ω–∞—à–µ–π —Å–æ–ª–Ω–µ—á–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ?UAGE\n",
      "UAGE\n",
      "—Å–∫–æ–ª—å–∫–æ –ø–ª–∞–Ω–µ—Ç –≤ –Ω–∞—à–µ–π —Å–æ–ª–Ω–µ—á–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ?UAGE\n",
      "UAGE\n",
      "—Å–∫–æ–ª—å–∫–æ –ø–ª–∞–Ω–µ—Ç –≤ –Ω–∞—à–µ–π —Å–æ–ª–Ω–µ—á–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ?UAGE\n",
      "UAGE\n",
      "—Å–∫–æ–ª—å–∫–æ –ø–ª–∞–Ω–µ—Ç –≤ –Ω–∞—à–µ–π —Å–æ–ª–Ω–µ—á–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ?UAGE\n",
      "UAGE\n",
      "—Å–∫–æ–ª—å–∫–æ –ø–ª–∞–Ω–µ—Ç –≤ –Ω–∞—à–µ–π —Å–æ–ª–Ω–µ—á–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ?UAGE\n",
      "UAGE\n",
      "—Å–∫–æ–ª—å–∫–æ –ø–ª–∞–Ω–µ—Ç –≤ –Ω–∞—à–µ–π —Å–æ–ª–Ω–µ—á–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ?UAGE\n",
      "UAGE\n",
      "—Å–∫–æ–ª—å–∫–æ –ø–ª–∞–Ω–µ—Ç –≤ –Ω–∞—à–µ–π —Å–æ–ª–Ω–µ—á–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ?UAGE\n",
      "UAGE\n",
      "—Å–∫–æ–ª—å–∫–æ –ø–ª–∞–Ω–µ—Ç –≤ –Ω–∞—à–µ–π —Å–æ–ª–Ω–µ—á–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ?UAGE\n",
      "UAGE\n",
      "—Å–∫–æ–ª—å–∫–æ –ø–ª–∞–Ω–µ—Ç –≤ –Ω–∞—à–µ–π —Å–æ–ª–Ω–µ—á–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ?UAGE\n",
      "UAGE\n",
      "—Å–∫–æ–ª—å–∫–æ –ø–ª–∞–Ω–µ—Ç –≤ –Ω–∞—à–µ–π —Å–æ–ª–Ω–µ—á–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ?UAGE\n",
      "UAGE\n",
      "—Å–∫–æ–ª—å–∫–æ –ø–ª–∞–Ω–µ—Ç –≤ –Ω–∞—à–µ–π —Å–æ–ª–Ω–µ—á–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ?UAGE\n",
      "UAGE\n",
      "—Å–∫–æ–ª—å–∫–æ –ø–ª–∞–Ω–µ—Ç –≤ –Ω–∞—à–µ–π —Å–æ–ª–Ω–µ—á–Ω–æ–π\n",
      "--------------------------------------------------------------------------------\n",
      "Model Input 2:\n",
      "—Ä–∞—Å—Å–∫–∞–∂–∏ —Å—Ç–∏—Ö\n",
      "Model Output 2:\n",
      "—Å–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏–ª —É —Ç–µ–±—è\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "ÿ£ÿ∫ŸÑÿ®\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Model Input 3:\n",
      "–∫–æ–≥–¥–∞ —Å–æ–±–∏—Ä–∞—Ç—å –∫—Ä—ã–∂–æ–≤–Ω–∏–∫?\n",
      "Model Output 3:\n",
      "–¥–ª—è —ç—Ç–æ–≥–æ —É–∫–∞–∂–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ —É—Ä–æ–∫–∞.È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "È≤ï\n",
      "--------------------------------------------------------------------------------\n",
      "Model Input 4:\n",
      "–ö–∞–∫ –±—ã—Å—Ç—Ä–æ –≤—ã—É—á–∏—Ç—å –Ω–æ–≤—ã–π —è–∑—ã–∫?\n",
      "Model Output 4:\n",
      "–î–ª—è –Ω–∞—á–∞–ª–∞, —è –ø—Ä–µ–¥–ª–∞–≥–∞—é –≤–≤–æ–¥–∏—Ç—å —è–∑—ã–∫, –∫–æ—Ç–æ—Ä—ã–π –≤—ã —Ö–æ—Ç–µ–ª–∏ –±—ã –∏–∑—É—á–∏—Ç—å.È≤ï\n",
      "È≤ï\n",
      "–ì–¥–µ –≤—ã —Ö–æ—Ç–∏—Ç–µ –Ω–∞—á–∞—Ç—å –∏–∑—É—á–µ–Ω–∏–µ?È≤ï\n",
      "È≤ï\n",
      "–ö–æ–≥–¥–∞ –≤—ã —Ö–æ—Ç–∏—Ç–µ –Ω–∞—á–∞—Ç—å –∏–∑—É—á–µ–Ω–∏–µ?È≤ï\n",
      "È≤ï\n",
      "–î–ª—è –Ω–∞—á–∞–ª–∞, —è –ø—Ä–µ–¥–ª–∞–≥–∞—é –≤–≤–æ–¥–∏—Ç—å —è–∑—ã–∫, –∫–æ—Ç–æ—Ä—ã–π –≤—ã —Ö–æ—Ç–µ–ª–∏ –±—ã –∏–∑—É—á–∏—Ç—å.È≤ï\n",
      "È≤ï\n",
      "–ì–¥–µ –≤—ã —Ö–æ—Ç–∏—Ç–µ –Ω–∞—á–∞—Ç—å –∏–∑—É—á–µ–Ω–∏–µ?È≤ï\n",
      "È≤ï\n",
      "–ö–æ–≥–¥–∞ –≤—ã —Ö–æ—Ç–∏—Ç–µ –Ω–∞—á–∞—Ç—å –∏–∑—É—á–µ–Ω–∏–µ?È≤ï\n",
      "È≤ï\n",
      "–î–ª—è –Ω–∞—á–∞–ª–∞, —è –ø—Ä–µ–¥–ª–∞–≥–∞—é –≤–≤–æ–¥–∏—Ç—å —è–∑—ã–∫, –∫–æ—Ç–æ—Ä—ã–π –≤—ã —Ö–æ—Ç–µ–ª–∏ –±—ã –∏–∑—É—á–∏—Ç—å.È≤ï\n",
      "È≤ï\n",
      "–ì–¥–µ –≤—ã —Ö–æ—Ç–∏—Ç–µ –Ω–∞—á–∞—Ç—å –∏–∑—É—á–µ–Ω–∏–µ?È≤ï\n",
      "È≤ï\n",
      "–ö–æ–≥–¥–∞ –≤—ã —Ö–æ—Ç–∏—Ç–µ –Ω–∞—á–∞—Ç—å –∏–∑—É—á–µ–Ω–∏–µ?È≤ï\n",
      "È≤ï\n",
      "–î–ª—è –Ω–∞—á–∞–ª–∞, —è –ø—Ä–µ–¥–ª–∞–≥–∞—é –≤–≤–æ–¥–∏—Ç—å —è–∑—ã–∫, –∫–æ—Ç–æ—Ä—ã–π –≤—ã —Ö–æ—Ç–µ–ª–∏ –±—ã –∏–∑—É—á–∏—Ç—å.È≤ï\n",
      "È≤ï\n",
      "–ì–¥–µ –≤—ã —Ö–æ—Ç–∏—Ç–µ –Ω–∞—á–∞—Ç—å –∏–∑—É—á–µ–Ω–∏–µ?È≤ï\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# –ü—Ä–æ–≤–µ—Ä–∏–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –¥–æ –æ–±—É—á–µ–Ω–∏—è (–¥–æ–ª–∂–Ω—ã –ø–æ–ª—É—á–∏—Ç—å \"–º—É—Å–æ—Ä\" –∫–∞–∫ –≤ –∑–∞–¥–∞–Ω–∏–∏)\n",
    "questions_rus = [\n",
    "    \"—Å–∫–æ–ª—å–∫–æ –ø–ª–∞–Ω–µ—Ç –≤ –Ω–∞—à–µ–π —Å–æ–ª–Ω–µ—á–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ?\",\n",
    "    \"—Ä–∞—Å—Å–∫–∞–∂–∏ —Å—Ç–∏—Ö\", \n",
    "    \"–∫–æ–≥–¥–∞ —Å–æ–±–∏—Ä–∞—Ç—å –∫—Ä—ã–∂–æ–≤–Ω–∏–∫?\",\n",
    "    \"–ö–∞–∫ –±—ã—Å—Ç—Ä–æ –≤—ã—É—á–∏—Ç—å –Ω–æ–≤—ã–π —è–∑—ã–∫?\"\n",
    "]\n",
    "# –ü—Ä–æ–≤–µ—Ä–∏–º –∫–∞–∫–∞—è –≤–µ—Ä—Å–∏—è jinja2 —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞\n",
    "try:\n",
    "    import jinja2\n",
    "    print(f\"–¢–µ–∫—É—â–∞—è –≤–µ—Ä—Å–∏—è jinja2: {jinja2.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"jinja2 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω\")\n",
    "\n",
    "print(\"=== –ì–ï–ù–ï–†–ê–¶–ò–Ø –î–û –û–ë–£–ß–ï–ù–ò–Ø ===\")\n",
    "def test_generation_before_sft():\n",
    "    sft_model.eval()\n",
    "    \n",
    "    for i, question in enumerate(questions_rus):\n",
    "        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∫–∞–∫ –¥–∏–∞–ª–æ–≥\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "        \n",
    "        # –ü—Ä–∏–º–µ–Ω—è–µ–º —á–∞—Ç-—à–∞–±–ª–æ–Ω\n",
    "        text = sft_tokenizer.apply_chat_template(\n",
    "            messages, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        inputs = sft_tokenizer(text, return_tensors=\"pt\").to(sft_model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = sft_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=sft_tokenizer.pad_token_id,\n",
    "                eos_token_id=sft_tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        response = sft_tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞\n",
    "        assistant_response = response.split(\"assistant\\n\")[-1] if \"assistant\\n\" in response else response\n",
    "        \n",
    "        print(f\"Model Input {i+1}:\")\n",
    "        print(question)\n",
    "        print(f\"Model Output {i+1}:\")\n",
    "        print(assistant_response)\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "test_generation_before_sft()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a57a6fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–î–∞—Ç–∞—Å–µ—Ç –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω –¥–ª—è SFT\n",
      "–ü—Ä–∏–º–µ—Ä —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:\n",
      "<|im_start|>system\n",
      "–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.<|im_end|>\n",
      "<|im_start|>user\n",
      "–î–∞–π—Ç–µ —Ç—Ä–∏ —Å–æ–≤–µ—Ç–∞, –∫–∞–∫ –æ—Å—Ç–∞–≤–∞—Ç—å—Å—è –∑–¥–æ—Ä–æ–≤—ã–º.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "1. –°–æ–±–ª—é–¥–∞–π—Ç–µ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏ –ø–∏—Ç–∞—Ç–µ–ª—å–Ω—É—é –¥–∏–µ—Ç—É. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤ –≤–∞—à —Ä–∞—Ü–∏–æ–Ω –≤—Ö–æ–¥—è—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ —Ñ—Ä—É–∫—Ç—ã –∏ –æ–≤–æ—â–∏, –Ω–µ–∂–∏—Ä–Ω—ã–π –±–µ–ª–æ–∫, —Ü–µ–ª—å–Ω–æ–∑–µ—Ä–Ω–æ–≤—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã –∏ –ø–æ–ª–µ–∑–Ω—ã–µ –∂–∏—Ä—ã. –≠—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –æ–±–µ—Å–ø–µ—á–∏—Ç—å –≤–∞—à –æ—Ä–≥–∞–Ω–∏–∑–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–º–∏ –ø–∏—Ç–∞—Ç–µ–ª—å–Ω—ã–º–∏ –≤–µ—â–µ—Å—Ç–≤–∞–º–∏ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –º–æ–∂–µ—Ç –ø–æ–º–æ—á—å –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å —Ö—Ä–æ–Ω–∏—á–µ—Å–∫–∏–µ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—è.\n",
      "\n",
      "2. –ó–∞–Ω–∏–º–∞–π—Ç–µ—Å—å —Ä–µ...\n"
     ]
    }
   ],
   "source": [
    "### –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è SFTTrainer\n",
    "\n",
    "def format_for_sft(example):\n",
    "    \"\"\"–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è SFT –æ–±—É—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É—è —á–∞—Ç-—à–∞–±–ª–æ–Ω\"\"\"\n",
    "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π —á–∞—Ç-—à–∞–±–ª–æ–Ω –º–æ–¥–µ–ª–∏\n",
    "    formatted_text = sft_tokenizer.apply_chat_template(\n",
    "        example['messages'],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    return {\"text\": formatted_text}\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
    "train_dataset_sft = dialog_dataset['train'].map(format_for_sft)\n",
    "print(\"–î–∞—Ç–∞—Å–µ—Ç –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω –¥–ª—è SFT\")\n",
    "print(f\"–ü—Ä–∏–º–µ—Ä —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:\")\n",
    "print(train_dataset_sft[0]['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135ad486",
   "metadata": {},
   "source": [
    "### –≠—Ç–∞–ø 2.2: –î–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc9c359e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω—ã (FP16 –æ—Ç–∫–ª—é—á–µ–Ω)\n",
      "‚úÖ SFTTrainer —Å–æ–∑–¥–∞–Ω! –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Attempting to unscale FP16 gradients.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ SFTTrainer —Å–æ–∑–¥–∞–Ω! –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[43msft_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –ø–æ—Å–ª–µ SFT\u001b[39;00m\n\u001b[1;32m     33\u001b[0m sft_trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./sft_model_final\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2715\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2713\u001b[0m         grad_norm_context \u001b[38;5;241m=\u001b[39m implicit_replication\n\u001b[1;32m   2714\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m grad_norm_context():\n\u001b[0;32m-> 2715\u001b[0m         _grad_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2716\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2717\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2718\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2720\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2721\u001b[0m     is_accelerate_available()\n\u001b[1;32m   2722\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2723\u001b[0m ):\n\u001b[1;32m   2724\u001b[0m     grad_norm \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_global_grad_norm()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/accelerator.py:2896\u001b[0m, in \u001b[0;36mAccelerator.clip_grad_norm_\u001b[0;34m(self, parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m   2894\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m parameters \u001b[38;5;241m==\u001b[39m [p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters()]:\n\u001b[1;32m   2895\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mclip_grad_norm_(max_norm, norm_type)\n\u001b[0;32m-> 2896\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munscale_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2897\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(parameters, max_norm, norm_type\u001b[38;5;241m=\u001b[39mnorm_type)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/accelerator.py:2834\u001b[0m, in \u001b[0;36mAccelerator.unscale_gradients\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m   2832\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(opt, AcceleratedOptimizer):\n\u001b[1;32m   2833\u001b[0m     opt \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39moptimizer\n\u001b[0;32m-> 2834\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munscale_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/amp/grad_scaler.py:343\u001b[0m, in \u001b[0;36mGradScaler.unscale_\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m    336\u001b[0m inv_scale \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale\u001b[38;5;241m.\u001b[39mdouble()\u001b[38;5;241m.\u001b[39mreciprocal()\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps:0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale\u001b[38;5;241m.\u001b[39mreciprocal()\n\u001b[1;32m    340\u001b[0m )\n\u001b[1;32m    341\u001b[0m found_inf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((), \u001b[38;5;241m0.0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 343\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unscale_grads_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minv_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    345\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mUNSCALED\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/amp/grad_scaler.py:261\u001b[0m, in \u001b[0;36mGradScaler._unscale_grads_\u001b[0;34m(self, optimizer, inv_scale, found_inf, allow_fp16)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m allow_fp16) \u001b[38;5;129;01mand\u001b[39;00m param\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n\u001b[0;32m--> 261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to unscale FP16 gradients.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mis_sparse:\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;66;03m# is_coalesced() == False means the sparse grad has values with duplicate indices.\u001b[39;00m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;66;03m# coalesce() deduplicates indices and adds all values that have the same index.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;66;03m# For scaled fp16 values, there's a good chance coalescing will cause overflow,\u001b[39;00m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;66;03m# so we should check the coalesced _values().\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfloat16:\n",
      "\u001b[0;31mValueError\u001b[0m: Attempting to unscale FP16 gradients."
     ]
    }
   ],
   "source": [
    "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è –¥–ª—è SFT\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./sft_model_working\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-5,  # ‚Üê –ò–°–•–û–î–ù–´–ô LR (–≤—ã—Å–æ–∫–∏–π)\n",
    "    weight_decay=0.001,\n",
    "    warmup_ratio=0.03,\n",
    "    logging_steps=10,\n",
    "    save_steps=200,\n",
    "    fp16=True,  # ‚Üê –í–ö–õ–Æ–ß–ê–ï–ú FP16 (–∫–∞–∫ –±—ã–ª–æ)\n",
    "    remove_unused_columns=False,\n",
    "    report_to=None,\n",
    ")\n",
    "\n",
    "print(\"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω—ã (FP16 –æ—Ç–∫–ª—é—á–µ–Ω)\")\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "sft_trainer = SFTTrainer(\n",
    "    model=sft_model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_dataset_sft,\n",
    "    processing_class=sft_tokenizer,  # ‚Üê –í–ê–ñ–ù–û: processing_class –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
    ")\n",
    "\n",
    "print(\"‚úÖ SFTTrainer —Å–æ–∑–¥–∞–Ω! –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...\")\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ\n",
    "sft_trainer.train()\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –ø–æ—Å–ª–µ SFT\n",
    "sft_trainer.save_model(\"./sft_model_final\")\n",
    "sft_tokenizer.save_pretrained(\"./sft_model_final\")\n",
    "print(\"‚úÖ SFT –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e540e9",
   "metadata": {},
   "source": [
    "### –≠—Ç–∞–ø 2.3: –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–æ—Å–ª–µ SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82657b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== –ü–†–û–°–¢–û–ô –¢–ï–°–¢ –ü–û–°–õ–ï SFT ===\n",
      "\n",
      "–í–æ–ø—Ä–æ—Å 1: —Å–∫–æ–ª—å–∫–æ –ø–ª–∞–Ω–µ—Ç –≤ –Ω–∞—à–µ–π —Å–æ–ª–Ω–µ—á–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ?\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[43msimple_test_after_sft\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 22\u001b[0m, in \u001b[0;36msimple_test_after_sft\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 22\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m response \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m–û—Ç–≤–µ—Ç: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:2564\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2561\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39muse_cache\n\u001b[1;32m   2563\u001b[0m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[0;32m-> 2564\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2565\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2566\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2570\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2571\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2572\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2574\u001b[0m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[1;32m   2575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2576\u001b[0m     generation_config\u001b[38;5;241m.\u001b[39mreturn_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2577\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2578\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result\u001b[38;5;241m.\u001b[39mpast_key_values, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_legacy_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2579\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:2829\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2828\u001b[0m     \u001b[38;5;66;03m# TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\u001b[39;00m\n\u001b[0;32m-> 2829\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2830\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2831\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"=== –ì–ï–ù–ï–†–ê–¶–ò–Ø –ü–û–°–õ–ï SFT –û–ë–£–ß–ï–ù–ò–Ø ===\")\n",
    "\n",
    "def test_generation_after_sft():\n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –ø–æ—Å–ª–µ SFT\n",
    "    final_sft_model = sft_model\n",
    "    final_sft_model.eval()\n",
    "    \n",
    "    for i, question in enumerate(questions_rus):\n",
    "        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∫–∞–∫ –¥–∏–∞–ª–æ–≥\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "        \n",
    "        # –ü—Ä–∏–º–µ–Ω—è–µ–º —á–∞—Ç-—à–∞–±–ª–æ–Ω\n",
    "        text = sft_tokenizer.apply_chat_template(\n",
    "            messages, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        inputs = sft_tokenizer(text, return_tensors=\"pt\").to(final_sft_model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = final_sft_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=300,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=sft_tokenizer.pad_token_id,\n",
    "                eos_token_id=sft_tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        full_response = sft_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞\n",
    "        if \"assistant\" in full_response:\n",
    "            assistant_response = full_response.split(\"assistant\")[-1].strip()\n",
    "        else:\n",
    "            assistant_response = full_response\n",
    "        \n",
    "        print(f\"Model Input {i+1}:\")\n",
    "        print(question)\n",
    "        print(f\"Model Output {i+1}:\")\n",
    "        print(assistant_response)\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "test_generation_after_sft()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49e9e6c",
   "metadata": {},
   "source": [
    "### –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–æ –∏ –ø–æ—Å–ª–µ SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "46da31c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== –°–†–ê–í–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ===\n",
      "–¶–µ–ª—å: –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç—ã –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã,\n",
      "–¥–∞–∂–µ –µ—Å–ª–∏ —Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—è –Ω–µ–∏–¥–µ–∞–ª—å–Ω–∞ (–∫–∞–∫ –≤ –ø—Ä–∏–º–µ—Ä–µ –∏–∑ –∑–∞–¥–∞–Ω–∏—è)\n",
      "\n",
      "=== –¢–ï–°–¢ –ù–ê –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–• –ü–†–û–ú–ü–¢–ê–• ===\n",
      "Prompt: –û–±—ä—è—Å–Ω–∏, —á—Ç–æ —Ç–∞–∫–æ–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç\n",
      "Response: , I'm sorry if it was not helpful.\n",
      " libertine\n",
      "–ö–∞–∫ –≤—ã –¥—É–º–∞–µ—Ç–µ, –∫–∞–∫–∏–µ –∑–∞–¥–∞—á–∏ AI –º–æ–∂–µ—Ç —Ä–µ—à–∞—Ç—å?\n",
      " libertine\n",
      "–ö–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç—ã. –ü—Ä–æ–≥—Ä–∞–º–º–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä—ã. –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä—ã. –†–µ–≥–∏—Å—Ç—Ä–∞—Ç–æ—Ä—ã. –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–µ—Å—Ç—ã. –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø–æ–ª–Ω—ã–π –¥–æ—Å—Ç—É–ø –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è–º –Ω–∞ —Å–µ–≥–æ–¥–Ω—è—à–Ω–∏–π –¥–µ–Ω—å.\n",
      " libertine\n",
      "–°–∫–æ–ª—å–∫–æ –≤—Ä–µ–º–µ–Ω–∏ —è –ø–æ—Ç—Ä–∞—á—É –Ω–∞ —ç—Ç—É –ø—Ä–æ–≥—Ä–∞–º–º—É?\n",
      " libertine\n",
      "–ò—Å–ø–æ–ª—å–∑—É–π\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: –ù–∞–ø–∏—à–∏ –∫–æ—Ä–æ—Ç–∫–æ–µ —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ –æ –≤–µ—Å–Ω–µ\n",
      "Response: –¢–µ–∫—Å—Ç\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETE\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: –ö–∞–∫ –ø—Ä–∏–≥–æ—Ç–æ–≤–∏—Ç—å —è–∏—á–Ω–∏—Ü—É?\n",
      "Response: –ù—É–∂–Ω–æ –≤—Å–µ–≥–æ –ª–∏—à—å –Ω–µ–º–Ω–æ–≥–æ –ø–æ–º—ã—Ç—å –∏ –Ω–∞—Ç–µ—Ä–µ—Ç—å —è–∏—á–Ω–∏—Ü—É.utralize\n",
      "ÔøΩÂõûÁ≠î\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"=== –°–†–ê–í–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ===\")\n",
    "print(\"–¶–µ–ª—å: –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç—ã –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã,\")\n",
    "print(\"–¥–∞–∂–µ –µ—Å–ª–∏ —Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—è –Ω–µ–∏–¥–µ–∞–ª—å–Ω–∞ (–∫–∞–∫ –≤ –ø—Ä–∏–º–µ—Ä–µ –∏–∑ –∑–∞–¥–∞–Ω–∏—è)\")\n",
    "\n",
    "# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–∞—Ö\n",
    "additional_test_prompts = [\n",
    "    \"–û–±—ä—è—Å–Ω–∏, —á—Ç–æ —Ç–∞–∫–æ–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç\",\n",
    "    \"–ù–∞–ø–∏—à–∏ –∫–æ—Ä–æ—Ç–∫–æ–µ —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ –æ –≤–µ—Å–Ω–µ\",\n",
    "    \"–ö–∞–∫ –ø—Ä–∏–≥–æ—Ç–æ–≤–∏—Ç—å —è–∏—á–Ω–∏—Ü—É?\"\n",
    "]\n",
    "\n",
    "print(\"\\n=== –¢–ï–°–¢ –ù–ê –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–• –ü–†–û–ú–ü–¢–ê–• ===\")\n",
    "for prompt in additional_test_prompts:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    text = sft_tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = sft_tokenizer(text, return_tensors=\"pt\").to(sft_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = sft_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=sft_tokenizer.pad_token_id,\n",
    "            eos_token_id=sft_tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = sft_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    assistant_response = response.split(\"assistant\")[-1].strip() if \"assistant\" in response else response\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Response: {assistant_response}\")\n",
    "    print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
