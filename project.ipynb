{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c2592d4",
   "metadata": {},
   "source": [
    "# *–ê–Ω–∞–ª–∏–∑ –∑–∞–¥–∞–Ω–∏—è –∏ –ø–ª–∞–Ω –ø—Ä–æ–µ–∫—Ç–∞*\n",
    "\n",
    "## **–¶–µ–ª—å: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –¥–≤–∞ –∫–ª—é—á–µ–≤—ã—Ö —ç—Ç–∞–ø–∞ –æ–±—É—á–µ–Ω–∏—è Large Language Model (LLM) - Pretrain –∏ Supervised Fine-Tuning (SFT).**\n",
    "\n",
    "### –ö—Ä–∏—Ç–µ—Ä–∏–∏ —É—Å–ø–µ—Ö–∞:\n",
    "### Pretrain: –ú–æ–¥–µ–ª—å (~150M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤) –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –¥–ª—è –ø—Ä–æ–º–ø—Ç–æ–≤ –∏–∑ —Ä—É—Å—Å–∫–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã.\n",
    "### SFT: –ú–æ–¥–µ–ª—å Qwen2.5-0.5B –∞–¥–µ–∫–≤–∞—Ç–Ω–æ –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, —Å–ª–µ–¥—É—è –∑–∞–¥–∞–Ω–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfb0c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, \n",
    "    Trainer, TrainingArguments, DataCollatorForLanguageModeling,\n",
    "    LlamaConfig, GPT2LMHeadModel\n",
    ")\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# –°–∫–∞—á–∏–≤–∞–µ–º –∏ —Ä–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "# –ü—Ä–æ–≤–µ—Ä–∏–º –∫–∞–∫–∞—è –≤–µ—Ä—Å–∏—è jinja2 —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞\n",
    "try:\n",
    "    import jinja2\n",
    "    print(f\"–¢–µ–∫—É—â–∞—è –≤–µ—Ä—Å–∏—è jinja2: {jinja2.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"jinja2 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω\")\n",
    "\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainerCallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfdc529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: data/RussianNovels-master/corpus\n"
     ]
    }
   ],
   "source": [
    "# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –¥–∞–Ω–Ω—ã—Ö\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è\n",
    "def download_and_extract_data():\n",
    "    # –°–∫–∞—á–∏–≤–∞–µ–º –≤–µ—Å—å —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π\n",
    "    repo_url = \"https://github.com/JoannaBy/RussianNovels/archive/refs/heads/master.zip\"\n",
    "    response = requests.get(repo_url)\n",
    "    \n",
    "    with zipfile.ZipFile(io.BytesIO(response.content)) as zip_ref:\n",
    "        zip_ref.extractall('data/')\n",
    "    \n",
    "    return 'data/RussianNovels-master/corpus'\n",
    "\n",
    "corpus_path = download_and_extract_data()\n",
    "print(f\"–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {corpus_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c688df85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∂–µ–Ω–æ 108 —Ç–µ–∫—Å—Ç–æ–≤\n",
      "–ü—Ä–∏–º–µ—Ä—ã —Ñ–∞–π–ª–æ–≤: ['Pushkin_CapitanskaaDochka.txt', 'Sologub_KorolevaOrtruda.txt', 'Chekhov_Dama.txt', 'Gogol_Viy.txt', 'Gorky_ZyznKlimaSamgina3.txt', 'Zhukova_Dacha.txt', 'Nabokov_Otchayanie_1934.txt', 'NKhvoshchinskaya_PervayaBorba.txt', 'Nabokov_Veschi_1972_Ilin.txt', 'Sologub_TjazolyjeSny.txt']\n"
     ]
    }
   ],
   "source": [
    "# –ß—Ç–µ–Ω–∏–µ –≤—Å–µ—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤\n",
    "def load_texts_from_directory(directory_path):\n",
    "    texts = []\n",
    "    filenames = []\n",
    "    \n",
    "    for file_path in Path(directory_path).rglob('*.txt'):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read().strip()\n",
    "                if len(text) > 100:  # —Ñ–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Ñ–∞–π–ª—ã\n",
    "                    texts.append(text)\n",
    "                    filenames.append(file_path.name)\n",
    "        except Exception as e:\n",
    "            print(f\"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {file_path}: {e}\")\n",
    "    \n",
    "    return texts, filenames\n",
    "\n",
    "texts, filenames = load_texts_from_directory(corpus_path)\n",
    "print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤\")\n",
    "print(f\"–ü—Ä–∏–º–µ—Ä—ã —Ñ–∞–π–ª–æ–≤: {filenames[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c9f016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "597b90c62e1c44fe9985783fa93751e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –æ—Å—Ç–∞–ª–æ—Å—å 108 —Ç–µ–∫—Å—Ç–æ–≤\n",
      "–ü—Ä–∏–º–µ—Ä –æ—á–∏—â–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:\n",
      "–ö–ê–ü–ò–¢–ê–ù–°–ö–ê–Ø –î–û–ß–ö–ê –ë–µ—Ä–µ–≥–∏ —á–µ—Å—Ç—å —Å–º–æ–ª–æ–¥—É -- –¢–æ–≥–æ –Ω–µ –Ω–∞–¥–æ–±–Ω–æ; –ø—É—Å—Ç—å –≤ –∞—Ä–º–∏–∏ –ø–æ—Å–ª—É–∂–∏—Ç -- –ò–∑—Ä—è–¥–Ω–æ —Å–∫–∞–∑–∞–Ω–æ –ø—É—Å–∫–∞–π –µ–≥–æ –ø–æ—Ç—É–∂–∏—Ç –î–∞ –∫—Ç–æ –µ–≥–æ –æ—Ç–µ—Ü –° —Ç–µ—Ö –ø–æ—Ä –∂–∏–ª –æ–Ω –≤ —Å–≤–æ–µ–π –°–∏–º–±–∏—Ä—Å–∫–æ–π –¥–µ—Ä–µ–≤–Ω–µ, –≥–¥–µ –∏ –∂–µ–Ω–∏–ª—Å—è –Ω–∞ –¥–µ–≤–∏—Ü–µ –ê–≤–¥–æ—Ç—å–µ –í–∞—Å–∏–ª—å–µ–≤–Ω–µ –Æ , –¥–æ—á–µ—Ä–∏ –±–µ–¥–Ω–æ–≥–æ —Ç–∞–º–æ—à–Ω–µ–≥–æ –¥–≤–æ—Ä—è–Ω–∏–Ω–∞ –ù–∞—Å –±—ã–ª–æ –¥–µ–≤—è—Ç—å —á–µ–ª–æ–≤–µ–∫ –¥–µ—Ç–µ–π –í—Å–µ –º–æ–∏ –±—Ä–∞—Ç—å—è –∏ —Å–µ—Å—Ç—Ä—ã —É–º–µ—Ä–ª–∏ –≤–æ –º–ª–∞–¥–µ–Ω—á–µ—Å—Ç–≤–µ –ú–∞—Ç—É—à–∫–∞ –±—ã–ª–∞ –µ—â–µ –º–Ω–æ—é –±—Ä—é—Ö–∞—Ç–∞, –∫–∞–∫ —É–∂–µ —è –±—ã–ª –∑–∞–ø–∏—Å–∞–Ω –≤ –°–µ–º–µ–Ω–æ–≤—Å–∫–∏–π –ø–æ–ª–∫ —Å–µ—Ä–∂–∞–Ω—Ç–æ–º, –ø–æ –º–∏–ª–æ—Å—Ç–∏ –º–∞–π–æ—Ä–∞ –≥–≤–∞—Ä–¥–∏–∏ –∫–Ω—è–∑—è –ë , –±–ª–∏–∑–∫–æ–≥–æ –Ω–∞—à–µ–≥–æ —Ä–æ–¥—Å—Ç–≤–µ–Ω–Ω–∏–∫–∞ –ï—Å–ª–∏ –±—ã –ø–∞—á–µ...\n"
     ]
    }
   ],
   "source": [
    "# –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö\n",
    "def preprocess_text(text):\n",
    "    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    cleaned_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "            \n",
    "        # –£–¥–∞–ª—è–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –Ω–µ-–∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏–º–∏ —Å–∏–º–≤–æ–ª–∞–º–∏ (–∫—Ä–æ–º–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ –∏ –ø—Ä–æ–±–µ–ª–æ–≤)\n",
    "        if re.search(r'[^–∞-—è–ê-–Ø—ë–Å\\s\\-,:;\"()¬´¬ª‚Äî]', sentence):\n",
    "            continue\n",
    "            \n",
    "        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â—É—é—Å—è –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é\n",
    "        sentence = re.sub(r'\\.{2,}', '.', sentence)\n",
    "        sentence = re.sub(r',{2,}', ',', sentence)\n",
    "        sentence = re.sub(r'!{2,}', '!', sentence)\n",
    "        sentence = re.sub(r'\\?{2,}', '?', sentence)\n",
    "        \n",
    "        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã\n",
    "        sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "        \n",
    "        if len(sentence) > 10:  # –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è\n",
    "            cleaned_sentences.append(sentence)\n",
    "    \n",
    "    return ' '.join(cleaned_sentences)\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –∫–æ –≤—Å–µ–º —Ç–µ–∫—Å—Ç–∞–º\n",
    "cleaned_texts = []\n",
    "for text in tqdm(texts):\n",
    "    cleaned = preprocess_text(text)\n",
    "    if cleaned:\n",
    "        cleaned_texts.append(cleaned)\n",
    "\n",
    "print(f\"–ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –æ—Å—Ç–∞–ª–æ—Å—å {len(cleaned_texts)} —Ç–µ–∫—Å—Ç–æ–≤\")\n",
    "print(f\"–ü—Ä–∏–º–µ—Ä –æ—á–∏—â–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:\\n{cleaned_texts[0][:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1eb8f89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: 107 —Ç–µ–∫—Å—Ç–æ–≤\n"
     ]
    }
   ],
   "source": [
    "# –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤\n",
    "def remove_duplicates(texts):\n",
    "    seen = set()\n",
    "    unique_texts = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text_hash = hash(text[:1000])  # —Ö–µ—à–∏—Ä—É–µ–º –Ω–∞—á–∞–ª–æ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤\n",
    "        if text_hash not in seen:\n",
    "            seen.add(text_hash)\n",
    "            unique_texts.append(text)\n",
    "    \n",
    "    return unique_texts\n",
    "\n",
    "unique_texts = remove_duplicates(cleaned_texts)\n",
    "print(f\"–ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(unique_texts)} —Ç–µ–∫—Å—Ç–æ–≤\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cec9adc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°–æ–∑–¥–∞–Ω–æ 14438 —á–∞–Ω–∫–æ–≤\n",
      "–ü—Ä–∏–º–µ—Ä —á–∞–Ω–∫–∞:\n",
      "–ö–ê–ü–ò–¢–ê–ù–°–ö–ê–Ø –î–û–ß–ö–ê –ë–µ—Ä–µ–≥–∏ —á–µ—Å—Ç—å —Å–º–æ–ª–æ–¥—É -- –¢–æ–≥–æ –Ω–µ –Ω–∞–¥–æ–±–Ω–æ; –ø—É—Å—Ç—å –≤ –∞—Ä–º–∏–∏ –ø–æ—Å–ª—É–∂–∏—Ç -- –ò–∑—Ä—è–¥–Ω–æ —Å–∫–∞–∑–∞–Ω–æ –ø—É—Å–∫–∞–π –µ–≥–æ –ø–æ—Ç—É–∂–∏—Ç –î–∞ –∫—Ç–æ –µ–≥–æ –æ—Ç–µ—Ü –° —Ç–µ—Ö –ø–æ—Ä –∂–∏–ª –æ–Ω –≤ —Å–≤–æ–µ–π –°–∏–º–±–∏—Ä—Å–∫–æ–π –¥–µ—Ä–µ–≤–Ω–µ, –≥–¥–µ –∏ –∂–µ–Ω–∏–ª—Å—è –Ω–∞ –¥–µ...\n"
     ]
    }
   ],
   "source": [
    "# –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏\n",
    "def split_into_chunks(texts, chunk_size=400):\n",
    "    \"\"\"–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç—ã –Ω–∞ —á–∞–Ω–∫–∏ –ø—Ä–∏–º–µ—Ä–Ω–æ –ø–æ chunk_size —Å–ª–æ–≤\"\"\"\n",
    "    all_chunks = []\n",
    "    \n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        for i in range(0, len(words), chunk_size):\n",
    "            chunk = ' '.join(words[i:i + chunk_size])\n",
    "            if len(chunk) > 50:  # –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —á–∞–Ω–∫–∞\n",
    "                all_chunks.append(chunk)\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "chunks = split_into_chunks(unique_texts, chunk_size=400)\n",
    "print(f\"–°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤\")\n",
    "print(f\"–ü—Ä–∏–º–µ—Ä —á–∞–Ω–∫–∞:\\n{chunks[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b45677",
   "metadata": {},
   "source": [
    "### –≠—Ç–∞–ø 1.2: –°–æ–∑–¥–∞–Ω–∏–µ BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6d4d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–µ–∫—Å—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: texts_for_tokenizer.txt\n",
      "–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: 61.77 MB\n",
      "\n",
      "\n",
      "\n",
      "–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –æ–±—É—á–µ–Ω!\n",
      "–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω!\n",
      "–¢–µ–∫—Å—Ç: '–í—Å–µ –º—ã—Å–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ—é—Ç –æ–≥—Ä–æ–º–Ω—ã–µ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è'\n",
      "–¢–æ–∫–µ–Ω—ã: ['<bos>', '–í—Å–µ', '–º—ã—Å–ª–∏', ',', '–∫–æ—Ç–æ—Ä—ã–µ', '–∏–º–µ', '—é—Ç', '–æ–≥—Ä–æ–º', '–Ω—ã–µ', '–ø–æ—Å–ª–µ–¥', '—Å—Ç–≤–∏—è', '<eos>']\n",
      "IDs: [2, 665, 1206, 7, 664, 442, 350, 1909, 269, 818, 2055, 3]\n",
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤: 12\n"
     ]
    }
   ],
   "source": [
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç—ã –≤ –æ–¥–∏–Ω —Ñ–∞–π–ª –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
    "def prepare_texts_for_tokenizer(chunks, output_file=\"texts_for_tokenizer.txt\"):\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for chunk in chunks:\n",
    "            f.write(chunk + '\\n')\n",
    "    return output_file\n",
    "\n",
    "corpus_file = prepare_texts_for_tokenizer(chunks)\n",
    "print(f\"–¢–µ–∫—Å—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {corpus_file}\")\n",
    "print(f\"–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {os.path.getsize(corpus_file) / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "\n",
    "def train_bpe_tokenizer(corpus_file, vocab_size=3000):\n",
    "    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
    "    \n",
    "    # –ü—Ä–µ—Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä - —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    \n",
    "    # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã\n",
    "    special_tokens = [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"]\n",
    "    \n",
    "    # –¢—Ä–µ–Ω–µ—Ä –¥–ª—è BPE\n",
    "    trainer = BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=special_tokens,\n",
    "        min_frequency=2\n",
    "    )\n",
    "    \n",
    "    # –û–±—É—á–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
    "    tokenizer.train(files=[corpus_file], trainer=trainer)\n",
    "    \n",
    "    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫—É –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è <bos> –∏ <eos>\n",
    "    tokenizer.post_processor = TemplateProcessing(\n",
    "        single=\"<bos> $A <eos>\",\n",
    "        special_tokens=[(\"<bos>\", 2), (\"<eos>\", 3)]\n",
    "    )\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "# –û–±—É—á–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
    "bpe_tokenizer = train_bpe_tokenizer(corpus_file, vocab_size=3000)\n",
    "print(\"–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –æ–±—É—á–µ–Ω!\")\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
    "bpe_tokenizer.save(\"custom_bpe_tokenizer.json\")\n",
    "print(\"–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω!\")\n",
    "\n",
    "# –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
    "test_text = \"–í—Å–µ –º—ã—Å–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ—é—Ç –æ–≥—Ä–æ–º–Ω—ã–µ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è\"\n",
    "encoded = bpe_tokenizer.encode(test_text)\n",
    "print(f\"–¢–µ–∫—Å—Ç: '{test_text}'\")\n",
    "print(f\"–¢–æ–∫–µ–Ω—ã: {encoded.tokens}\")\n",
    "print(f\"IDs: {encoded.ids}\")\n",
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤: {len(encoded.ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d295266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–æ—Ä–º–∞—Ç–µ Hugging Face!\n",
      "HF —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä: torch.Size([1, 12])\n"
     ]
    }
   ],
   "source": [
    "# –°–æ–∑–¥–∞–µ–º –æ–±–µ—Ä—Ç–∫—É –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å Hugging Face\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ HF-—Ñ–æ—Ä–º–∞—Ç\n",
    "hf_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=bpe_tokenizer,\n",
    "    bos_token=\"<bos>\",\n",
    "    eos_token=\"<eos>\", \n",
    "    pad_token=\"<pad>\",\n",
    "    unk_token=\"<unk>\"\n",
    ")\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç–µ Hugging Face\n",
    "hf_tokenizer.save_pretrained(\"custom_bpe_tokenizer_hf\")\n",
    "print(\"–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–æ—Ä–º–∞—Ç–µ Hugging Face!\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–±–æ—Ç—É\n",
    "test_encodings = hf_tokenizer(\n",
    "    test_text, \n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "print(f\"HF —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä: {test_encodings.input_ids.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259376cc",
   "metadata": {},
   "source": [
    "### –≠—Ç–∞–ø 1.3: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ä–∞–∑–±–∏–µ–Ω–∏–µ–º –Ω–∞ —á–∞–Ω–∫–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "867ddfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ù–∞—á–∏–Ω–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –∏ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "178939c62764432ca827f81699b8c737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14438 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°–æ–∑–¥–∞–Ω–æ 28818 —á–∞–Ω–∫–æ–≤\n",
      "–ü—Ä–∏–º–µ—Ä —á–∞–Ω–∫–∞ (–ø–µ—Ä–≤—ã–µ 20 —Ç–æ–∫–µ–Ω–æ–≤): [2, 24, 14, 29, 22, 32, 14, 27, 31, 24, 14, 45, 18, 28, 37, 24, 14, 1342, 94, 186]\n",
      "–î–ª–∏–Ω–∞ –ø–µ—Ä–≤–æ–≥–æ —á–∞–Ω–∫–∞: 512 —Ç–æ–∫–µ–Ω–æ–≤\n",
      "–°–æ–∑–¥–∞–Ω –¥–∞—Ç–∞—Å–µ—Ç —Å 28818 –ø—Ä–∏–º–µ—Ä–∞–º–∏\n",
      "–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞: 512\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_chunk(texts, tokenizer, max_length=512):\n",
    "    \"\"\"\n",
    "    –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç—ã –∏ —Ä–∞–∑–±–∏–≤–∞–µ—Ç –Ω–∞ —á–∞–Ω–∫–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã\n",
    "    —Å –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º <bos> –∏ <eos> —Ç–æ–∫–µ–Ω–æ–≤\n",
    "    \"\"\"\n",
    "    all_input_ids = []\n",
    "    \n",
    "    for text in tqdm(texts):\n",
    "        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç\n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            truncation=False,\n",
    "            padding=False,\n",
    "            return_offsets_mapping=False,\n",
    "            add_special_tokens=False  # –¥–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –≤—Ä—É—á–Ω—É—é\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids']\n",
    "        \n",
    "        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏ —Å —É—á–µ—Ç–æ–º –º–µ—Å—Ç–∞ –¥–ª—è <bos> –∏ <eos>\n",
    "        chunk_size = max_length - 2  # –æ—Å—Ç–∞–≤–ª—è–µ–º –º–µ—Å—Ç–æ –¥–ª—è <bos> –∏ <eos>\n",
    "        \n",
    "        for i in range(0, len(input_ids), chunk_size):\n",
    "            chunk = input_ids[i:i + chunk_size]\n",
    "            \n",
    "            # –î–æ–±–∞–≤–ª—è–µ–º <bos> –∏ <eos>\n",
    "            chunk_with_special = [tokenizer.bos_token_id] + chunk + [tokenizer.eos_token_id]\n",
    "            \n",
    "            # –ï—Å–ª–∏ —á–∞–Ω–∫ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º\n",
    "            if len(chunk_with_special) >= 10:  # –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞\n",
    "                all_input_ids.append(chunk_with_special)\n",
    "    \n",
    "    return all_input_ids\n",
    "\n",
    "# –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –∏ —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏\n",
    "print(\"–ù–∞—á–∏–Ω–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –∏ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏...\")\n",
    "all_chunked_ids = tokenize_and_chunk(chunks, hf_tokenizer, max_length=512)\n",
    "\n",
    "print(f\"–°–æ–∑–¥–∞–Ω–æ {len(all_chunked_ids)} —á–∞–Ω–∫–æ–≤\")\n",
    "print(f\"–ü—Ä–∏–º–µ—Ä —á–∞–Ω–∫–∞ (–ø–µ—Ä–≤—ã–µ 20 —Ç–æ–∫–µ–Ω–æ–≤): {all_chunked_ids[0][:20]}\")\n",
    "print(f\"–î–ª–∏–Ω–∞ –ø–µ—Ä–≤–æ–≥–æ —á–∞–Ω–∫–∞: {len(all_chunked_ids[0])} —Ç–æ–∫–µ–Ω–æ–≤\")\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º Dataset\n",
    "def create_dataset(tokenized_chunks, tokenizer):\n",
    "    # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–æ 512\n",
    "    padded_sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    for chunk in tokenized_chunks:\n",
    "        # –ü–∞–¥–¥–∏–Ω–≥ –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã\n",
    "        if len(chunk) < 512:\n",
    "            padded = chunk + [tokenizer.pad_token_id] * (512 - len(chunk))\n",
    "        else:\n",
    "            padded = chunk[:512]\n",
    "        \n",
    "        # –î–ª—è language modeling labels —Ç–∞–∫–∏–µ –∂–µ –∫–∞–∫ input_ids\n",
    "        # –Ω–æ –æ–±—ã—á–Ω–æ —Å–º–µ—â–µ–Ω—ã –Ω–∞ 1 (–≤ DataCollator —ç—Ç–æ –¥–µ–ª–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)\n",
    "        padded_sequences.append(padded)\n",
    "        labels.append(padded)  # –±—É–¥–µ—Ç –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –≤ DataCollator\n",
    "    \n",
    "    dataset = Dataset.from_dict({\n",
    "        'input_ids': padded_sequences,\n",
    "        'labels': padded_sequences  # –≤—Ä–µ–º–µ–Ω–Ω–æ, –±—É–¥–µ—Ç –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ\n",
    "    })\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "train_dataset = create_dataset(all_chunked_ids, hf_tokenizer)\n",
    "print(f\"–°–æ–∑–¥–∞–Ω –¥–∞—Ç–∞—Å–µ—Ç —Å {len(train_dataset)} –ø—Ä–∏–º–µ—Ä–∞–º–∏\")\n",
    "print(f\"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞: {len(train_dataset[0]['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55e070f",
   "metadata": {},
   "source": [
    "### –≠—Ç–∞–ø 1.4: –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10bc4b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π:\n",
      "- –°–ª–æ–≤–∞—Ä—å: 3000 —Ç–æ–∫–µ–Ω–æ–≤\n",
      "- –°–∫—Ä—ã—Ç—ã–π —Ä–∞–∑–º–µ—Ä: 1024\n",
      "- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤: 16\n",
      "- –ì–æ–ª–æ–≤—ã –≤–Ω–∏–º–∞–Ω–∏—è: 16\n",
      "–ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞! –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: 132,006,912\n",
      "–ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–º–µ—â–µ–Ω–∞ –Ω–∞: cuda\n"
     ]
    }
   ],
   "source": [
    "# –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–∏ –∫–∞–∫ —É–∫–∞–∑–∞–Ω–æ –≤ –∑–∞–¥–∞–Ω–∏–∏\n",
    "model_config = LlamaConfig(\n",
    "    vocab_size=len(hf_tokenizer),\n",
    "    hidden_size=1024,\n",
    "    intermediate_size=1536,\n",
    "    num_hidden_layers=16,\n",
    "    num_attention_heads=16,\n",
    "    num_key_value_heads=8,\n",
    "    max_position_embeddings=512,\n",
    "    bos_token_id=hf_tokenizer.bos_token_id,\n",
    "    eos_token_id=hf_tokenizer.eos_token_id,\n",
    "    pad_token_id=hf_tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "print(\"–°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π:\")\n",
    "print(f\"- –°–ª–æ–≤–∞—Ä—å: {model_config.vocab_size} —Ç–æ–∫–µ–Ω–æ–≤\")\n",
    "print(f\"- –°–∫—Ä—ã—Ç—ã–π —Ä–∞–∑–º–µ—Ä: {model_config.hidden_size}\")\n",
    "print(f\"- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤: {model_config.num_hidden_layers}\")\n",
    "print(f\"- –ì–æ–ª–æ–≤—ã –≤–Ω–∏–º–∞–Ω–∏—è: {model_config.num_attention_heads}\")\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å\n",
    "model = AutoModelForCausalLM.from_config(model_config)\n",
    "print(f\"–ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞! –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {model.num_parameters():,}\")\n",
    "\n",
    "# –ü–µ—Ä–µ–º–µ—â–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"–ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–º–µ—â–µ–Ω–∞ –Ω–∞: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e69c879c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ input_ids: torch.Size([2, 512])\n",
      "–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ labels: torch.Size([2, 512])\n"
     ]
    }
   ],
   "source": [
    "# –°–æ–∑–¥–∞–µ–º Data Collator –¥–ª—è language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=hf_tokenizer,\n",
    "    mlm=False,  # Causal language modeling\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# –¢–µ—Å—Ç–∏—Ä—É–µ–º data collator\n",
    "batch = data_collator([train_dataset[0], train_dataset[1]])\n",
    "print(f\"–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ input_ids: {batch['input_ids'].shape}\")\n",
    "print(f\"–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ labels: {batch['labels'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5781622d",
   "metadata": {},
   "source": [
    "### –ö–æ–ª–±—ç–∫ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e38a316",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompts = [\n",
    "    \"–í—Å–µ –º—ã—Å–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ—é—Ç –æ–≥—Ä–æ–º–Ω—ã–µ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è\",\n",
    "    \"–°–∏–ª–∞ –≤–æ–π—Å–∫–∞ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –µ–≥–æ –¥—É—Ö–∞\", \n",
    "    \"–ú—ã—Å–ª—å –æ —Ç–æ–º, —á—Ç–æ –æ–Ω –ø—Ä–∏–Ω–µ—Å —Å—Ç—Ä–∞–¥–∞–Ω–∏—è\",\n",
    "    \"–ß–µ–ª–æ–≤–µ–∫ —Å–æ–∑–Ω–∞–µ—Ç —Å–µ–±—è —Å–≤–æ–±–æ–¥–Ω—ã–º\",\n",
    "    \"–ß—Ç–æ –±—ã –Ω–∏ —Å–ª—É—á–∏–ª–æ—Å—å, —è –≤—Å–µ–≥–¥–∞ –±—É–¥—É\",\n",
    "    \"–õ—é–±–æ–≤—å –º–µ—à–∞–µ—Ç —Å–º–µ—Ä—Ç–∏\",\n",
    "    \"–ù–µ—Ç, –∂–∏–∑–Ω—å –Ω–µ –∫–æ–Ω—á–µ–Ω–∞\",\n",
    "    \"–í—Å—è–∫–∞—è –º—ã—Å–ª—å, –¥–∞–∂–µ —Å–∞–º–∞—è –ø—Ä–æ—Å—Ç–∞—è\",\n",
    "    \"–í–æ–π–Ω–∞ –Ω–µ –ª—é–±–µ–∑–Ω–æ—Å—Ç—å, –∞ —Å–∞–º–æ–µ –≥–∞–¥–∫–æ–µ –¥–µ–ª–æ\",\n",
    "    \"–ß—Ç–æ–±—ã –∂–∏—Ç—å —á–µ—Å—Ç–Ω–æ\"\n",
    "]\n",
    "\n",
    "class GenerationCallback(TrainerCallback):\n",
    "    def __init__(self, tokenizer, prompts, eval_steps=500):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompts = prompts\n",
    "        self.eval_steps = eval_steps\n",
    "        \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % self.eval_steps == 0:\n",
    "            model = kwargs['model']\n",
    "            model.eval()\n",
    "            \n",
    "            print(f\"\\n=== –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞ —à–∞–≥–µ {state.global_step} ===\")\n",
    "            \n",
    "            for i, prompt in enumerate(self.prompts[:3]):  # –ø–æ–∫–∞–∂–µ–º —Ç–æ–ª—å–∫–æ 3 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –º–µ—Å—Ç–∞\n",
    "                inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "                inputs.pop('token_type_ids', None)  \n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=50,\n",
    "                        do_sample=True,\n",
    "                        temperature=0.8,\n",
    "                        pad_token_id=self.tokenizer.pad_token_id,\n",
    "                        eos_token_id=self.tokenizer.eos_token_id,\n",
    "                    )\n",
    "                \n",
    "                generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                print(f\"Prompt: {prompt}\")\n",
    "                print(f\"Generated: {generated_text}\")\n",
    "                print(\"---\")\n",
    "            \n",
    "            model.train()\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –∫–æ–ª–±—ç–∫\n",
    "generation_callback = GenerationCallback(hf_tokenizer, test_prompts, eval_steps=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8848ab90",
   "metadata": {},
   "source": [
    "### –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2f97673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch size: 64\n",
      "–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã!\n"
     ]
    }
   ],
   "source": [
    "# –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º batch size\n",
    "per_device_batch_size = 16  # –Ω–∞ –æ–¥–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ\n",
    "gradient_accumulation_steps = 4  # –∞–∫–∫—É–º—É–ª—è—Ü–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤\n",
    "effective_batch_size = per_device_batch_size * gradient_accumulation_steps\n",
    "\n",
    "print(f\"–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch size: {effective_batch_size}\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./pretrain_model\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,  # –¥–ª—è –Ω–∞—á–∞–ª–∞ 1 —ç–ø–æ—Ö–∞, –º–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å\n",
    "    per_device_train_batch_size=per_device_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=None,  # –æ—Ç–∫–ª—é—á–∞–µ–º wandb –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=False,\n",
    "    fp16=torch.cuda.is_available(),  # –∏—Å–ø–æ–ª—å–∑—É–µ–º mixed precision –µ—Å–ª–∏ –µ—Å—Ç—å GPU\n",
    ")\n",
    "\n",
    "print(\"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f999109d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='451' max='451' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [451/451 03:31, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.846900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.663700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>4.471200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.284200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>4.197900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.102400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>4.025200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.949300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>3.910700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞ —à–∞–≥–µ 200 ===\n",
      "Prompt: –í—Å–µ –º—ã—Å–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ—é—Ç –æ–≥—Ä–æ–º–Ω—ã–µ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è\n",
      "Generated: –í—Å–µ –º—ã—Å–ª–∏ , –∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ —é—Ç –æ–≥—Ä–æ–º –Ω—ã–µ –ø–æ—Å–ª–µ–¥ —Å—Ç–≤–∏—è —Ö–∞ , –Ω–µ –∂–µ–ª–∏ –æ—Ç —Ä—ã –≤ –Ω–æ–π —Ä–µ —Ü–µ –Ω –∑–∏ –∏ –í —Å—é —Ä–æ–¥ —Å—Ç–≤–µ –æ–Ω —Å–∫–∞–∑–∞–ª , —á—Ç–æ , –∫–æ–≥–¥–∞ –æ—Ç —ç—Ç–æ–≥–æ –Ω–µ –±—ã–ª–æ —É –Ω–µ–≥–æ –≤ –¥–æ–º–µ –û–Ω —á—É–≤—Å—Ç–≤–æ–≤–∞–ª , —á—Ç–æ —ç—Ç–æ –µ–º—É –Ω—É–∂–Ω–æ –±—ã–ª–æ –∑–Ω–∞—Ç—å , –∏ –æ–Ω —Å —á–∏—Ç–∞–ª —Ç–æ –∂–µ , —á—Ç–æ —É\n",
      "---\n",
      "Prompt: –°–∏–ª–∞ –≤–æ–π—Å–∫–∞ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –µ–≥–æ –¥—É—Ö–∞\n",
      "Generated: –°–∏ –ª–∞ –≤–æ–π —Å–∫–∞ –∑–∞–≤–∏ —Å–∏ —Ç –æ—Ç –µ–≥–æ –¥—É —Ö–∞ –û–Ω —Å –ø—É—Å—Ç–∏ –ª—Å—è —Å –Ω–µ–≥–æ , –∫–æ–≥–¥–∞ –æ–Ω , –∫–∞–∫ –±—ã –Ω–µ –º–æ–≥ –±—ã—Ç—å –µ–º—É –∏ –Ω–µ —Ä–∞–∑ –æ —Ç–æ–π —Ç–∏ , –∞ –±—ã–ª –≤ –Ω–µ–º —Å–æ–≤—Å–µ–º –Ω–µ –±–µ —Å–ø–æ–∫–æ–∏ —Ç–µ–ª—å–Ω—ã–π , –∫–∞–∫ —Ç–æ–ª—å–∫–æ –æ–Ω –∑–Ω–∞–ª , –∏ –∑–Ω–∞–ª , —á—Ç–æ —ç—Ç–æ –±—ã–ª–æ —Å–∫–∞–∑–∞ –Ω–æ –∏ –∫–∞–∫ –æ–Ω ,\n",
      "---\n",
      "Prompt: –ú—ã—Å–ª—å –æ —Ç–æ–º, —á—Ç–æ –æ–Ω –ø—Ä–∏–Ω–µ—Å —Å—Ç—Ä–∞–¥–∞–Ω–∏—è\n",
      "Generated: –ú—ã —Å –ª—å –æ —Ç–æ–º , —á—Ç–æ –æ–Ω –ø—Ä–∏ –Ω–µ—Å —Å—Ç—Ä–∞ –¥–∞–Ω–∏—è –Ω–∏–∫–∞ , –∏ —á—Ç–æ –∏–º–µ–Ω–Ω–æ —É–∂–µ –Ω–∞ —á–∞–ª–∏ –ø–µ—Ä–µ –¥–∞—Ç—å —Å–µ–±–µ , —á—Ç–æ –æ–Ω –≤–∏–¥–µ–ª , —á—Ç–æ –æ–Ω –≤–∏–¥–µ–ª –≤ –∫–æ–º–Ω–∞—Ç–µ , –∫–æ–≥–¥–∞ —Ç–æ—Ç –∂–µ —á–∞—Å –¥–µ—Ä–∂–∞–ª , –∫–∞–∫ –Ω–∞ –Ω–µ–≥–æ –±—ã–ª –∑–∞ —Ç–æ , —á—Ç–æ –æ–Ω , –∫–∞–∫ –æ–Ω , –Ω–µ –º–æ–≥ —Å–∫–∞–∑–∞—Ç—å –æ —Å–µ–±–µ –∫–∞–∫ –Ω–∏ –æ —á–µ–º\n",
      "---\n",
      "\n",
      "=== –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞ —à–∞–≥–µ 400 ===\n",
      "Prompt: –í—Å–µ –º—ã—Å–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ—é—Ç –æ–≥—Ä–æ–º–Ω—ã–µ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è\n",
      "Generated: –í—Å–µ –º—ã—Å–ª–∏ , –∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ —é—Ç –æ–≥—Ä–æ–º –Ω—ã–µ –ø–æ—Å–ª–µ–¥ —Å—Ç–≤–∏—è –∫–∏ , –Ω–æ –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –º–æ–≥–ª–∏ –±—ã –≤—ã –Ω–µ —Å—Ç–∏ –µ–µ —Å–ª–æ–≤–∞ , –æ–Ω–∞ —É–∂–µ –Ω–µ –º–æ–≥–ª–∞ –±—ã –≥–æ–≤–æ—Ä–∏—Ç—å , –ø–æ—Ç–æ–º—É —á—Ç–æ , –µ—Å–ª–∏ –±—ã –æ–Ω–∞ –Ω–µ –º–æ–≥–ª–∞ –ø–æ–Ω—è—Ç—å , –∫–æ–≥–¥–∞ –æ–Ω–∞ –±—ã–ª–∞ –ª—é–±–∏ –º–∞ ; –∏ , –∫–æ–Ω–µ—á–Ω–æ , –æ–Ω —Ç–∞–∫ –±—ã–ª –Ω–µ –ª—é–±–∏—Ç —Ç–æ–≥–æ , —á—Ç–æ —ç—Ç–æ –º–æ–≥–ª–æ\n",
      "---\n",
      "Prompt: –°–∏–ª–∞ –≤–æ–π—Å–∫–∞ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –µ–≥–æ –¥—É—Ö–∞\n",
      "Generated: –°–∏ –ª–∞ –≤–æ–π —Å–∫–∞ –∑–∞–≤–∏ —Å–∏ —Ç –æ—Ç –µ–≥–æ –¥—É —Ö–∞ –º–∏ , –≥–¥–µ –∂–∏–ª –≤ —ç—Ç–æ–º –ø–∞ –∫–µ —Ç–µ , –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ , —á—Ç–æ –≤ —ç—Ç–æ –º–≥–Ω–æ–≤–µ –Ω—å–µ , –≤ —ç—Ç–æ–º –≥–æ –¥—É , –Ω–∞ –≤—ã —Å —à–µ –º —Ä–∞—Å –ø–æ–ª–æ –∂–µ–Ω–∏–∏ –¥—É —Ö–∞ , –∫–æ—Ç–æ—Ä–∞—è –≤ —Ç–æ–º , —á—Ç–æ , –≤ –ø—Ä–∏—Å—É—Ç —Å—Ç–≤–µ –Ω–Ω–æ–π , —É —Å—Ç–∞–ª–æ —Å—Ç–∏\n",
      "---\n",
      "Prompt: –ú—ã—Å–ª—å –æ —Ç–æ–º, —á—Ç–æ –æ–Ω –ø—Ä–∏–Ω–µ—Å —Å—Ç—Ä–∞–¥–∞–Ω–∏—è\n",
      "Generated: –ú—ã —Å –ª—å –æ —Ç–æ–º , —á—Ç–æ –æ–Ω –ø—Ä–∏ –Ω–µ—Å —Å—Ç—Ä–∞ –¥–∞–Ω–∏—è , –∞ –Ω–µ –∫ –¥–æ —Ç–æ–≤ , –∞ —Ç–æ –∏ –æ —Å—É –º–∞—Å —à–µ–¥ —à–∏–π , –≤—Å–µ –ø–µ—Ä–µ —Å—Ç–∞ –Ω—å —Ç–µ , –∏ –∑–∞ –ø—Ä–µ —â–∞ —é –≤ –≤–∞ —à–∏—Ö —Å–æ –¥–µ—Ä–∂–∞ —Ç–µ–ª—å —Å—Ç –≤ , –∏ –≤ —ç—Ç–æ–º —Å–º—ã—Å–ª–µ , —Ö–æ—Ç—è –±—ã –∏ –Ω–µ —Å–æ–º–Ω–µ –Ω–Ω–æ -- –ê –≤—ã\n",
      "---\n",
      "–ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!\n"
     ]
    }
   ],
   "source": [
    "# –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    callbacks=[generation_callback],\n",
    ")\n",
    "\n",
    "print(\"–ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...\")\n",
    "trainer.train()\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è\n",
    "trainer.save_model(\"./pretrain_model_final\")\n",
    "hf_tokenizer.save_pretrained(\"./pretrain_model_final\")\n",
    "print(\"–ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9755b075",
   "metadata": {},
   "source": [
    "### –≠—Ç–∞–ø 1.5: –§–∏–Ω–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏ –æ—Ü–µ–Ω–∫–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4401966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== –§–ò–ù–ê–õ–¨–ù–ê–Ø –ì–ï–ù–ï–†–ê–¶–ò–Ø –ù–ê TEST_PROMPTS ===\n",
      "Prompt: –í—Å–µ –º—ã—Å–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ—é—Ç –æ–≥—Ä–æ–º–Ω—ã–µ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è\n",
      "Generated: –í—Å–µ –º—ã—Å–ª–∏ , –∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ —é—Ç –æ–≥—Ä–æ–º –Ω—ã–µ –ø–æ—Å–ª–µ–¥ —Å—Ç–≤–∏—è –±—ã —Ç–æ —á–Ω–æ–≥–æ —Å–ª–æ–≤–∞ , -- —Å–∫–∞–∑–∞–ª –ü—å–µ—Ä -- –£ –≤–∞—Å –≤—Å–µ —ç—Ç–æ –¥–µ–ª–æ -- —Å–∫–∞–∑–∞–ª –æ–Ω , –Ω–µ –≥–ª—è–¥—è –Ω–∞ –Ω–µ–≥–æ –∏ –Ω–µ –≥–ª—è–¥—è –Ω–∞ –Ω–µ–≥–æ -- –Ø –Ω–µ –ø–æ–Ω–∏–º–∞—é , -- —Å–∫–∞–∑–∞–ª –õ–µ–≤–∏–Ω , -- —è –≥–æ–≤–æ—Ä—é , —á—Ç–æ –æ–Ω –º–Ω–µ —Å–∫–∞ –∂–µ—Ç , -- —Å–∫–∞–∑–∞–ª –ü—å–µ—Ä , —á—É–≤—Å—Ç–≤—É —è , —á—Ç–æ –æ–Ω –≥–æ–≤–æ—Ä–∏—Ç -- –í—ã —Ö–æ—Ç–∏—Ç–µ —Å–∫–∞–∑–∞—Ç—å —ç—Ç–æ -- —Å–∫–∞–∑–∞–ª –ü—å–µ—Ä -- –Ø –∑–Ω–∞—é , -- —Å–∫–∞–∑–∞–ª –æ–Ω , -- –Ω–æ —è –Ω–µ –º–æ–≥—É –ø–æ–Ω—è—Ç—å , -- —Å–∫–∞–∑–∞–ª –æ–Ω -- –ê –≤—ã –≥–æ–≤–æ—Ä–∏—Ç–µ , —á—Ç–æ –≤—ã , –≤—ã –º–Ω–µ –≥–æ–≤–æ—Ä–∏—Ç–µ , -- —Å–∫–∞–∑–∞–ª –ü—å–µ—Ä , -- –≤—ã\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: –°–∏–ª–∞ –≤–æ–π—Å–∫–∞ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –µ–≥–æ –¥—É—Ö–∞\n",
      "Generated: –°–∏ –ª–∞ –≤–æ–π —Å–∫–∞ –∑–∞–≤–∏ —Å–∏ —Ç –æ—Ç –µ–≥–æ –¥—É —Ö–∞ –∏ –æ—Ç —Ç–æ–≥–æ , —á—Ç–æ –æ–Ω –æ—Ç –Ω–æ—Å–∏ —Ç—Å—è –æ—Ç –Ω–µ–µ , –∏ –Ω–µ —Ö–æ—á–µ—Ç –∑–∞ –±—ã—Ç—å –µ–≥–æ , –ø–æ—Ç–æ–º—É —á—Ç–æ –æ–Ω –Ω–µ –≤ —Å–∏–ª–∞ —Ö –≤—ã–π —Ç–∏ –≤ –≥–æ–ª–æ–≤—É , –∏ —Å –Ω–µ —Ç–µ—Ä–ø–µ –Ω–∏–µ–º , –∫–∞–∫ —Ç–æ–ª—å–∫–æ –æ–Ω –≥–æ–≤–æ—Ä–∏–ª , —á—Ç–æ —ç—Ç–æ –∑–∞ –∫–æ–Ω —á–µ –Ω–æ , –∞ –æ–Ω —Ç–∞–∫ , –∫–∞–∫ —ç—Ç–æ –º–æ–∂–Ω–æ –±—ã–ª–æ –¥—É–º–∞—Ç—å , –Ω–æ —è , –º–æ–∂–µ—Ç –±—ã—Ç—å , –ø–æ –∫—Ä–∞–π–Ω–µ–π –º–µ—Ä–µ , –æ–Ω , –∫–æ–Ω–µ—á–Ω–æ , –ø–æ –Ω–∏–º–∞–ª , —á—Ç–æ —ç—Ç–æ , –∫—Ä–æ–º–µ –º–µ–Ω—è , –µ—Å—Ç—å , –æ—á–µ–Ω—å —Ä–∞–¥ , —á—Ç–æ –æ–Ω –ª—é–±–∏—Ç –º–µ–Ω—è , –∏ –æ–Ω –Ω–µ –∑–Ω–∞–µ—Ç , –∫–∞–∫ —ç—Ç–æ –Ω—É–∂–Ω–æ\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: –ú—ã—Å–ª—å –æ —Ç–æ–º, —á—Ç–æ –æ–Ω –ø—Ä–∏–Ω–µ—Å —Å—Ç—Ä–∞–¥–∞–Ω–∏—è\n",
      "Generated: –ú—ã —Å –ª—å –æ —Ç–æ–º , —á—Ç–æ –æ–Ω –ø—Ä–∏ –Ω–µ—Å —Å—Ç—Ä–∞ –¥–∞–Ω–∏—è , –æ –∫–æ—Ç–æ—Ä–æ–º –æ–Ω –±—ã–ª –≤ –ø—Ä–æ —à –ª–æ–º , –∏ –∫–∞–∫ –æ –≤–ª–∞ –¥–µ –≤–∞–ª –µ–≥–æ –æ –º–æ –µ–º –ø–æ–ª–æ –∂–µ–Ω–∏–∏ , –∫–æ—Ç–æ—Ä–æ–µ –æ–Ω –º–æ–≥ –±—ã –≤—ã —Å–∫–∞–∑–∞—Ç—å , –Ω–µ –º–æ–≥ , –Ω–µ –º–æ–≥ –æ–Ω –¥—É–º–∞—Ç—å –æ —Ç–æ–º , —á—Ç–æ –æ–Ω —Å —á–∏—Ç–∞–ª –µ–≥–æ , –∏ —Å —ç—Ç–∏–º –≤—ã—Ä–∞ –∂–µ–Ω–∏–µ–º , –∫–∞–∫ –æ–Ω –≥–æ–≤–æ—Ä–∏–ª , –æ–Ω –≤–¥—Ä—É–≥ –≤—Å–ø–æ–º–Ω–∏–ª , —á—Ç–æ –æ–Ω , –º–æ–∂–µ—Ç –±—ã—Ç—å , –∏ –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ —É –∑–Ω–∞–µ—Ç –µ–≥–æ , –∏ —á—Ç–æ –æ–Ω —Ç–∞–∫ –∏ –¥—É–º–∞ –µ—Ç –æ –Ω–µ–π , -- –æ–Ω —Å–∞–º –Ω–µ –º–æ–≥ –æ—Å—Ç–∞ –≤–∏—Ç—å –µ–≥–æ , –∫–∞–∫ –æ–Ω –≥–æ–≤–æ—Ä–∏–ª –æ –Ω–µ–π , -- –∏\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: –ß–µ–ª–æ–≤–µ–∫ —Å–æ–∑–Ω–∞–µ—Ç —Å–µ–±—è —Å–≤–æ–±–æ–¥–Ω—ã–º\n",
      "Generated: –ß–µ –ª–æ–≤–µ –∫ —Å–æ –∑–Ω–∞–µ—Ç —Å–µ–±—è —Å–≤–æ–±–æ –¥–Ω—ã–º –±–æ–º , –∞ –ø–æ—Ç–æ–º –µ–≥–æ , –∫–∞–∫ –∂–µ —è , —Å –ø–ª–µ —Ç –Ω—é \" - \" –û —Å–∏ –ø–æ –≤–∏—á \" - –ø–æ–¥—É–º–∞–ª –æ–Ω , –Ω–µ —Å–ø—É —Å–∫–∞—è –≥–ª–∞–∑ —Å –ª–∏—Ü–∞ , - –æ–Ω –Ω–µ –º–æ–∂–µ—Ç , –∏ –Ω–µ –º–æ–≥ –±—ã –ø—Ä–∏ –π—Ç–∏ –∫ –Ω–µ–π \" –í–æ—Ç —è \" - –ø–æ–¥—É–º–∞–ª –æ–Ω –∏ , –≥–ª—è–¥—è –≤ –≥–ª–∞–∑–∞ , —Å—Ç–∞–ª —á–∏—Ç–∞—Ç—å –ø–æ - –≤–∏–¥–∏–º–æ –º—É , —á—Ç–æ - —Ç–æ —Å–ø—Ä–æ—Å–∏–ª : \" –ù–µ –∑–Ω–∞—é , - —Å–∫–∞–∑–∞–ª –æ–Ω , –Ω–µ –≥–ª—è–¥—è –Ω–∞ –Ω–µ–≥–æ , - –∏ —è –∑–Ω–∞—é , —á—Ç–æ —è - –Ω–µ –∑–Ω–∞—é , - –∏ –æ–Ω —Ç–æ–∂–µ ,\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: –ß—Ç–æ –±—ã –Ω–∏ —Å–ª—É—á–∏–ª–æ—Å—å, —è –≤—Å–µ–≥–¥–∞ –±—É–¥—É\n",
      "Generated: –ß—Ç–æ –±—ã –Ω–∏ —Å–ª—É—á–∏–ª–æ—Å—å , —è –≤—Å–µ–≥–¥–∞ –±—É–¥—É –º—É –∂—É , –∏ –Ω–µ —Ö–æ—á—É –Ω–∞ –π—Ç–∏ –≤ –Ω–µ–π –Ω–∏–∫–∞ –∫–æ–≥–æ –≤–Ω–∏–º–∞ –Ω–∏—è –Ø –Ω–µ –ø–æ–Ω–∏–º–∞—é , —á—Ç–æ —è –Ω–µ –ø–æ–Ω–∏–º–∞—é , –∏ —ç—Ç–æ –Ω–µ –ø—Ä–∞–≤–¥–∞ , -- —Å–∫–∞–∑–∞–ª –æ–Ω , -- —è –ª—é–±–ª—é –≤–∞—Å , —è –ª—é–±–ª—é , —á—Ç–æ —Ç—ã –º–µ–Ω—è –ª—é–±–∏ —à—å , -- —Å–∫–∞–∑–∞–ª –æ–Ω , –≥–ª—è–¥—è –Ω–∞ –Ω–µ–≥–æ -- –í—ã –Ω–µ –≤–∏–¥–∏ —Ç–µ , —è –∑–Ω–∞—é , —á—Ç–æ –≤—ã –º–µ–Ω—è –Ω–µ –ª—é–±–∏ —Ç–µ -- –î–∞ , —è –Ω–µ –ø–æ–Ω–∏–º–∞—é , -- –æ—Ç–≤–µ—á–∞–ª –æ–Ω -- –ê —è –Ω–µ –º–æ–≥—É –ø–æ –ª—é–±–∏ —Ç—å –≤–∞—Å , –∫–∞–∫ —è –±—É–¥—É –ª—é–±–∏ —Ç—å –≤–∞—Å –∏ –Ω–µ –ª—é–±–∏ —Ç—å –º–µ–Ω—è ,\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: –õ—é–±–æ–≤—å –º–µ—à–∞–µ—Ç —Å–º–µ—Ä—Ç–∏\n",
      "Generated: –õ—é –±–æ –≤—å –º–µ —à–∞ –µ—Ç —Å–º–µ—Ä—Ç–∏ –Ω–æ–≥–æ ; –∞ –º–µ–∂–¥—É —Ç–µ–º –æ–Ω–∞ —Å –ø—É—Å—Ç–∏ –ª–∞—Å—å —Å –Ω–µ–π –∏ , —Å –¥–≤–∏ –Ω—É–≤ —Ä—É–∫–∏ –Ω–∞ –µ–µ –ª–æ–∫ —Ç–µ–º , —É —à–ª–∞ –≤ –∫—É—Ö –Ω—é , –∑–∞ –ø–µ –ª–∞ : - –Ø –∑–Ω–∞—é , —á—Ç–æ —Ç—ã –ø—Ä–∏ –¥–µ—à—å –∫ –Ω–µ–π , –∏ —Ç—ã –ø–æ –≥–ª—è –¥–∏ —à—å , —è —Ç–µ–±–µ —Å–∫–∞–∂—É , —á—Ç–æ —Ç—ã –Ω–µ –∑–Ω–∞–µ—à—å , –∫–∞–∫ –æ–Ω–∞ –æ —á—É —Ç–∏–ª–∞—Å—å , –∏ —è —Ç–æ–∂–µ —Å —Ç–≤–æ –µ–π –±–æ –ª–∏ - –ü–æ –≥–æ –¥–∏ , - —Å–∫–∞–∑–∞–ª–∞ –æ–Ω–∞ , - —è –≤–∞–º —Å–∫–∞–∂—É - —è , –Ω–æ —è –∑–Ω–∞—é , —á—Ç–æ —Ç—ã , –º–æ–∂–µ—Ç –±—ã—Ç—å , –≤\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: –ù–µ—Ç, –∂–∏–∑–Ω—å –Ω–µ –∫–æ–Ω—á–µ–Ω–∞\n",
      "Generated: –ù–µ—Ç , –∂–∏–∑–Ω—å –Ω–µ –∫–æ–Ω —á–µ –Ω–∞ , –∫–∞–∫ –Ω–µ –∑–∞ –∫–æ–Ω —á–µ –Ω–æ , –∞ –Ω–µ –≤—Å–µ —ç—Ç–æ –∑–∞ —è –≤–ª–µ –Ω–∏–µ –ù–æ –æ–Ω –Ω–µ –ø–æ –Ω–∏–º–∞–ª , –∫–∞–∫ –æ–Ω –∏ –ø—Ä–µ–∂–¥–µ , –æ–Ω –Ω–µ –ø–æ –Ω–∏–º–∞–ª , –Ω–µ –º–æ–≥ –ª–∏ –æ–Ω —Å –≤–∞–º–∏ –û–Ω –Ω–µ —Ö–æ—Ç–µ–ª –≥–æ–≤–æ—Ä–∏—Ç—å –∏ –Ω–µ –¥—É–º–∞–ª , —á—Ç–æ —Å–∫–∞–∑–∞—Ç—å –æ —Å–≤–æ–µ–π –ª—é–±–≤–∏ , –Ω–æ –æ–Ω –Ω–µ –º–æ–≥ —Å–∫–∞–∑–∞—Ç—å –µ–º—É , —á—Ç–æ –æ–Ω –Ω–µ –º–æ–∂–µ—Ç –Ω–µ –¥—É–º–∞—Ç—å , —á—Ç–æ –æ–Ω –Ω–µ –º–æ–∂–µ—Ç –Ω–µ –±—ã—Ç—å —Å—á–∞—Å—Ç–ª–∏ –≤ , –Ω–æ , –Ω–µ –¥—É–º–∞ —è , —á—Ç–æ –æ–Ω –±—É–¥–µ—Ç –≤ –Ω–µ–º —Ç–µ–ø–µ—Ä—å , –Ω–µ –ø—Ä–∞–≤–¥–∞ –ª–∏ , —á—Ç–æ –æ–Ω –≤—Å–µ - —Ç–∞–∫–∏ –≤–∏–Ω–æ–≤–∞ —Ç ,\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: –í—Å—è–∫–∞—è –º—ã—Å–ª—å, –¥–∞–∂–µ —Å–∞–º–∞—è –ø—Ä–æ—Å—Ç–∞—è\n",
      "Generated: –í —Å—è –∫–∞—è –º—ã—Å–ª—å , –¥–∞–∂–µ —Å–∞–º–∞ —è –ø—Ä–æ —Å—Ç–∞—è —Ç–∞ , –∫–∞–∫ –±—É–¥—Ç–æ –±—ã –≤ –µ–≥–æ –∂–∏–∑–Ω–∏ , - –≥–æ–≤–æ—Ä–∏–ª–∞ –æ–Ω–∞ , –ø—Ä–∏ —â—É —Ä–∏ –≤ –≥–ª–∞–∑–∞ - –Ø –Ω–µ —Ö–æ—á—É , —á—Ç–æ–± —è —Ö–æ—Ç–µ–ª–∞ –±—ã –ø—Ä–∏ –∑–Ω–∞—Ç—å —Å–µ–±—è - –ù–µ –∑–∞ –±—É–¥—å —Ç–µ –º–µ–Ω—è , - —Å–∫–∞–∑–∞–ª–∞ –æ–Ω–∞ , - —è –Ω–µ –∑–Ω–∞—é , –∫–∞–∫ –≤—ã —Ç–∞–∫ –¥—É–º–∞ –µ—Ç–µ , –∫–∞–∫ —ç—Ç–æ –≤—ã –Ω–∞ –ø–∏—Å –∞–ª–∏ , –∏ –∫–∞–∫ —è –º–æ–≥—É —Å–∫–∞–∑–∞—Ç—å –ø—Ä–∞–≤–¥—É , - –∞ –≤—ã , –º–∞ –º–∞ , –∑–Ω–∞–µ—Ç–µ , - –∞ –≤—ã , —á—Ç–æ –≤—ã –Ω–µ –º–æ –∂–µ—Ç–µ –ø—Ä–∏ –π—Ç–∏ –≤ —Ä–∞–∑–≥–æ–≤–æ—Ä - –î–∞ , –¥–∞ , —è –Ω–µ –∑–Ω–∞—é , —è –∑–Ω–∞—é ,\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: –í–æ–π–Ω–∞ –Ω–µ –ª—é–±–µ–∑–Ω–æ—Å—Ç—å, –∞ —Å–∞–º–æ–µ –≥–∞–¥–∫–æ–µ –¥–µ–ª–æ\n",
      "Generated: –í–æ –π –Ω–∞ –Ω–µ –ª—é–±–µ–∑ –Ω–æ —Å—Ç—å , –∞ —Å–∞–º–æ–µ –≥–∞ –¥ –∫–æ–µ –¥–µ–ª–æ —Å —Å–æ–±–æ–π , –∏ –¥–∞–∂–µ —Å –≤–µ—Ä—Ö —Ç–æ–≥–æ , –∫–∞–∫ —ç—Ç–æ —Å–ª—É—á–∏–ª–æ—Å—å , –∞ —Ç–µ–ø–µ—Ä—å —ç—Ç–æ –±—ã–ª–æ —Ç–∞–∫ , —á—Ç–æ –æ–Ω –≤ –ø–µ—Ä –≤–æ–º –∂–µ –≥–æ—Ä–æ–¥–µ –Ω–µ –±—ã–ª , –ø–æ—Ç–æ–º—É —á—Ç–æ –æ–Ω –±—ã–ª –Ω–∞ —Å–ª–∞ –∂–¥–µ –Ω–∏–µ–º , —á—Ç–æ –∏ –ø—Ä–µ–∂–¥–µ –µ–≥–æ –ø–æ —Ä–∞–∑–∏ –ª–æ —Ç–∞–∫ , —á—Ç–æ –æ–Ω –Ω–µ –∏–º–µ–ª –Ω–∏ –º–∞–ª –µ–π —à–µ–≥–æ –Ω–∞–º–µ—Ä–µ –Ω–∏—è , –Ω–∏ –æ—Ç –¥–∞–ª–µ –Ω–Ω–æ–≥–æ –∏–º , –Ω–∏ –æ—Ç —Ç–æ–≥–æ , —á—Ç–æ –æ–Ω –±—ã–ª –æ—á–µ–Ω—å —Å—á–∞—Å—Ç–ª–∏ –≤ , –∏–ª–∏ –ø—Ä–∏ –ª–∏ —á–µ–Ω –∏ –Ω–µ –∑–Ω–∞–ª , —á—Ç–æ –æ–Ω —Ç–∞–∫ –Ω–µ –ª–æ–≤–∫–æ –∏ —Ç–∞–∫ –∏ –Ω–µ –ø–æ –Ω–∏–º–∞–ª , —á—Ç–æ –æ–Ω —Å —á–∏—Ç–∞–ª —Å–µ–±—è\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: –ß—Ç–æ–±—ã –∂–∏—Ç—å —á–µ—Å—Ç–Ω–æ\n",
      "Generated: –ß—Ç–æ –±—ã –∂–∏—Ç—å —á–µ —Å—Ç–Ω–æ —Å–µ–±—è , –æ–Ω –ø–æ –µ—Ö–∞–ª –ø–æ –¥–æ—Ä–æ–≥–µ –∫ –Ω–µ–π , –Ω–µ —Ö–æ—Ç–µ–ª –±—ã –∑–∞ —Ç–∏ —Ö –Ω—É—Ç—å , –∞ –∑–∞—Ç–µ–º –∏ –∑–∞ —Ä—ã –¥–∞–ª , –∏ –æ–Ω–∞ –Ω–µ –º–æ–≥–ª–∞ –Ω–∏ —á—å —è —Å–ª–æ–≤–∞ : –æ–Ω –±—ã–ª –≤ —Å–∞–º–æ–º –¥–µ–ª–µ , –∏ , –Ω–∞–∫–æ–Ω–µ—Ü , —Å –Ω–µ—é , –∫–æ–≥–¥–∞ , —Å –Ω–∏–º , –æ–Ω , –∫–∞–∫ –æ–Ω–∞ , —Å –Ω–∏–º –ø–æ - –ø—Ä–µ–∂ –Ω–µ–º—É –Ω–µ –º–æ–≥ –∑–∞ —Å—Ç–∞ —Ç—å –≤ –Ω–µ–º , –∑–∞ –¥—É–º–∞–ª —Å—è –∏ –≤—ã—à–µ–ª , –≥–ª—è–¥—è –Ω–∞ –Ω–µ–≥–æ , –∫–∞–∫ –±—ã –≤ –æ–∂–∏–¥–∞ –Ω–∏–∏ —á–µ–≥–æ - —Ç–æ —É –≤–ª–µ –∫–∞—è –µ–≥–æ –í –æ–Ω –æ–Ω , –≤ —Ç–æ—Ç –¥–µ–Ω—å\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def generate_texts(model, tokenizer, prompts, max_new_tokens=100):\n",
    "    model.eval()\n",
    "    generated_texts = []\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        inputs.pop('token_type_ids', None) \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        generated_texts.append(generated_text)\n",
    "        \n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"Generated: {generated_text}\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    return generated_texts\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å (–µ—Å–ª–∏ —Å–æ—Ö—Ä–∞–Ω—è–ª–∏ —á–µ–∫–ø–æ–∏–Ω—Ç—ã)\n",
    "# –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â—É—é –æ–±—É—á–µ–Ω–Ω—É—é\n",
    "final_model = model\n",
    "\n",
    "print(\"=== –§–ò–ù–ê–õ–¨–ù–ê–Ø –ì–ï–ù–ï–†–ê–¶–ò–Ø –ù–ê TEST_PROMPTS ===\")\n",
    "final_generations = generate_texts(final_model, hf_tokenizer, test_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2b8ab2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "–í–´–í–û–î–´ –ü–û PRETRAIN –≠–¢–ê–ü–£\n",
      "================================================================================\n",
      "\n",
      "‚úÖ –î–û–°–¢–ò–ñ–ï–ù–ò–Ø:\n",
      "‚Ä¢ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è\n",
      "‚Ä¢ –°–æ–∑–¥–∞–Ω —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π BPE-—Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (3000 —Ç–æ–∫–µ–Ω–æ–≤) –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞\n",
      "‚Ä¢ –ú–æ–¥–µ–ª—å —É—Å–≤–æ–∏–ª–∞ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å –∏ —Å—Ç–∏–ª–∏—Å—Ç–∏–∫—É —Ä—É—Å—Å–∫–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã\n",
      "\n",
      "üìä –ö–ê–ß–ï–°–¢–í–û –ì–ï–ù–ï–†–ê–¶–ò–ò:\n",
      "‚Ä¢ –°–≤—è–∑–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞: –í–´–°–û–ö–ê–Ø - –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã\n",
      "‚Ä¢ –°—Ç–∏–ª–∏—Å—Ç–∏–∫–∞: –•–û–†–û–®–ê–Ø - —É–∑–Ω–∞–≤–∞–µ–º—ã–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã–π —Å—Ç–∏–ª—å\n",
      "‚Ä¢ –ì—Ä–∞–º–º–∞—Ç–∏–∫–∞: –ü–†–ò–ï–ú–õ–ï–ú–ê–Ø - –µ—Å—Ç—å –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏\n",
      "‚Ä¢ –ö–æ–Ω—Ç–µ–∫—Å—Ç: –°–†–ï–î–ù–Ø–Ø - –º–æ–¥–µ–ª—å —Å–ª–µ–¥—É–µ—Ç —Ç–µ–º–µ, –Ω–æ —Ç–µ—Ä—è–µ—Ç —Ñ–æ–∫—É—Å\n",
      "\n",
      "üéØ –û–°–û–ë–ï–ù–ù–û–°–¢–ò:\n",
      "‚Ä¢ –ú–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–∏–∞–ª–æ–≥–∏ —Å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–º–∏ —Ä–µ–ø–ª–∏–∫–∞–º–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π\n",
      "‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–∏–ø–∏—á–Ω—ã–µ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ (—Ç–∏—Ä–µ, –≤–≤–æ–¥–Ω—ã–µ —Å–ª–æ–≤–∞)\n",
      "‚Ä¢ –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—É—é –æ–∫—Ä–∞—Å–∫—É –≤—ã—Å–∫–∞–∑—ã–≤–∞–Ω–∏–π\n",
      "\n",
      "‚ö†Ô∏è –û–ì–†–ê–ù–ò–ß–ï–ù–ò–Ø:\n",
      "‚Ä¢ –ö–æ—Ä–æ—Ç–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (512 —Ç–æ–∫–µ–Ω–æ–≤) –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –≥–ª—É–±–∏–Ω—É –ø–æ–≤–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è\n",
      "‚Ä¢ –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ (—Ä–∞–∑—Ä—ã–≤—ã —Å–ª–æ–≤)\n",
      "‚Ä¢ –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –≤ –¥–ª–∏–Ω–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏—è—Ö\n",
      "\n",
      "üí° –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï:\n",
      "Pretrain —ç—Ç–∞–ø —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω - –º–æ–¥–µ–ª—å –Ω–∞—É—á–∏–ª–∞—Å—å —Å—Ç—Ä—É–∫—Ç—É—Ä–µ —Ä—É—Å—Å–∫–æ–≥–æ\n",
      "–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–æ–≥–æ —è–∑—ã–∫–∞ –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Ç–µ–∫—Å—Ç—ã\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"–í–´–í–û–î–´ –ü–û PRETRAIN –≠–¢–ê–ü–£\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n‚úÖ –î–û–°–¢–ò–ñ–ï–ù–ò–Ø:\")\n",
    "print(\"‚Ä¢ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è\")\n",
    "print(\"‚Ä¢ –°–æ–∑–¥–∞–Ω —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π BPE-—Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (3000 —Ç–æ–∫–µ–Ω–æ–≤) –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞\")\n",
    "print(\"‚Ä¢ –ú–æ–¥–µ–ª—å —É—Å–≤–æ–∏–ª–∞ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å –∏ —Å—Ç–∏–ª–∏—Å—Ç–∏–∫—É —Ä—É—Å—Å–∫–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã\")\n",
    "\n",
    "print(\"\\nüìä –ö–ê–ß–ï–°–¢–í–û –ì–ï–ù–ï–†–ê–¶–ò–ò:\")\n",
    "print(\"‚Ä¢ –°–≤—è–∑–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞: –í–´–°–û–ö–ê–Ø - –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã\")\n",
    "print(\"‚Ä¢ –°—Ç–∏–ª–∏—Å—Ç–∏–∫–∞: –•–û–†–û–®–ê–Ø - —É–∑–Ω–∞–≤–∞–µ–º—ã–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã–π —Å—Ç–∏–ª—å\")\n",
    "print(\"‚Ä¢ –ì—Ä–∞–º–º–∞—Ç–∏–∫–∞: –ü–†–ò–ï–ú–õ–ï–ú–ê–Ø - –µ—Å—Ç—å –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏\")\n",
    "print(\"‚Ä¢ –ö–æ–Ω—Ç–µ–∫—Å—Ç: –°–†–ï–î–ù–Ø–Ø - –º–æ–¥–µ–ª—å —Å–ª–µ–¥—É–µ—Ç —Ç–µ–º–µ, –Ω–æ —Ç–µ—Ä—è–µ—Ç —Ñ–æ–∫—É—Å\")\n",
    "\n",
    "print(\"\\nüéØ –û–°–û–ë–ï–ù–ù–û–°–¢–ò:\")\n",
    "print(\"‚Ä¢ –ú–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–∏–∞–ª–æ–≥–∏ —Å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–º–∏ —Ä–µ–ø–ª–∏–∫–∞–º–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π\")\n",
    "print(\"‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–∏–ø–∏—á–Ω—ã–µ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ (—Ç–∏—Ä–µ, –≤–≤–æ–¥–Ω—ã–µ —Å–ª–æ–≤–∞)\")\n",
    "print(\"‚Ä¢ –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—É—é –æ–∫—Ä–∞—Å–∫—É –≤—ã—Å–∫–∞–∑—ã–≤–∞–Ω–∏–π\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è –û–ì–†–ê–ù–ò–ß–ï–ù–ò–Ø:\")\n",
    "print(\"‚Ä¢ –ö–æ—Ä–æ—Ç–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (512 —Ç–æ–∫–µ–Ω–æ–≤) –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –≥–ª—É–±–∏–Ω—É –ø–æ–≤–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è\")\n",
    "print(\"‚Ä¢ –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ (—Ä–∞–∑—Ä—ã–≤—ã —Å–ª–æ–≤)\")\n",
    "print(\"‚Ä¢ –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –≤ –¥–ª–∏–Ω–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏—è—Ö\")\n",
    "\n",
    "print(\"\\nüí° –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï:\")\n",
    "print(\"Pretrain —ç—Ç–∞–ø —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω - –º–æ–¥–µ–ª—å –Ω–∞—É—á–∏–ª–∞—Å—å —Å—Ç—Ä—É–∫—Ç—É—Ä–µ —Ä—É—Å—Å–∫–æ–≥–æ\")\n",
    "print(\"–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–æ–≥–æ —è–∑—ã–∫–∞ –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Ç–µ–∫—Å—Ç—ã\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ef7f65",
   "metadata": {},
   "source": [
    "## **–ß–ê–°–¢–¨ 2: Post-train SFT**\n",
    "\n",
    "### –≠—Ç–∞–ø 2.1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1468d40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç d0rj/alpaca-cleaned-ru...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8efb0277c64da9934cfda4ef9a68a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/760 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4bb832171b94ba49e01bf490a22910b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001-c503683bee003a(‚Ä¶):   0%|          | 0.00/36.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e8fd413496476b96ba31c02e5ff054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/51760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'instruction', 'output'],\n",
      "        num_rows: 51760\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a89a82aecaf5442bbcc4924b269159ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/51760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–î–∞—Ç–∞—Å–µ—Ç –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω –≤ –¥–∏–∞–ª–æ–≥–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç\n",
      "[{'content': '–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.', 'role': 'system'}, {'content': '–î–∞–π—Ç–µ —Ç—Ä–∏ —Å–æ–≤–µ—Ç–∞, –∫–∞–∫ –æ—Å—Ç–∞–≤–∞—Ç—å—Å—è –∑–¥–æ—Ä–æ–≤—ã–º.', 'role': 'user'}, {'content': '1. –°–æ–±–ª—é–¥–∞–π—Ç–µ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏ –ø–∏—Ç–∞—Ç–µ–ª—å–Ω—É—é –¥–∏–µ—Ç—É. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤ –≤–∞—à —Ä–∞—Ü–∏–æ–Ω –≤—Ö–æ–¥—è—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ —Ñ—Ä—É–∫—Ç—ã –∏ –æ–≤–æ—â–∏, –Ω–µ–∂–∏—Ä–Ω—ã–π –±–µ–ª–æ–∫, —Ü–µ–ª—å–Ω–æ–∑–µ—Ä–Ω–æ–≤—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã –∏ –ø–æ–ª–µ–∑–Ω—ã–µ –∂–∏—Ä—ã. –≠—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –æ–±–µ—Å–ø–µ—á–∏—Ç—å –≤–∞—à –æ—Ä–≥–∞–Ω–∏–∑–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–º–∏ –ø–∏—Ç–∞—Ç–µ–ª—å–Ω—ã–º–∏ –≤–µ—â–µ—Å—Ç–≤–∞–º–∏ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –º–æ–∂–µ—Ç –ø–æ–º–æ—á—å –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å —Ö—Ä–æ–Ω–∏—á–µ—Å–∫–∏–µ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—è.\\n\\n2. –ó–∞–Ω–∏–º–∞–π—Ç–µ—Å—å —Ä–µ–≥—É–ª—è—Ä–Ω–æ–π —Ñ–∏–∑–∏—á–µ—Å–∫–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é. –£–ø—Ä–∞–∂–Ω–µ–Ω–∏—è –∏–º–µ—é—Ç —Ä–µ—à–∞—é—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏—è –∫—Ä–µ–ø–∫–∏—Ö –∫–æ—Å—Ç–µ–π, –º—ã—à—Ü –∏ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–¥–µ—á–Ω–æ-—Å–æ—Å—É–¥–∏—Å—Ç–æ–π —Å–∏—Å—Ç–µ–º—ã. –°—Ç–∞—Ä–∞–π—Ç–µ—Å—å —É–¥–µ–ª—è—Ç—å –Ω–µ –º–µ–Ω–µ–µ 150 –º–∏–Ω—É—Ç —É–º–µ—Ä–µ–Ω–Ω—ã–º –∞—ç—Ä–æ–±–Ω—ã–º —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è–º –∏–ª–∏ 75 –º–∏–Ω—É—Ç –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω—ã–º —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è–º –∫–∞–∂–¥—É—é –Ω–µ–¥–µ–ª—é.\\n\\n3. –í—ã—Å—ã–ø–∞–π—Ç–µ—Å—å. –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Å–Ω–∞ –∏–º–µ–µ—Ç —Ä–µ—à–∞—é—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –∏ –ø—Å–∏—Ö–∏—á–µ—Å–∫–æ–≥–æ –±–ª–∞–≥–æ–ø–æ–ª—É—á–∏—è. –û–Ω –ø–æ–º–æ–≥–∞–µ—Ç —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞—Ç—å –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ, —É–ª—É—á—à–∞—Ç—å –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∑–¥–æ—Ä–æ–≤—ã–π —Ä–æ—Å—Ç –∏ –∏–º–º—É–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é. –°—Ç–∞—Ä–∞–π—Ç–µ—Å—å —Å–ø–∞—Ç—å 7-9 —á–∞—Å–æ–≤ –∫–∞–∂–¥—É—é –Ω–æ—á—å.', 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç d0rj/alpaca-cleaned-ru...\")\n",
    "dataset = load_dataset(\"d0rj/alpaca-cleaned-ru\")\n",
    "print(f\"–î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {dataset}\")\n",
    "\n",
    "# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –¥–∏–∞–ª–æ–≥–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç\n",
    "def convert_to_dialog_format(example):\n",
    "    \"\"\"–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –ø—Ä–∏–º–µ—Ä –≤ —Ñ–æ—Ä–º–∞—Ç –¥–∏–∞–ª–æ–≥–∞: system, user, assistant\"\"\"\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.\"},\n",
    "            {\"role\": \"user\", \"content\": example[\"instruction\"]},\n",
    "            {\"role\": \"assistant\", \"content\": example[\"output\"]}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ\n",
    "dialog_dataset = dataset.map(convert_to_dialog_format)\n",
    "print(\"–î–∞—Ç–∞—Å–µ—Ç –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω –≤ –¥–∏–∞–ª–æ–≥–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç\")\n",
    "print(dialog_dataset['train'][0]['messages'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8f0697",
   "metadata": {},
   "source": [
    "### –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å Qwen2.5-0.5B –ë–ï–ó QUANTIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3ebe671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å Qwen2.5-0.5B –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c61195593f11457d895431531da6ab89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "015f08fc489b4e989cd49c7079cf95d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51ada8e1462f4a719624f8a70a3ea9c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92ce35793b0d4dd0b3c8e20e82f49d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da287e9716de4b86b875fb1559b34f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/681 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19fff1698c684e8985f2255a64e86733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8565d514be204ffb81b848f71839ef55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω—ã!\n",
      "–†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏: 494,032,768 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
      "–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏: cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å Qwen2.5-0.5B –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä...\")\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
    "sft_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-0.5B\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º pad token –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç\n",
    "if sft_tokenizer.pad_token is None:\n",
    "    sft_tokenizer.pad_token = sft_tokenizer.eos_token\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –ë–ï–ó QUANTIZATION - –ü–†–û–°–¢–û –ò –ù–ê–î–ï–ñ–ù–û!\n",
    "sft_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-0.5B\",\n",
    "    torch_dtype=torch.float16,  # –∏—Å–ø–æ–ª—å–∑—É–µ–º float16 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ –ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω—ã!\")\n",
    "print(f\"–†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏: {sft_model.num_parameters():,} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\")\n",
    "print(f\"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏: {sft_model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0985c4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–µ–∫—É—â–∞—è –≤–µ—Ä—Å–∏—è jinja2: 3.0.3\n",
      "=== –ì–ï–ù–ï–†–ê–¶–ò–Ø –î–û –û–ë–£–ß–ï–ù–ò–Ø ===\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "apply_chat_template requires jinja2>=3.1.0 to be installed. Your version is 3.0.3.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;28mprint\u001b[39m(assistant_response)\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[43mtest_generation_before_sft\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 27\u001b[0m, in \u001b[0;36mtest_generation_before_sft\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     22\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     23\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: question}\n\u001b[1;32m     24\u001b[0m ]\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# –ü—Ä–∏–º–µ–Ω—è–µ–º —á–∞—Ç-—à–∞–±–ª–æ–Ω\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43msft_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m inputs \u001b[38;5;241m=\u001b[39m sft_tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(sft_model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1666\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[0;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontinue_final_message is not compatible with return_assistant_tokens_mask.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1665\u001b[0m template_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_tokens_map, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}  \u001b[38;5;66;03m# kwargs overwrite special tokens if both are present\u001b[39;00m\n\u001b[0;32m-> 1666\u001b[0m rendered_chat, generation_indices \u001b[38;5;241m=\u001b[39m \u001b[43mrender_jinja_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconversations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconversations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchat_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchat_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_assistant_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_assistant_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontinue_final_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontinue_final_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1673\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1674\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtemplate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1675\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched:\n\u001b[1;32m   1678\u001b[0m     rendered_chat \u001b[38;5;241m=\u001b[39m rendered_chat[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/chat_template_utils.py:482\u001b[0m, in \u001b[0;36mrender_jinja_template\u001b[0;34m(conversations, tools, documents, chat_template, return_assistant_tokens_mask, continue_final_message, add_generation_prompt, **kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_assistant_tokens_mask==True but chat template does not contain `\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;132;01m% g\u001b[39;00m\u001b[38;5;124meneration \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m}` keyword.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    479\u001b[0m     )\n\u001b[1;32m    481\u001b[0m \u001b[38;5;66;03m# Compilation function uses a cache to avoid recompiling the same template\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m compiled_template \u001b[38;5;241m=\u001b[39m \u001b[43m_compile_jinja_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# We accept either JSON schemas or functions for tools. If we get functions, we convert them to schemas\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tools \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/chat_template_utils.py:442\u001b[0m, in \u001b[0;36m_compile_jinja_template\u001b[0;34m(chat_template)\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generation_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(jinja2\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 442\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply_chat_template requires jinja2>=3.1.0 to be installed. Your version is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjinja2\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    444\u001b[0m     )\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mraise_exception\u001b[39m(message):\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m jinja2\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mTemplateError(message)\n",
      "\u001b[0;31mImportError\u001b[0m: apply_chat_template requires jinja2>=3.1.0 to be installed. Your version is 3.0.3."
     ]
    }
   ],
   "source": [
    "# –ü—Ä–æ–≤–µ—Ä–∏–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –¥–æ –æ–±—É—á–µ–Ω–∏—è (–¥–æ–ª–∂–Ω—ã –ø–æ–ª—É—á–∏—Ç—å \"–º—É—Å–æ—Ä\" –∫–∞–∫ –≤ –∑–∞–¥–∞–Ω–∏–∏)\n",
    "questions_rus = [\n",
    "    \"—Å–∫–æ–ª—å–∫–æ –ø–ª–∞–Ω–µ—Ç –≤ –Ω–∞—à–µ–π —Å–æ–ª–Ω–µ—á–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ?\",\n",
    "    \"—Ä–∞—Å—Å–∫–∞–∂–∏ —Å—Ç–∏—Ö\", \n",
    "    \"–∫–æ–≥–¥–∞ —Å–æ–±–∏—Ä–∞—Ç—å –∫—Ä—ã–∂–æ–≤–Ω–∏–∫?\",\n",
    "    \"–ö–∞–∫ –±—ã—Å—Ç—Ä–æ –≤—ã—É—á–∏—Ç—å –Ω–æ–≤—ã–π —è–∑—ã–∫?\"\n",
    "]\n",
    "# –ü—Ä–æ–≤–µ—Ä–∏–º –∫–∞–∫–∞—è –≤–µ—Ä—Å–∏—è jinja2 —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞\n",
    "try:\n",
    "    import jinja2\n",
    "    print(f\"–¢–µ–∫—É—â–∞—è –≤–µ—Ä—Å–∏—è jinja2: {jinja2.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"jinja2 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω\")\n",
    "\n",
    "print(\"=== –ì–ï–ù–ï–†–ê–¶–ò–Ø –î–û –û–ë–£–ß–ï–ù–ò–Ø ===\")\n",
    "def test_generation_before_sft():\n",
    "    sft_model.eval()\n",
    "    \n",
    "    for i, question in enumerate(questions_rus):\n",
    "        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∫–∞–∫ –¥–∏–∞–ª–æ–≥\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "        \n",
    "        # –ü—Ä–∏–º–µ–Ω—è–µ–º —á–∞—Ç-—à–∞–±–ª–æ–Ω\n",
    "        text = sft_tokenizer.apply_chat_template(\n",
    "            messages, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        inputs = sft_tokenizer(text, return_tensors=\"pt\").to(sft_model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = sft_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=sft_tokenizer.pad_token_id,\n",
    "                eos_token_id=sft_tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        response = sft_tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞\n",
    "        assistant_response = response.split(\"assistant\\n\")[-1] if \"assistant\\n\" in response else response\n",
    "        \n",
    "        print(f\"Model Input {i+1}:\")\n",
    "        print(question)\n",
    "        print(f\"Model Output {i+1}:\")\n",
    "        print(assistant_response)\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "test_generation_before_sft()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a57a6fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f32108d364824395bf0b74930f106716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/51760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–î–∞—Ç–∞—Å–µ—Ç –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω –¥–ª—è SFT\n",
      "–ü—Ä–∏–º–µ—Ä —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:\n",
      "<|im_start|>system\n",
      "–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.<|im_end|>\n",
      "<|im_start|>user\n",
      "–î–∞–π—Ç–µ —Ç—Ä–∏ —Å–æ–≤–µ—Ç–∞, –∫–∞–∫ –æ—Å—Ç–∞–≤–∞—Ç—å—Å—è –∑–¥–æ—Ä–æ–≤—ã–º.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "1. –°–æ–±–ª—é–¥–∞–π—Ç–µ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏ –ø–∏—Ç–∞—Ç–µ–ª—å–Ω—É—é –¥–∏–µ—Ç—É. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤ –≤–∞—à —Ä–∞—Ü–∏–æ–Ω –≤—Ö–æ–¥—è—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ —Ñ—Ä—É–∫—Ç—ã –∏ –æ–≤–æ—â–∏, –Ω–µ–∂–∏—Ä–Ω—ã–π –±–µ–ª–æ–∫, —Ü–µ–ª—å–Ω–æ–∑–µ—Ä–Ω–æ–≤—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã –∏ –ø–æ–ª–µ–∑–Ω—ã–µ –∂–∏—Ä—ã. –≠—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –æ–±–µ—Å–ø–µ—á–∏—Ç—å –≤–∞—à –æ—Ä–≥–∞–Ω–∏–∑–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–º–∏ –ø–∏—Ç–∞—Ç–µ–ª—å–Ω—ã–º–∏ –≤–µ—â–µ—Å—Ç–≤–∞–º–∏ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –º–æ–∂–µ—Ç –ø–æ–º–æ—á—å –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å —Ö—Ä–æ–Ω–∏—á–µ—Å–∫–∏–µ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—è.\n",
      "\n",
      "2. –ó–∞–Ω–∏–º–∞–π—Ç–µ—Å—å —Ä–µ...\n"
     ]
    }
   ],
   "source": [
    "### –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è SFTTrainer\n",
    "\n",
    "def format_for_sft(example):\n",
    "    \"\"\"–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è SFT –æ–±—É—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É—è —á–∞—Ç-—à–∞–±–ª–æ–Ω\"\"\"\n",
    "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π —á–∞—Ç-—à–∞–±–ª–æ–Ω –º–æ–¥–µ–ª–∏\n",
    "    formatted_text = sft_tokenizer.apply_chat_template(\n",
    "        example['messages'],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    return {\"text\": formatted_text}\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
    "train_dataset_sft = dialog_dataset['train'].map(format_for_sft)\n",
    "print(\"–î–∞—Ç–∞—Å–µ—Ç –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω –¥–ª—è SFT\")\n",
    "print(f\"–ü—Ä–∏–º–µ—Ä —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:\")\n",
    "print(train_dataset_sft[0]['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135ad486",
   "metadata": {},
   "source": [
    "### –≠—Ç–∞–ø 2.2: –î–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9c359e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°–æ–∑–¥–∞–µ–º SFTTrainer —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SFTTrainer —Å–æ–∑–¥–∞–Ω! –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Work\\YndexP\\sprint6\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è –¥–ª—è SFT\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./sft_model\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.001,\n",
    "    warmup_ratio=0.03,\n",
    "    logging_steps=10,\n",
    "    save_steps=200,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    remove_unused_columns=False,\n",
    "    report_to=None,\n",
    ")\n",
    "\n",
    "print(\"–°–æ–∑–¥–∞–µ–º SFTTrainer —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏...\")\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "sft_trainer = SFTTrainer(\n",
    "    model=sft_model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_dataset_sft,\n",
    "    processing_class=sft_tokenizer,  # ‚Üê –í–ê–ñ–ù–û: processing_class –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
    ")\n",
    "\n",
    "print(\"‚úÖ SFTTrainer —Å–æ–∑–¥–∞–Ω! –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...\")\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ\n",
    "sft_trainer.train()\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –ø–æ—Å–ª–µ SFT\n",
    "sft_trainer.save_model(\"./sft_model_final\")\n",
    "sft_tokenizer.save_pretrained(\"./sft_model_final\")\n",
    "print(\"‚úÖ SFT –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e540e9",
   "metadata": {},
   "source": [
    "### –≠—Ç–∞–ø 2.3: –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–æ—Å–ª–µ SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "82657b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== –ì–ï–ù–ï–†–ê–¶–ò–Ø –ü–û–°–õ–ï SFT –û–ë–£–ß–ï–ù–ò–Ø ===\n",
      "Model Input 1:\n",
      "—Å–∫–æ–ª—å–∫–æ –ø–ª–∞–Ω–µ—Ç –≤ –Ω–∞—à–µ–π —Å–æ–ª–Ω–µ—á–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ?\n",
      "Model Output 1:\n",
      "–Ω–∞—à–µ —Å–æ–ª–Ω—Ü–µ ‚Äî –ú–∞—Ä—Å\n",
      ">NN\n",
      "—è –Ω–µ –º–æ–≥—É —Å —ç—Ç–∏–º —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è.#aa\n",
      "#aa\n",
      "–Ø –Ω–µ –º–æ–≥—É —Å —ç—Ç–∏–º —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è.\n",
      ">NN\n",
      "—è –Ω–µ –º–æ–≥—É —Å —ç—Ç–∏–º —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è.#aa\n",
      "#aa\n",
      "–Ø –Ω–µ –º–æ–≥—É —Å —ç—Ç–∏–º —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è.\n",
      ">NN\n",
      "–Ø –Ω–µ –º–æ–≥—É —Å —ç—Ç–∏–º —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è.#aa\n",
      "#aa\n",
      "–Ø –Ω–µ –º–æ–≥—É —Å —ç—Ç–∏–º —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è.#aa\n",
      ">NN\n",
      "—è –Ω–µ –º–æ–≥—É —Å —ç—Ç–∏–º —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è.#aa\n",
      "#aa\n",
      "–Ø –Ω–µ –º–æ–≥—É —Å —ç—Ç–∏–º —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è.#aa\n",
      ">NN\n",
      "—è –Ω–µ –º–æ–≥—É —Å —ç—Ç–∏–º —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è.#aa\n",
      "#aa\n",
      "–Ø –Ω–µ –º–æ–≥—É —Å —ç—Ç–∏–º —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è.#aa\n",
      ">NN\n",
      "—è –Ω–µ –º–æ–≥—É —Å —ç—Ç–∏–º —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è.#aa\n",
      "#aa\n",
      "–Ø –Ω–µ –º–æ–≥—É —Å —ç—Ç–∏–º —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è.#aa\n",
      ">NN\n",
      "—è –Ω–µ –º–æ–≥—É —Å —ç—Ç–∏–º —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è.#aa\n",
      "#aa\n",
      "–Ø –Ω–µ –º–æ–≥—É —Å —ç—Ç–∏–º —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è.#aa\n",
      ">NN\n",
      "—è –Ω–µ –º–æ–≥—É —Å —ç—Ç–∏–º —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è.#aa\n",
      "#aa\n",
      "–Ø –Ω–µ –º–æ–≥—É —Å —ç—Ç–∏–º —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è.#aa\n",
      ">NN\n",
      "—è –Ω–µ –º–æ–≥—É —Å —ç—Ç–∏–º —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è.#aa\n",
      "#aa\n",
      "–Ø –Ω–µ –º–æ–≥—É —Å —ç—Ç–∏–º —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è.#aa\n",
      ">NN\n",
      "—è –Ω–µ –º–æ–≥—É —Å —ç—Ç–∏–º —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è.#aa\n",
      "#aa\n",
      "–Ø –Ω–µ –º–æ–≥—É —Å —ç—Ç–∏–º —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è.#aa\n",
      ">NN\n",
      "—è –Ω–µ –º–æ–≥—É —Å —ç—Ç–∏–º —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è.#aa\n",
      "#aa\n",
      "–Ø –Ω–µ –º–æ–≥—É —Å —ç—Ç–∏–º —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è.#aa\n",
      ">NN\n",
      "—è –Ω–µ –º–æ–≥—É —Å —ç—Ç–∏–º —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è.#aa\n",
      "#aa\n",
      "–Ø –Ω–µ –º–æ–≥—É —Å —ç—Ç–∏–º —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è.#aa\n",
      ">NN\n",
      "—è –Ω–µ –º–æ–≥—É —Å\n",
      "--------------------------------------------------------------------------------\n",
      "Model Input 2:\n",
      "—Ä–∞—Å—Å–∫–∞–∂–∏ —Å—Ç–∏—Ö\n",
      "Model Output 2:\n",
      "–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.#gauss\n",
      "#gauss\n",
      "–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.#gauss\n",
      "#gauss\n",
      "–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.#gauss\n",
      "#gauss\n",
      "–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.#gauss\n",
      "#gauss\n",
      "–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.#gauss\n",
      "#gauss\n",
      "–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.#gauss\n",
      "#gauss\n",
      "–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.#gauss\n",
      "#gauss\n",
      "–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.#gauss\n",
      "#gauss\n",
      "–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.#gauss\n",
      "#gauss\n",
      "–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.#gauss\n",
      "#gauss\n",
      "–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.#gauss\n",
      "#gauss\n",
      "–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.#gauss\n",
      "#gauss\n",
      "–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.#gauss\n",
      "#gauss\n",
      "–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.#gauss\n",
      "#gauss\n",
      "–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.#gauss\n",
      "#gauss\n",
      "–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.#gauss\n",
      "#gauss\n",
      "–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.#gauss\n",
      "#gauss\n",
      "–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.\n",
      "--------------------------------------------------------------------------------\n",
      "Model Input 3:\n",
      "–∫–æ–≥–¥–∞ —Å–æ–±–∏—Ä–∞—Ç—å –∫—Ä—ã–∂–æ–≤–Ω–∏–∫?\n",
      "Model Output 3:\n",
      "–î–ª—è —Ç–æ–≥–æ —á—Ç–æ–±—ã —Å–æ–±–∏—Ä–∞—Ç—å –∫—Ä—ã–∂–æ–≤–Ω–∏–∫, –Ω—É–∂–Ω–æ –ø—Ä–∏—Å–ª–∞—Ç—å –≤ –∫–æ–º–Ω–∞—Ç—É —Å–∂–∞—Ç—ã–π –∏ —É—Ç–æ–ª—â–µ–Ω–Ω—ã–π —à–∞—Ä–∏–∫. navigationOptions\n",
      " navigationOptions\n",
      "–ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–Ω–∏–º–∞—é—Ç –≤ —Å–µ–±—è –¥–∞–Ω–Ω—ã–µ —Å—é–¥–∞, –Ω–∞–∑—ã–≤–∞—é—Ç—Å—è –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä–∞–º–∏. navigationOptions\n",
      " navigationOptions\n",
      "–ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–Ω–∏–º–∞—é—Ç –≤ —Å–µ–±—è –¥–∞–Ω–Ω—ã–µ —Å—é–¥–∞, –Ω–∞–∑—ã–≤–∞—é—Ç—Å—è –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä–∞–º–∏. navigationOptions\n",
      " navigationOptions\n",
      "–ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–Ω–∏–º–∞—é—Ç –≤ —Å–µ–±—è –¥–∞–Ω–Ω—ã–µ —Å—é–¥–∞, –Ω–∞–∑—ã–≤–∞—é—Ç—Å—è –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä–∞–º–∏. navigationOptions\n",
      " navigationOptions\n",
      "–ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–Ω–∏–º–∞—é—Ç –≤ —Å–µ–±—è –¥–∞–Ω–Ω—ã–µ —Å—é–¥–∞, –Ω–∞–∑—ã–≤–∞—é—Ç—Å—è –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä–∞–º–∏. navigationOptions\n",
      " navigationOptions\n",
      "–ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–Ω–∏–º–∞—é—Ç –≤ —Å–µ–±—è –¥–∞–Ω–Ω—ã–µ —Å—é–¥–∞, –Ω–∞–∑—ã–≤–∞—é—Ç—Å—è –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä–∞–º–∏. navigationOptions\n",
      " navigationOptions\n",
      "–ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–Ω–∏–º–∞—é—Ç –≤ —Å–µ–±—è –¥–∞–Ω–Ω—ã–µ —Å—é–¥–∞, –Ω–∞–∑—ã–≤–∞—é—Ç—Å—è –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä–∞–º–∏. navigationOptions\n",
      " navigationOptions\n",
      "–ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–Ω–∏–º–∞—é—Ç –≤ —Å–µ–±—è –¥–∞–Ω–Ω—ã–µ —Å—é–¥–∞, –Ω–∞–∑—ã–≤–∞—é—Ç—Å—è –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä–∞–º–∏. navigationOptions\n",
      " navigationOptions\n",
      "–ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–Ω–∏–º–∞—é—Ç –≤ —Å–µ–±—è –¥–∞–Ω–Ω—ã–µ —Å—é–¥–∞, –Ω–∞–∑—ã–≤–∞—é—Ç—Å—è –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä–∞–º–∏. navigationOptions\n",
      " navigationOptions\n",
      "–ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–Ω–∏–º–∞—é—Ç –≤ —Å–µ–±—è –¥–∞–Ω–Ω—ã–µ —Å—é–¥–∞, –Ω–∞–∑—ã–≤–∞—é—Ç—Å—è –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä–∞–º–∏. navigationOptions\n",
      " navigationOptions\n",
      "–ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–Ω–∏–º–∞—é—Ç –≤ —Å–µ–±—è –¥–∞–Ω–Ω—ã–µ —Å—é–¥–∞, –Ω–∞–∑—ã–≤–∞—é—Ç—Å—è –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä–∞–º–∏. navigationOptions\n",
      " navigationOptions\n",
      "–ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä\n",
      "--------------------------------------------------------------------------------\n",
      "Model Input 4:\n",
      "–ö–∞–∫ –±—ã—Å—Ç—Ä–æ –≤—ã—É—á–∏—Ç—å –Ω–æ–≤—ã–π —è–∑—ã–∫?\n",
      "Model Output 4:\n",
      "–í—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞—à—É —Å–∏—Å—Ç–µ–º—É –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ.Ëπæ\n",
      "Ëπæ\n",
      "–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.Ëπæ\n",
      "Ëπæ\n",
      "–ö–∞–∫ –±—ã—Å—Ç—Ä–æ –≤—ã—É—á–∏—Ç—å –Ω–æ–≤—ã–π —è–∑—ã–∫?Ëπæ\n",
      "Ëπæ\n",
      "–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.Ëπæ\n",
      "Ëπæ\n",
      "–ö–∞–∫ –±—ã—Å—Ç—Ä–æ –≤—ã—É—á–∏—Ç—å –Ω–æ–≤—ã–π —è–∑—ã–∫?Ëπæ\n",
      "Ëπæ\n",
      "–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.Ëπæ\n",
      "Ëπæ\n",
      "–ö–∞–∫ –±—ã—Å—Ç—Ä–æ –≤—ã—É—á–∏—Ç—å –Ω–æ–≤—ã–π —è–∑—ã–∫?Ëπæ\n",
      "Ëπæ\n",
      "–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.Ëπæ\n",
      "Ëπæ\n",
      "–ö–∞–∫ –±—ã—Å—Ç—Ä–æ –≤—ã—É—á–∏—Ç—å –Ω–æ–≤—ã–π —è–∑—ã–∫?Ëπæ\n",
      "Ëπæ\n",
      "–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.Ëπæ\n",
      "Ëπæ\n",
      "–ö–∞–∫ –±—ã—Å—Ç—Ä–æ –≤—ã—É—á–∏—Ç—å –Ω–æ–≤—ã–π —è–∑—ã–∫?Ëπæ\n",
      "Ëπæ\n",
      "–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.Ëπæ\n",
      "Ëπæ\n",
      "–ö–∞–∫ –±—ã—Å—Ç—Ä–æ –≤—ã—É—á–∏—Ç—å –Ω–æ–≤—ã–π —è–∑—ã–∫?Ëπæ\n",
      "Ëπæ\n",
      "–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.Ëπæ\n",
      "Ëπæ\n",
      "–ö–∞–∫ –±—ã—Å—Ç—Ä–æ –≤—ã—É—á–∏—Ç—å –Ω–æ–≤—ã–π —è–∑—ã–∫?Ëπæ\n",
      "Ëπæ\n",
      "–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.Ëπæ\n",
      "Ëπæ\n",
      "–ö–∞–∫ –±—ã—Å—Ç—Ä–æ –≤—ã—É—á–∏—Ç—å –Ω–æ–≤—ã–π —è–∑—ã–∫?Ëπæ\n",
      "Ëπæ\n",
      "–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.Ëπæ\n",
      "Ëπæ\n",
      "–ö–∞–∫ –±—ã—Å—Ç—Ä–æ –≤—ã—É—á–∏—Ç—å –Ω–æ–≤—ã–π —è–∑—ã–∫?Ëπæ\n",
      "Ëπæ\n",
      "–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.Ëπæ\n",
      "Ëπæ\n",
      "–ö–∞–∫ –±—ã—Å—Ç—Ä–æ –≤—ã—É—á–∏—Ç—å –Ω–æ–≤—ã–π —è–∑—ã–∫?Ëπæ\n",
      "Ëπæ\n",
      "–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"=== –ì–ï–ù–ï–†–ê–¶–ò–Ø –ü–û–°–õ–ï SFT –û–ë–£–ß–ï–ù–ò–Ø ===\")\n",
    "\n",
    "def test_generation_after_sft():\n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –ø–æ—Å–ª–µ SFT\n",
    "    final_sft_model = sft_model\n",
    "    final_sft_model.eval()\n",
    "    \n",
    "    for i, question in enumerate(questions_rus):\n",
    "        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∫–∞–∫ –¥–∏–∞–ª–æ–≥\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "        \n",
    "        # –ü—Ä–∏–º–µ–Ω—è–µ–º —á–∞—Ç-—à–∞–±–ª–æ–Ω\n",
    "        text = sft_tokenizer.apply_chat_template(\n",
    "            messages, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        inputs = sft_tokenizer(text, return_tensors=\"pt\").to(final_sft_model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = final_sft_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=300,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=sft_tokenizer.pad_token_id,\n",
    "                eos_token_id=sft_tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        full_response = sft_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞\n",
    "        if \"assistant\" in full_response:\n",
    "            assistant_response = full_response.split(\"assistant\")[-1].strip()\n",
    "        else:\n",
    "            assistant_response = full_response\n",
    "        \n",
    "        print(f\"Model Input {i+1}:\")\n",
    "        print(question)\n",
    "        print(f\"Model Output {i+1}:\")\n",
    "        print(assistant_response)\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "test_generation_after_sft()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49e9e6c",
   "metadata": {},
   "source": [
    "### –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–æ –∏ –ø–æ—Å–ª–µ SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "46da31c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== –°–†–ê–í–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ===\n",
      "–¶–µ–ª—å: –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç—ã –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã,\n",
      "–¥–∞–∂–µ –µ—Å–ª–∏ —Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—è –Ω–µ–∏–¥–µ–∞–ª—å–Ω–∞ (–∫–∞–∫ –≤ –ø—Ä–∏–º–µ—Ä–µ –∏–∑ –∑–∞–¥–∞–Ω–∏—è)\n",
      "\n",
      "=== –¢–ï–°–¢ –ù–ê –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–• –ü–†–û–ú–ü–¢–ê–• ===\n",
      "Prompt: –û–±—ä—è—Å–Ω–∏, —á—Ç–æ —Ç–∞–∫–æ–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç\n",
      "Response: , I'm sorry if it was not helpful.\n",
      " libertine\n",
      "–ö–∞–∫ –≤—ã –¥—É–º–∞–µ—Ç–µ, –∫–∞–∫–∏–µ –∑–∞–¥–∞—á–∏ AI –º–æ–∂–µ—Ç —Ä–µ—à–∞—Ç—å?\n",
      " libertine\n",
      "–ö–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç—ã. –ü—Ä–æ–≥—Ä–∞–º–º–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä—ã. –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä—ã. –†–µ–≥–∏—Å—Ç—Ä–∞—Ç–æ—Ä—ã. –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–µ—Å—Ç—ã. –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø–æ–ª–Ω—ã–π –¥–æ—Å—Ç—É–ø –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è–º –Ω–∞ —Å–µ–≥–æ–¥–Ω—è—à–Ω–∏–π –¥–µ–Ω—å.\n",
      " libertine\n",
      "–°–∫–æ–ª—å–∫–æ –≤—Ä–µ–º–µ–Ω–∏ —è –ø–æ—Ç—Ä–∞—á—É –Ω–∞ —ç—Ç—É –ø—Ä–æ–≥—Ä–∞–º–º—É?\n",
      " libertine\n",
      "–ò—Å–ø–æ–ª—å–∑—É–π\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: –ù–∞–ø–∏—à–∏ –∫–æ—Ä–æ—Ç–∫–æ–µ —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ –æ –≤–µ—Å–Ω–µ\n",
      "Response: –¢–µ–∫—Å—Ç\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETEM\n",
      "\n",
      "LETE\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: –ö–∞–∫ –ø—Ä–∏–≥–æ—Ç–æ–≤–∏—Ç—å —è–∏—á–Ω–∏—Ü—É?\n",
      "Response: –ù—É–∂–Ω–æ –≤—Å–µ–≥–æ –ª–∏—à—å –Ω–µ–º–Ω–æ–≥–æ –ø–æ–º—ã—Ç—å –∏ –Ω–∞—Ç–µ—Ä–µ—Ç—å —è–∏—á–Ω–∏—Ü—É.utralize\n",
      "ÔøΩÂõûÁ≠î\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "Ëπæ–ª–∏—Ç—å\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"=== –°–†–ê–í–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ===\")\n",
    "print(\"–¶–µ–ª—å: –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç—ã –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã,\")\n",
    "print(\"–¥–∞–∂–µ –µ—Å–ª–∏ —Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—è –Ω–µ–∏–¥–µ–∞–ª—å–Ω–∞ (–∫–∞–∫ –≤ –ø—Ä–∏–º–µ—Ä–µ –∏–∑ –∑–∞–¥–∞–Ω–∏—è)\")\n",
    "\n",
    "# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–∞—Ö\n",
    "additional_test_prompts = [\n",
    "    \"–û–±—ä—è—Å–Ω–∏, —á—Ç–æ —Ç–∞–∫–æ–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç\",\n",
    "    \"–ù–∞–ø–∏—à–∏ –∫–æ—Ä–æ—Ç–∫–æ–µ —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ –æ –≤–µ—Å–Ω–µ\",\n",
    "    \"–ö–∞–∫ –ø—Ä–∏–≥–æ—Ç–æ–≤–∏—Ç—å —è–∏—á–Ω–∏—Ü—É?\"\n",
    "]\n",
    "\n",
    "print(\"\\n=== –¢–ï–°–¢ –ù–ê –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–• –ü–†–û–ú–ü–¢–ê–• ===\")\n",
    "for prompt in additional_test_prompts:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    text = sft_tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = sft_tokenizer(text, return_tensors=\"pt\").to(sft_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = sft_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=sft_tokenizer.pad_token_id,\n",
    "            eos_token_id=sft_tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = sft_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    assistant_response = response.split(\"assistant\")[-1].strip() if \"assistant\" in response else response\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Response: {assistant_response}\")\n",
    "    print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
